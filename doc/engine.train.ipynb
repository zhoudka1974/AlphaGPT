{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6af233",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "`AlphaEngine.train()` 是一个**端到端的强化学习训练循环**，专为“用神经网络自动生成可执行量化因子公式”这一任务设计。其核心思想是：**将公式生成建模为序列决策问题，通过策略梯度（REINFORCE）优化，结合真实回测奖励进行训练，并引入低秩正则化提升泛化能力**。下面逐层解析其设计逻辑与执行步骤：\n",
    "\n",
    "---\n",
    "\n",
    "## 🌐 整体架构定位\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[AlphaGPT<br>生成公式序列] --> B[StackVM<br>执行公式]\n",
    "    B --> C[MemeBacktest<br>回测评分]\n",
    "    C --> D[奖励信号]\n",
    "    D --> E[策略梯度更新]\n",
    "    E --> A\n",
    "    F[LoRD正则化] -.->|每步后| A\n",
    "    G[稳定秩监控] -.->|每100步| H[训练日志]\n",
    "```\n",
    "\n",
    "---\n",
    "## 🔁 训练循环详细步骤（每 step）\n",
    "\n",
    "### 1️⃣ **初始化输入序列**\n",
    "```python\n",
    "inp = torch.zeros((bs, 1), dtype=torch.long, device=ModelConfig.DEVICE)\n",
    "```\n",
    "- 创建 batch_size 个“空公式”起始点（假设 token 0 为起始符或占位符）\n",
    "- 后续通过自回归逐步拼接生成的 token\n",
    "\n",
    "### 2️⃣ **自回归生成公式序列（核心生成逻辑）**\n",
    "```python\n",
    "for _ in range(ModelConfig.MAX_FORMULA_LEN):\n",
    "    logits, _, _ = self.model(inp)      # 模型预测下一 token\n",
    "    dist = Categorical(logits=logits)   # 构建离散分布\n",
    "    action = dist.sample()              # 采样动作（随机探索）\n",
    "    log_probs.append(dist.log_prob(action))  # 记录 log_prob（用于梯度）\n",
    "    tokens_list.append(action)\n",
    "    inp = torch.cat([inp, action.unsqueeze(1)], dim=1)  # 拼接新 token\n",
    "```\n",
    "- **关键设计**：\n",
    "  - 使用 `Categorical` 实现**随机策略**（非贪婪），保证探索性\n",
    "  - 保存每步 `log_prob`：为后续策略梯度计算提供梯度路径\n",
    "  - 生成长度固定（`MAX_FORMULA_LEN`），避免动态长度处理复杂度\n",
    "\n",
    "### 3️⃣ **公式执行与奖励计算（连接符号世界与数值世界）**\n",
    "```python\n",
    "for i in range(bs):\n",
    "    formula = seqs[i].tolist()\n",
    "    res = self.vm.execute(formula, self.loader.feat_tensor)  # StackVM 执行\n",
    "    \n",
    "    # 有效性校验（关键！避免无效梯度）\n",
    "    if res is None: rewards[i] = -5.0      # 执行失败（语法错误/除零等）\n",
    "    elif res.std() < 1e-4: rewards[i] = -2.0  # 信号无变化（无效因子）\n",
    "    else:\n",
    "        score, ret_val = self.bt.evaluate(...)  # 回测：夏普比率/年化收益等\n",
    "        rewards[i] = score\n",
    "        # 实时更新全局最优公式\n",
    "        if score > self.best_score: ...\n",
    "```\n",
    "- **StackVM 作用**：将离散 token 序列（如 `[RET, DELAY1, MUL, VOL]`）编译为可执行计算图，输出时间序列信号\n",
    "- **MemeBacktest 作用**：在加密货币数据上回测信号，返回量化指标（如夏普比率）\n",
    "- **惩罚机制设计**：\n",
    "  - `-5.0`：严重惩罚无效公式（引导模型学习语法合法性）\n",
    "  - `-2.0`：惩罚无信息量信号（避免“常数因子”）\n",
    "\n",
    "### 4️⃣ **优势归一化（稳定训练关键）**\n",
    "```python\n",
    "adv = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "```\n",
    "- 将原始奖励转换为**零均值、单位方差**的优势值\n",
    "- **作用**：大幅降低策略梯度方差，加速收敛（REINFORCE 算法标准技巧）\n",
    "\n",
    "### 5️⃣ **策略梯度损失计算（REINFORCE 核心）**\n",
    "```python\n",
    "loss = 0\n",
    "for t in range(len(log_probs)):\n",
    "    loss += -log_probs[t] * adv  # 每步 log_prob × 优势\n",
    "loss = loss.mean()\n",
    "```\n",
    "- **数学本质**：$\\nabla J(\\theta) \\approx \\mathbb{E}[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot A(s,a)]$\n",
    "- **关键细节**：\n",
    "  - 使用**整个序列的标量奖励**作为每步优势（蒙特卡洛回报）\n",
    "  - 负号：最大化期望奖励 ⇨ 最小化 `-log_prob * reward`\n",
    "  - 未使用 AlphaGPT 的 `value` 头（纯 REINFORCE，非 Actor-Critic）\n",
    "\n",
    "### 6️⃣ **参数更新与正则化**\n",
    "```python\n",
    "self.opt.zero_grad()\n",
    "loss.backward()\n",
    "self.opt.step()          # AdamW 更新主参数\n",
    "\n",
    "if self.use_lord:\n",
    "    self.lord_opt.step() # Newton-Schulz 低秩衰减（作用于 attention 权重）\n",
    "```\n",
    "- **LoRD 时机**：在优化器更新**之后**施加，作为额外正则化步骤\n",
    "- **物理意义**：抑制注意力矩阵的高秩噪声成分，保留核心低秩结构\n",
    "\n",
    "### 7️⃣ **监控与日志**\n",
    "```python\n",
    "if step % 100 == 0 and self.use_lord:\n",
    "    stable_rank = self.rank_monitor.compute()  # 监控 q_proj/k_proj 的稳定秩\n",
    "postfix_dict = {'AvgRew': ..., 'BestScore': ..., 'Rank': ...}\n",
    "pbar.set_postfix(postfix_dict)\n",
    "```\n",
    "- **稳定秩下降** → LoRD 生效（模型复杂度降低）\n",
    "- **BestScore 持续上升** → 策略有效学习\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 设计亮点与工程智慧\n",
    "\n",
    "| 特性 | 价值 | 说明 |\n",
    "|------|------|------|\n",
    "| **符号-数值闭环** | 可执行性保障 | StackVM 确保生成公式100%可计算 |\n",
    "| **双重惩罚机制** | 避免无效梯度 | 语法错误/无变化信号直接负奖励 |\n",
    "| **优势归一化** | 训练稳定性 | 解决 REINFORCE 高方差痛点 |\n",
    "| **LoRD 正则化** | 小数据泛化 | 针对金融数据噪声大、样本少的特点 |\n",
    "| **实时最优追踪** | 无需额外验证集 | 每步更新 best_formula，节省资源 |\n",
    "| **进度条集成监控** | 训练透明化 | 关键指标实时可见，便于调试 |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ 潜在改进方向（代码未实现但值得思考）\n",
    "\n",
    "1. **引入 Critic 网络**  \n",
    "   - 当前未使用 AlphaGPT 输出的 `value` 头  \n",
    "   - 可升级为 **Actor-Critic**：`adv = reward - value`，进一步降方差\n",
    "\n",
    "2. **课程学习（Curriculum Learning）**  \n",
    "   - 初期限制公式长度，逐步增加复杂度  \n",
    "   - 避免早期生成超长无效公式\n",
    "\n",
    "3. **重要性采样（PPO）**  \n",
    "   - 当前为纯 on-policy REINFORCE  \n",
    "   - 可引入 clipping 机制提升样本效率\n",
    "\n",
    "4. **语法约束采样**  \n",
    "   - 在采样时屏蔽非法 token（如操作符后不能接操作符）  \n",
    "   - 减少无效公式比例，提升训练效率\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 总结：train 函数的核心价值\n",
    "\n",
    "> **它构建了一个“生成-执行-评估-优化”的闭环系统，将神经网络的表示学习能力与符号程序的可解释性、可执行性深度融合，专为金融领域小样本、高噪声场景下的自动化因子挖掘而设计。**\n",
    "\n",
    "- **不是普通语言模型训练**：奖励来自真实市场回测，而非文本 likelihood\n",
    "- **不是纯黑盒优化**：生成的公式人类可读、可验证、可部署\n",
    "- **正则化与监控并重**：LoRD + 稳定秩监控，直面金融数据过拟合挑战\n",
    "\n",
    "这种“神经+符号+强化学习”的混合架构，代表了 **AI for Quant** 领域的前沿实践方向。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
