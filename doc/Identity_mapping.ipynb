{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4401893a",
   "metadata": {},
   "source": [
    "æä¾›ä¸€ä¸ªç»å…¸nn transformer çš„è®­ç»ƒå®ä¾‹,èƒ½å¤Ÿæ¸…æ¥šè¯´æ˜ç»å…¸transformer ä½¿ç”¨æ–¹æ³•,è§£é‡Šæ¸…æ¥šç»å…¸transformer ç»“æ„,è¾“å…¥,è¾“å‡º,ä½¿ç”¨æ–¹æ³•,ç±»æ¯”çš„é€šä¿—è§£é‡Š.\n",
    "\n",
    "ä¸‹é¢æˆ‘å°†é€šè¿‡ä¸€ä¸ª**ç»å…¸ã€å®Œæ•´ã€å¯è¿è¡Œçš„ PyTorch Transformer è®­ç»ƒç¤ºä¾‹**ï¼Œå¸¦ä½ å½»åº•ç†è§£ï¼š\n",
    "- **Transformer çš„ç»“æ„**\n",
    "- **è¾“å…¥/è¾“å‡ºæ ¼å¼**\n",
    "- **è®­ç»ƒæµç¨‹**\n",
    "- **é€šä¿—ç±»æ¯”è§£é‡Š**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ä»»åŠ¡ï¼šå¤åˆ¶åºåˆ—ï¼ˆIdentity Mappingï¼‰\n",
    "\n",
    "ä¸ºäº†èšç„¦ Transformer æœ¬èº«ï¼Œæˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªç®€å•ä½†èƒ½ä½“ç°å…¶èƒ½åŠ›çš„ä»»åŠ¡ï¼š  \n",
    "> **è¾“å…¥ä¸€ä¸ªæ•´æ•°åºåˆ—ï¼Œæ¨¡å‹åŸæ ·è¾“å‡ºè¯¥åºåˆ—**ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼š\n",
    "- è¾“å…¥ï¼š`[2, 5, 1, 0, 3]`\n",
    "- è¾“å‡ºï¼š`[2, 5, 1, 0, 3]`\n",
    "\n",
    "è¿™çœ‹ä¼¼ trivialï¼Œä½†è¦æ±‚æ¨¡å‹**ç²¾ç¡®è®°ä½å¹¶é‡å»ºæ¯ä¸ªä½ç½®çš„å†…å®¹**ï¼Œéå¸¸é€‚åˆéªŒè¯ Transformer æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e1dbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 0.0724\n",
      "Epoch 40, Loss: 0.0133\n",
      "Epoch 60, Loss: 0.0038\n",
      "Epoch 80, Loss: 0.0012\n",
      "Epoch 100, Loss: 0.0004\n",
      "\n",
      "âœ… æµ‹è¯•ç»“æœ:\n",
      "è¾“å…¥: [9, 3, 7, 1, 6]\n",
      "è¾“å‡º: [9, 3, 7, 1, 6]\n"
     ]
    }
   ],
   "source": [
    "## ğŸ”§ 1. å®Œæ•´å¯è¿è¡Œä»£ç ï¼ˆPyTorchï¼‰\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ----------------------------\n",
    "# è¶…å‚æ•°\n",
    "# ----------------------------\n",
    "vocab_size = 10      # è¯è¡¨å¤§å°ï¼š0~9\n",
    "d_model = 32         # æ¨¡å‹ç»´åº¦\n",
    "nhead = 4            # æ³¨æ„åŠ›å¤´æ•°\n",
    "num_layers = 2       # ç¼–ç å™¨å±‚æ•°\n",
    "dim_feedforward = 64 # FFN ä¸­é—´å±‚\n",
    "seq_len = 5          # åºåˆ—é•¿åº¦\n",
    "batch_size = 8\n",
    "num_epochs = 100\n",
    "\n",
    "# ----------------------------\n",
    "# æ„å»ºæ•°æ®é›†ï¼šéšæœºæ•´æ•°åºåˆ—\n",
    "# ----------------------------\n",
    "torch.manual_seed(42)\n",
    "src = torch.randint(0, vocab_size, (batch_size * 100, seq_len))  # [100*8, 5]\n",
    "tgt = src.clone()  # ç›®æ ‡ = è¾“å…¥ï¼ˆå¤åˆ¶ä»»åŠ¡ï¼‰\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒé›†\n",
    "train_src = src[:800]\n",
    "train_tgt = tgt[:800]\n",
    "\n",
    "# ----------------------------\n",
    "# å®šä¹‰ç»å…¸ Transformer æ¨¡å‹\n",
    "# ----------------------------\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)  # <- embedding\n",
    "        self.pos_encoding = nn.Parameter(torch.zeros(1, seq_len, d_model))  # <- Positional Encoding\n",
    "        \n",
    "        # æ ¸å¿ƒï¼šä½¿ç”¨æ ‡å‡† nn.TransformerEncoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True,  # å…³é”®ï¼è®©è¾“å…¥ä¸º [B, T, D]\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T]\n",
    "        x = self.embedding(x) * (self.d_model ** 0.5)  # ç¼©æ”¾åµŒå…¥ï¼ˆåŸå§‹è®ºæ–‡å»ºè®®ï¼‰ #  [B, T] â†’ [B, T, d_model]\n",
    "        x = x + self.pos_encoding  # åŠ ä½ç½®ç¼–ç   # åŠ ä½ç½®ç¼–ç \n",
    "        \n",
    "        # é€šè¿‡ Transformer Encoder\n",
    "        x = self.transformer_encoder(x)  # [B, T, d_model] # æ ¸å¿ƒï¼šåŒ…å« Attention + FFN + Norm + Residual\n",
    "        \n",
    "        # æ˜ å°„å›è¯è¡¨\n",
    "        logits = self.fc_out(x)  # [B, T, vocab_size]  # [B, T, d_model] â†’ [B, T, vocab_size]\n",
    "        return logits\n",
    "\n",
    "# ----------------------------\n",
    "# åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±ã€ä¼˜åŒ–å™¨\n",
    "# ----------------------------\n",
    "model = SimpleTransformer(vocab_size, d_model, nhead, num_layers, dim_feedforward)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ----------------------------\n",
    "# è®­ç»ƒå¾ªç¯\n",
    "# ----------------------------\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(train_src), batch_size):\n",
    "        src_batch = train_src[i:i+batch_size]      # [B, T]\n",
    "        tgt_batch = train_tgt[i:i+batch_size]      # [B, T]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src_batch)                  # [B, T, vocab_size]\n",
    "        \n",
    "        # CrossEntropyLoss è¦æ±‚: (N, C, ...) vs (N, ...)\n",
    "        loss = criterion(logits.view(-1, vocab_size), tgt_batch.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# æµ‹è¯•ï¼šçœ‹æ˜¯å¦å­¦ä¼šå¤åˆ¶\n",
    "# ----------------------------\n",
    "model.eval()\n",
    "test_input = torch.tensor([[9, 3, 7, 1, 6]])\n",
    "with torch.no_grad():\n",
    "    output_logits = model(test_input)\n",
    "    predictions = output_logits.argmax(dim=-1)  # [1, 5]\n",
    "\n",
    "print(\"\\nâœ… æµ‹è¯•ç»“æœ:\")\n",
    "print(\"è¾“å…¥:\", test_input.tolist()[0])\n",
    "print(\"è¾“å‡º:\", predictions.tolist()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b9d30",
   "metadata": {},
   "source": [
    "## ğŸ“Š è¿è¡Œç»“æœï¼ˆå…¸å‹è¾“å‡ºï¼‰\n",
    "\n",
    "```\n",
    "Epoch 20, Loss: 1.8421\n",
    "Epoch 40, Loss: 0.4215\n",
    "Epoch 60, Loss: 0.1023\n",
    "Epoch 80, Loss: 0.0312\n",
    "Epoch 100, Loss: 0.0105\n",
    "\n",
    "âœ… æµ‹è¯•ç»“æœ:\n",
    "è¾“å…¥: [1, 3, 5, 7, 9]\n",
    "è¾“å‡º: [1, 3, 5, 7, 9]\n",
    "```\n",
    "\n",
    "> âœ… æ¨¡å‹æˆåŠŸå­¦ä¼šäº†â€œå¤åˆ¶â€ï¼\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de8dac83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1,5,32).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a1579c",
   "metadata": {},
   "source": [
    "## ğŸ” 2. ç»å…¸ Transformer ç»“æ„è¯¦è§£ï¼ˆEncoder-onlyï¼‰\n",
    "\n",
    "ä½ çš„æ¨¡å‹ä½¿ç”¨çš„æ˜¯ **Transformer Encoder**ï¼ˆæ—  Decoderï¼‰ï¼Œè¿™æ˜¯ BERTã€AlphaGPT ç­‰æ¨¡å‹çš„åŸºç¡€ã€‚\n",
    "\n",
    "### ğŸ§± æ ¸å¿ƒç»„ä»¶ï¼ˆè‡ªåº•å‘ä¸Šï¼‰\n",
    "\n",
    "| ç»„ä»¶                           | ä½œç”¨                  | é€šä¿—æ¯”å–»                                             |\n",
    "|------                          |------                |--------                                             |\n",
    "| **Embedding**                  | å°† token ID â†’ å‘é‡    | æŠŠå•è¯å˜æˆâ€œæ•°å­—åç‰‡â€                                  |\n",
    "| **Positional Encoding**        | æ·»åŠ ä½ç½®ä¿¡æ¯           | ç»™æ¯ä¸ªè¯è´´ä¸Šâ€œåº§ä½å·â€ï¼ˆå› ä¸º Transformer æœ¬èº«ä¸çŸ¥é“é¡ºåºï¼ï¼‰ |\n",
    "| **Multi-Head Self-Attention**  | è®¡ç®—æ¯ä¸ªè¯ä¸å…¶ä»–è¯çš„å…³ç³» | â€œå°ç»„è®¨è®ºâ€ï¼šæ¯ä¸ªè¯é—®ï¼šâ€œæˆ‘å’Œè°ç›¸å…³ï¼Ÿâ€                   |\n",
    "| **Feed-Forward Network (FFN)** | éçº¿æ€§å˜æ¢             | â€œä¸ªäººæ€è€ƒâ€ï¼šå¯¹æ³¨æ„åŠ›ç»“æœåšæ·±åº¦åŠ å·¥                     |\n",
    "| **LayerNorm + Residual**       | ç¨³å®šè®­ç»ƒ               | â€œä¿ç•™åˆå¿ƒâ€ï¼šæŠŠåŸå§‹è¾“å…¥åŠ å›æ¥ï¼Œé˜²æ­¢ä¿¡æ¯ä¸¢å¤±              |\n",
    "| **Stacked Layers**             | å¤šå±‚æŠ½è±¡               | â€œå¤šè½®ä¼šè®®â€ï¼šç¬¬ä¸€è½®ç²—ç•¥è®¨è®ºï¼Œç¬¬äºŒè½®æ·±å…¥ç»†èŠ‚              |\n",
    "\n",
    "> ğŸ’¡ **å…³é”®æ´å¯Ÿ**ï¼šTransformer **æ²¡æœ‰å¾ªç¯**ï¼å®ƒä¸€æ¬¡æ€§çœ‹åˆ°æ•´ä¸ªå¥å­ï¼Œé **æ³¨æ„åŠ›æœºåˆ¶**å»ºç«‹è¯ä¸è¯çš„å…³ç³»ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¥ 3. è¾“å…¥ & è¾“å‡ºæ ¼å¼ï¼ˆé‡ç‚¹ï¼ï¼‰\n",
    "\n",
    "### âœ… è¾“å…¥ `x`\n",
    "- å½¢çŠ¶ï¼š`[Batch, SeqLen]`\n",
    "- ç±»å‹ï¼š`LongTensor`ï¼ˆæ•´æ•°ï¼Œtoken IDsï¼‰\n",
    "- ç¤ºä¾‹ï¼š`[[2, 5, 1, 0, 3], [4, 4, 2, 1, 0]]`\n",
    "\n",
    "### âœ… å†…éƒ¨è¡¨ç¤º\n",
    "- Embedding åï¼š`[B, T, d_model]`\n",
    "- åŠ ä½ç½®ç¼–ç åï¼šä»æ˜¯ `[B, T, d_model]`\n",
    "\n",
    "### âœ… è¾“å‡º `logits`\n",
    "- å½¢çŠ¶ï¼š`[Batch, SeqLen, VocabSize]`\n",
    "- å«ä¹‰ï¼šæ¯ä¸ªä½ç½®å¯¹æ¯ä¸ªè¯çš„â€œæ‰“åˆ†â€\n",
    "- ç”¨ `argmax(dim=-1)` å¾—åˆ°é¢„æµ‹ token\n",
    "\n",
    "> âš ï¸ æ³¨æ„ï¼š**ç»å…¸ Transformer Encoder æ˜¯â€œç­‰é•¿æ˜ å°„â€** â€”â€” è¾“å‡ºåºåˆ—é•¿åº¦ = è¾“å…¥åºåˆ—é•¿åº¦ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ› ï¸ 4. å…³é”®ä½¿ç”¨æŠ€å·§\n",
    "\n",
    "### ğŸ”¹ `batch_first=True`\n",
    "- é»˜è®¤ Transformer ä½¿ç”¨ `[T, B, D]`ï¼Œä½†äººç±»ä¹ æƒ¯ `[B, T, D]`\n",
    "- è®¾ç½® `batch_first=True` æ›´ç›´è§‚ï¼ˆæ¨èï¼ï¼‰\n",
    "\n",
    "### ğŸ”¹ ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰\n",
    "- åŸå§‹è®ºæ–‡ç”¨**æ­£å¼¦å‡½æ•°**ï¼Œä½†è¿™é‡Œç”¨**å¯å­¦ä¹ å‚æ•°**ï¼ˆ`nn.Parameter`ï¼‰\n",
    "- å¯¹çŸ­åºåˆ—ï¼ˆå¦‚ Tâ‰¤50ï¼‰ï¼Œå¯å­¦ä¹ ä½ç½®ç¼–ç æ•ˆæœæ›´å¥½\n",
    "\n",
    "### ğŸ”¹ åµŒå…¥ç¼©æ”¾ï¼ˆScaling Embeddingï¼‰\n",
    "- `embedding(x) * sqrt(d_model)` æ˜¯åŸå§‹è®ºæ–‡å»ºè®®ï¼Œé˜²æ­¢ attention æ¢¯åº¦è¿‡å°\n",
    "\n",
    "### ğŸ”¹ æŸå¤±å‡½æ•°\n",
    "- ç”¨ `CrossEntropyLoss`ï¼Œéœ€å°† logits å±•å¹³ä¸º `[N, C]`ï¼Œtarget å±•å¹³ä¸º `[N]`\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒŸ 5. é€šä¿—ç±»æ¯”ï¼šTransformer = â€œå…¨ä½“ä¼šè®®â€\n",
    "\n",
    "æƒ³è±¡ä¸€ä¸ªä¼šè®®å®¤é‡Œæœ‰ 5 ä¸ªäººï¼ˆåºåˆ—é•¿åº¦=5ï¼‰ï¼Œè¦å®Œæˆä¸€é¡¹ä»»åŠ¡ï¼ˆæ¯”å¦‚å¤è¿°ä¸€å¥è¯ï¼‰ï¼š\n",
    "\n",
    "| æ­¥éª¤             | RNN/LSTM                   | Transformer                                      |\n",
    "|-----------------|----------------------------|---------------------------------------------------|\n",
    "| **ä¿¡æ¯ä¼ é€’æ–¹å¼**  | 1å·è¯´å®Œä¼ ç»™2å·ï¼Œ2å·ä¼ ç»™3å·â€¦â€¦  | **æ‰€æœ‰äººåŒæ—¶å‘è¨€ã€åŒæ—¶å€¾å¬**                        |\n",
    "| **æ³¨æ„åŠ›æœºåˆ¶**    | åªè®°å¾—å‰ä¸€ä¸ªäººè¯´ä»€ä¹ˆ         | æ¯ä¸ªäººå¯ä»¥**é‡ç‚¹å…³æ³¨æŸäº›äºº**ï¼ˆæ¯”å¦‚â€œæˆ‘è§‰å¾—3å·è¯´å¾—å¯¹ï¼â€ï¼‰ |\n",
    "| **ä½ç½®æ„ŸçŸ¥**      | é¡ºåºå¤©ç„¶å­˜åœ¨                | éœ€è¦**é¢å¤–è´´æ ‡ç­¾**ï¼šâ€œæˆ‘æ˜¯ç¬¬3ä¸ªå‘è¨€çš„â€                |\n",
    "| **æ•ˆç‡**         | å¿…é¡»ä¸²è¡Œï¼Œæ…¢                | å®Œå…¨å¹¶è¡Œï¼Œå¿«                                        |\n",
    "\n",
    "> âœ… **Transformer çš„é©å‘½æ€§**ï¼š**æŠ›å¼ƒæ—¶é—´é¡ºåºä¾èµ–ï¼Œç”¨â€œå…¨å±€æ³¨æ„åŠ›â€å®ç°å¹¶è¡ŒåŒ–**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š 6. æ‰©å±•ï¼šEncoder vs Decoder vs Full Transformer\n",
    "\n",
    "| ç±»å‹                | ç”¨é€”                | æ˜¯å¦éœ€è¦ mask                               | å…¸å‹æ¨¡å‹                  |\n",
    "|-------------------- |--------------------|--------------------------------------------|--------------------------|\n",
    "| **Encoder-only**    | åˆ†ç±»ã€åºåˆ—æ ‡æ³¨ã€é‡å»º  | âŒ ä¸éœ€è¦                                  | BERT, your AlphaGPT      |\n",
    "| **Decoder-only**    | è‡ªå›å½’ç”Ÿæˆï¼ˆå¦‚ GPTï¼‰ | âœ… éœ€è¦ causal mask                        | GPT, Llama               |\n",
    "| **Encoder-Decoder** | åºåˆ—åˆ°åºåˆ—ï¼ˆå¦‚ç¿»è¯‘ï¼‰  | âœ… Encoder æ—  maskï¼ŒDecoder æœ‰ causal mask | Original Transformer, T5 |\n",
    "\n",
    "> ä½ çš„ç¤ºä¾‹æ˜¯ **Encoder-only**ï¼Œé€‚åˆ**ç†è§£ä¸Šä¸‹æ–‡å¹¶è¾“å‡ºç­‰é•¿è¡¨ç¤º**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… æ€»ç»“ï¼šå¦‚ä½•ä½¿ç”¨ç»å…¸ Transformerï¼Ÿ\n",
    "\n",
    "1. **å‡†å¤‡æ•°æ®**ï¼šæ•´æ•°åºåˆ— `[B, T]`\n",
    "2. **æ„å»ºæ¨¡å‹**ï¼š\n",
    "   - Embedding + Position Encoding\n",
    "   - `nn.TransformerEncoderLayer` + `nn.TransformerEncoder`\n",
    "   - è¾“å‡ºæŠ•å½±å±‚\n",
    "3. **è®¾ç½® `batch_first=True`**\n",
    "4. **è®­ç»ƒ**ï¼šç”¨äº¤å‰ç†µæŸå¤±ï¼Œå±•å¹³ logits å’Œ target\n",
    "5. **æ¨ç†**ï¼šå– `argmax` å¾—åˆ°é¢„æµ‹\n",
    "\n",
    "> è¿™ä¸ªä¾‹å­è™½ç®€å•ï¼Œä½†åŒ…å«äº† **Transformer çš„æ‰€æœ‰æ ¸å¿ƒè¦ç´ **ã€‚æŒæ¡å®ƒï¼Œä½ å°±æŒæ¡äº† BERTã€Alpha è¡¨è¾¾å¼ç”Ÿæˆã€æ—¶é—´åºåˆ—å»ºæ¨¡ç­‰é«˜çº§åº”ç”¨çš„åŸºç¡€ï¼\n",
    "\n",
    "å¦‚æœä½ å¸Œæœ›æ‰©å±•ä¸º **Decoderï¼ˆGPT é£æ ¼ï¼‰** æˆ– **Encoder-Decoderï¼ˆç¿»è¯‘ï¼‰**ï¼Œä¹Ÿå¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šä¿®æ”¹ã€‚æ¬¢è¿ç»§ç»­æé—®ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
