{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92dbcce",
   "metadata": {},
   "source": [
    "éå¸¸å¥½çš„é—®é¢˜ï¼æˆ‘ä»¬å°†**ä¸¥æ ¼å¯¹ç…§ä½ æä¾›çš„æ ¸å¿ƒç»„ä»¶è¡¨æ ¼**ï¼Œé€é¡¹åˆ†æå®ƒä»¬åœ¨ `SimpleTransformer` ç±»çš„ `__init__` å’Œ `forward` æ–¹æ³•ä¸­æ˜¯å¦‚ä½•ä½“ç°çš„ï¼ŒåŒ…æ‹¬ï¼š\n",
    "\n",
    "- **ä»£ç ä½ç½®ï¼ˆè¡Œï¼‰**\n",
    "- **è¾“å…¥ / è¾“å‡ºå¼ é‡å½¢çŠ¶**\n",
    "- **è®¡ç®—é€»è¾‘**\n",
    "- **ä½¿ç”¨çš„ PyTorch æ ‡å‡†æ¨¡å—**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§± æ¨¡å‹ä»£ç å›é¡¾ï¼ˆç®€åŒ– + è¡Œå·æ ‡æ³¨ï¼‰\n",
    "\n",
    "```python\n",
    "# __init__\n",
    "1: class SimpleTransformer(nn.Module):\n",
    "2:     def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward):\n",
    "3:         super().__init__()\n",
    "4:         self.d_model = d_model\n",
    "5:         self.embedding = nn.Embedding(vocab_size, d_model)                     # â† Embedding\n",
    "6:         self.pos_encoding = nn.Parameter(torch.zeros(1, seq_len, d_model))     # â† Positional Encoding\n",
    "7:         \n",
    "8:         encoder_layer = nn.TransformerEncoderLayer(                           # â† Attention + FFN + Norm + Residual\n",
    "9:             d_model=d_model,\n",
    "10:            nhead=nhead,\n",
    "11:            dim_feedforward=dim_feedforward,\n",
    "12:            batch_first=True,\n",
    "13:            dropout=0.1\n",
    "14:        )\n",
    "15:        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)  # â† Stacked Layers\n",
    "16:        \n",
    "17:        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "# forward\n",
    "18:    def forward(self, x):\n",
    "19:        x = self.embedding(x) * (self.d_model ** 0.5)   # [B, T] â†’ [B, T, d_model]\n",
    "20:        x = x + self.pos_encoding                       # åŠ ä½ç½®ç¼–ç \n",
    "21:        x = self.transformer_encoder(x)                 # æ ¸å¿ƒå¤„ç†\n",
    "22:        logits = self.fc_out(x)                         # [B, T, d_model] â†’ [B, T, vocab_size]\n",
    "23:        return logits\n",
    "```\n",
    "\n",
    "> å‡è®¾ `seq_len = 5`, `batch_size = B`, `d_model = D`, `vocab_size = V`\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” é€ç»„ä»¶è¯¦è§£\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ **Embedding**\n",
    "\n",
    "| é¡¹ç›® | è¯´æ˜ |\n",
    "|------|------|\n",
    "| **ä½œç”¨** | å°†ç¦»æ•£ token ID æ˜ å°„ä¸ºç¨ å¯†å‘é‡ |\n",
    "| **é€šä¿—æ¯”å–»** | æŠŠå•è¯å˜æˆâ€œæ•°å­—åç‰‡â€ |\n",
    "\n",
    "#### âœ… `__init__` ä¸­ï¼ˆç¬¬5è¡Œï¼‰\n",
    "```python\n",
    "self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "```\n",
    "- **åŠŸèƒ½**ï¼šåˆ›å»ºä¸€ä¸ªå¯å­¦ä¹ çš„åµŒå…¥çŸ©é˜µ `weight`ï¼Œå½¢çŠ¶ `[V, D]`\n",
    "- **PyTorch æ¨¡å—**ï¼š`torch.nn.Embedding`\n",
    "- **å‚æ•°**ï¼š\n",
    "  - `num_embeddings = vocab_size`\n",
    "  - `embedding_dim = d_model`\n",
    "\n",
    "#### âœ… `forward` ä¸­ï¼ˆç¬¬19è¡Œï¼‰\n",
    "```python\n",
    "x = self.embedding(x) * (self.d_model ** 0.5)\n",
    "```\n",
    "- **è¾“å…¥ `x`**ï¼š`[B, T]`ï¼Œ`LongTensor`ï¼Œå€¼ âˆˆ `[0, V)`\n",
    "- **`self.embedding(x)` è¾“å‡º**ï¼š`[B, T, D]`ï¼ˆæŸ¥è¡¨ç»“æœï¼‰\n",
    "- **ç¼©æ”¾æ“ä½œ**ï¼šä¹˜ä»¥ `âˆšd_model`ï¼ˆåŸå§‹ Transformer è®ºæ–‡å»ºè®®ï¼Œé˜²æ­¢ attention æ¢¯åº¦è¿‡å°ï¼‰\n",
    "- **æœ€ç»ˆè¾“å‡º**ï¼š`[B, T, D]`\n",
    "\n",
    "> âš™ï¸ **è®¡ç®—æœ¬è´¨**ï¼š`output[i,j,:] = embedding_weight[x[i,j], :] * âˆšD`\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ **Positional Encoding**\n",
    "\n",
    "| é¡¹ç›® | è¯´æ˜ |\n",
    "|------|------|\n",
    "| **ä½œç”¨** | æ³¨å…¥åºåˆ—é¡ºåºä¿¡æ¯ï¼ˆTransformer æ— å¾ªç¯ï¼Œå¤©ç„¶â€œä½ç½®ç›²â€ï¼‰ |\n",
    "| **é€šä¿—æ¯”å–»** | ç»™æ¯ä¸ªè¯è´´ä¸Šâ€œåº§ä½å·â€ |\n",
    "\n",
    "#### âœ… `__init__` ä¸­ï¼ˆç¬¬6è¡Œï¼‰\n",
    "```python\n",
    "self.pos_encoding = nn.Parameter(torch.zeros(1, seq_len, d_model))\n",
    "```\n",
    "- **åŠŸèƒ½**ï¼šåˆ›å»ºä¸€ä¸ª**å¯å­¦ä¹ çš„ä½ç½®ç¼–ç è¡¨**ï¼Œå½¢çŠ¶ `[1, T, D]`\n",
    "- **PyTorch åŠŸèƒ½**ï¼š\n",
    "  - `torch.zeros(...)`ï¼šåˆå§‹åŒ–ä¸ºå…¨é›¶\n",
    "  - `nn.Parameter(...)`ï¼šå°†å…¶æ³¨å†Œä¸º**æ¨¡å‹å‚æ•°**ï¼ˆä¼šè¢«ä¼˜åŒ–å™¨æ›´æ–°ï¼‰\n",
    "- **æ³¨æ„**ï¼šè¿™æ˜¯**å¯å­¦ä¹ ä½ç½®ç¼–ç **ï¼ˆéåŸå§‹è®ºæ–‡çš„æ­£å¼¦å‡½æ•°ï¼‰\n",
    "\n",
    "#### âœ… `forward` ä¸­ï¼ˆç¬¬20è¡Œï¼‰\n",
    "```python\n",
    "x = x + self.pos_encoding\n",
    "```\n",
    "- **è¾“å…¥ `x`**ï¼šæ¥è‡ª embeddingï¼Œ`[B, T, D]`\n",
    "- **`self.pos_encoding`**ï¼š`[1, T, D]`\n",
    "- **å¹¿æ’­æœºåˆ¶**ï¼š`[B, T, D] + [1, T, D] â†’ [B, T, D]`\n",
    "- **è¾“å‡º**ï¼šå¸¦ä½ç½®ä¿¡æ¯çš„è¡¨ç¤ºï¼Œ`[B, T, D]`\n",
    "\n",
    "> âš™ï¸ **ç›®çš„**ï¼šè®©æ¨¡å‹çŸ¥é“â€œç¬¬1ä¸ªä½ç½®â€å’Œâ€œç¬¬5ä¸ªä½ç½®â€ä¸åŒã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ **Multi-Head Self-Attention + FFN + LayerNorm + Residual**\n",
    "\n",
    "> è¿™äº›å­ç»„ä»¶**å…¨éƒ¨å°è£…åœ¨ `nn.TransformerEncoderLayer` å†…éƒ¨**ï¼Œç”± PyTorch è‡ªåŠ¨å®ç°ã€‚\n",
    "\n",
    "#### âœ… `__init__` ä¸­ï¼ˆç¬¬8â€“14è¡Œï¼‰\n",
    "```python\n",
    "encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    batch_first=True,\n",
    "    dropout=0.1\n",
    ")\n",
    "```\n",
    "- **PyTorch æ¨¡å—**ï¼š`torch.nn.TransformerEncoderLayer`\n",
    "- **å†…éƒ¨åŒ…å«**ï¼š\n",
    "  - `nn.MultiheadAttention`\n",
    "  - `nn.LayerNorm`ï¼ˆÃ—2ï¼‰\n",
    "  - `nn.Dropout`ï¼ˆÃ—2ï¼‰\n",
    "  - `nn.Linear`ï¼ˆFFN çš„ä¸¤å±‚ï¼‰\n",
    "  - æ®‹å·®è¿æ¥ï¼ˆ`x + ...`ï¼‰\n",
    "\n",
    "#### âœ… `forward` ä¸­ï¼ˆç¬¬21è¡Œï¼‰\n",
    "```python\n",
    "x = self.transformer_encoder(x)\n",
    "```\n",
    "ä½†å®é™…æ ¸å¿ƒè®¡ç®—å‘ç”Ÿåœ¨ `TransformerEncoderLayer.forward` å†…éƒ¨ã€‚æˆ‘ä»¬å±•å¼€å…¶**å†…éƒ¨æ•°æ®æµ**ï¼š\n",
    "\n",
    "##### ğŸ”¸ å­æµç¨‹ï¼ˆå•å±‚ `TransformerEncoderLayer`ï¼‰ï¼š\n",
    "```python\n",
    "# Step 1: Multi-Head Self-Attention\n",
    "attn_output, _ = self.self_attn(x, x, x)          # Q=K=V=x\n",
    "x = self.norm1(x + self.dropout1(attn_output))    # Residual + LayerNorm\n",
    "\n",
    "# Step 2: Feed-Forward Network\n",
    "ffn_output = self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "x = self.norm2(x + self.dropout2(ffn_output))     # Residual + LayerNorm\n",
    "```\n",
    "\n",
    "| å­ç»„ä»¶ | è¾“å…¥ â†’ è¾“å‡º | PyTorch æ¨¡å— | è¯´æ˜ |\n",
    "|-------|-----------|-------------|------|\n",
    "| **Multi-Head Self-Attention** | `[B, T, D]` â†’ `[B, T, D]` | `nn.MultiheadAttention` | è®¡ç®—ä»»æ„ä¸¤ä½ç½®ç›¸å…³æ€§ |\n",
    "| **Residual + LayerNorm (1)** | `[B, T, D]` â†’ `[B, T, D]` | `nn.LayerNorm`, `+` | ç¨³å®šè®­ç»ƒï¼Œé˜²æ¢¯åº¦æ¶ˆå¤± |\n",
    "| **FFN** | `[B, T, D]` â†’ `[B, T, D]` | `nn.Linear` Ã—2 + `F.relu` | æ¯ä¸ªä½ç½®ç‹¬ç«‹éçº¿æ€§å˜æ¢ |\n",
    "| **Residual + LayerNorm (2)** | `[B, T, D]` â†’ `[B, T, D]` | `nn.LayerNorm`, `+` | åŒä¸Š |\n",
    "\n",
    "> âœ… æ‰€æœ‰è¿™äº›éƒ½åœ¨ **ä¸€è¡Œä»£ç  `self.transformer_encoder(x)` ä¸­è‡ªåŠ¨å®Œæˆ**ï¼\n",
    "\n",
    "---\n",
    "\n",
    "### 4ï¸âƒ£ **Stacked Layersï¼ˆå¤šå±‚å †å ï¼‰**\n",
    "\n",
    "| é¡¹ç›® | è¯´æ˜ |\n",
    "|------|------|\n",
    "| **ä½œç”¨** | å¤šå±‚æŠ½è±¡ï¼Œé€æ­¥æå–é«˜é˜¶ç‰¹å¾ |\n",
    "| **é€šä¿—æ¯”å–»** | â€œå¤šè½®ä¼šè®®â€ï¼šç¬¬ä¸€è½®ç²—ç•¥è®¨è®ºï¼Œç¬¬äºŒè½®æ·±å…¥ç»†èŠ‚ |\n",
    "\n",
    "#### âœ… `__init__` ä¸­ï¼ˆç¬¬15è¡Œï¼‰\n",
    "```python\n",
    "self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "```\n",
    "- **PyTorch æ¨¡å—**ï¼š`torch.nn.TransformerEncoder`\n",
    "- **åŠŸèƒ½**ï¼šå°† `num_layers` ä¸ª `TransformerEncoderLayer` **é¡ºåºå †å **\n",
    "- **å‚æ•°**ï¼šæ¯å±‚æœ‰**ç‹¬ç«‹å‚æ•°**ï¼ˆä¸å…±äº«ï¼‰\n",
    "\n",
    "#### âœ… `forward` ä¸­ï¼ˆç¬¬21è¡Œï¼‰\n",
    "```python\n",
    "x = self.transformer_encoder(x)\n",
    "```\n",
    "- **è¾“å…¥**ï¼š`[B, T, D]`\n",
    "- **å†…éƒ¨æ‰§è¡Œ**ï¼š\n",
    "  ```python\n",
    "  for layer in self.layers:\n",
    "      x = layer(x)   # æ¯å±‚è¾“å‡ºä½œä¸ºä¸‹ä¸€å±‚è¾“å…¥\n",
    "  ```\n",
    "- **è¾“å‡º**ï¼š`[B, T, D]`ï¼ˆç»è¿‡ `num_layers` å±‚å¤„ç†ï¼‰\n",
    "\n",
    "> ğŸ“Œ æ€»æœ‰æ•ˆæ·±åº¦ = `num_layers`ï¼Œå‚æ•°é‡ âˆ `num_layers`\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§¾ å…¨å±€æ•°æ®æµæ€»ç»“ï¼ˆforward è¿‡ç¨‹ï¼‰\n",
    "\n",
    "| æ­¥éª¤ | ä»£ç è¡Œ | è¾“å…¥ â†’ è¾“å‡º | ç»„ä»¶ |\n",
    "|------|-------|------------|------|\n",
    "| 1. Token â†’ Vector | 19 | `[B, T]` â†’ `[B, T, D]` | Embedding |\n",
    "| 2. Add Position | 20 | `[B, T, D]` â†’ `[B, T, D]` | Positional Encoding |\n",
    "| 3. Transformer Block | 21 | `[B, T, D]` â†’ `[B, T, D]` | Multi-Head Attention + FFN + Norm + Residualï¼ˆÃ—num_layersï¼‰ |\n",
    "| 4. Output Logits | 22 | `[B, T, D]` â†’ `[B, T, V]` | Linear Projection |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… æœ€ç»ˆç»“è®ºï¼šç»„ä»¶ â†” ä»£ç æ˜ å°„\n",
    "\n",
    "| æ ¸å¿ƒç»„ä»¶ | `__init__` ä½ç½® | `forward` ä½ç½® | PyTorch æ ‡å‡†æ¨¡å— | è¾“å…¥ â†’ è¾“å‡º |\n",
    "|--------|----------------|---------------|------------------|------------|\n",
    "| **Embedding** | L5 | L19 | `nn.Embedding` | `[B, T]` â†’ `[B, T, D]` |\n",
    "| **Positional Encoding** | L6 | L20 | `nn.Parameter` + å¹¿æ’­ | `[B, T, D]` â†’ åŒå½¢ |\n",
    "| **Multi-Head Self-Attention** | L8â€“14ï¼ˆå†…éƒ¨ï¼‰ | L21ï¼ˆå†…éƒ¨ï¼‰ | `nn.MultiheadAttention` | `[B, T, D]` â†’ åŒå½¢ |\n",
    "| **FFN** | L8â€“14ï¼ˆå†…éƒ¨ï¼‰ | L21ï¼ˆå†…éƒ¨ï¼‰ | `nn.Linear` + `F.relu` | `[B, T, D]` â†’ åŒå½¢ |\n",
    "| **LayerNorm + Residual** | L8â€“14ï¼ˆå†…éƒ¨ï¼‰ | L21ï¼ˆå†…éƒ¨ï¼‰ | `nn.LayerNorm`, `+` | `[B, T, D]` â†’ åŒå½¢ |\n",
    "| **Stacked Layers** | L15 | L21 | `nn.TransformerEncoder` | é‡å¤ N æ¬¡ä¸Šè¿°æµç¨‹ |\n",
    "\n",
    "---\n",
    "\n",
    "> ğŸ’¡ **å…³é”®æ´è§**ï¼š  \n",
    "> **ä½ ä¸éœ€è¦æ‰‹åŠ¨å®ç° Attention æˆ– FFN** â€”â€” PyTorch çš„ `nn.TransformerEncoderLayer` æ˜¯ä¸€ä¸ª**å·¥ä¸šçº§ã€æ ‡å‡†åŒ–ã€é«˜åº¦ä¼˜åŒ–çš„é»‘ç›’**ï¼Œå®ƒå®Œæ•´å®ç°äº† Transformer Encoder å±‚çš„æ‰€æœ‰æ ¸å¿ƒç»„ä»¶ã€‚\n",
    "\n",
    "æŒæ¡è¿™ä¸ªå°è£…ï¼Œä½ å°±æŒæ¡äº† BERTã€æ—¶é—´åºåˆ— Transformerã€Alpha è¡¨è¾¾å¼ç”Ÿæˆç­‰æ¨¡å‹çš„åŸºçŸ³ï¼"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
