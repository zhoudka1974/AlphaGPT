import torch
import torch.nn as nn

class LSTMAutoencoder(nn.Module):
    """
    LSTM Autoencoder for multivariate time series anomaly detection.
    
    Architecture:
        Encoder: Input -> LSTM -> Hidden State (bottleneck)
        Decoder: Hidden State -> Repeat -> LSTM -> Reconstructed Output
    
    Args:
        input_dim (int): Number of features (e.g., 1 for univariate, 5 for multivariate)
        hidden_dim (int): Hidden size of LSTM (bottleneck dimension)
        num_layers (int): Number of LSTM layers (default: 1)
        dropout (float): Dropout rate (default: 0.0)
    """
    def __init__(self, input_dim: int, hidden_dim: int = 64, num_layers: int = 1, dropout: float = 0.0):
        super().__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        # Encoder: compress time series into a latent vector
        self.encoder = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0.0
        )
        
        # Decoder: reconstruct the sequence from latent vector
        self.decoder = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0.0
        )
        
        # Output projection: map decoder output to original feature space
        self.fc_out = nn.Linear(hidden_dim, input_dim)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass: reconstruct input sequence.
        
        Args:
            x (torch.Tensor): Input time series of shape [B, T, D]
                B = batch size
                T = sequence length
                D = number of features (input_dim)
                
        Returns:
            recon_x (torch.Tensor): Reconstructed sequence of shape [B, T, D]
        """
        B, T, D = x.shape
        
        # Encode: get final hidden state
        _, (h_n, c_n) = self.encoder(x)  # h_n: [num_layers, B, hidden_dim]
        
        # Use last layer's hidden state as bottleneck
        latent = h_n[-1]  # [B, hidden_dim]
        
        # Repeat latent vector to match sequence length
        decoder_input = latent.unsqueeze(1).repeat(1, T, 1)  # [B, T, hidden_dim]
        
        # Decode
        decoder_output, _ = self.decoder(decoder_input)
        
        # Project to original feature space
        recon_x = self.fc_out(decoder_output)  # [B, T, D]
        
        return recon_x
    
import torch
import numpy as np

# Simulate 1000 sequences of 50-timestep CPU usage (normal + some anomalies)
np.random.seed(42)
#ç”Ÿæˆä¸€ä¸ªæ¨¡æ‹Ÿçš„â€œæ­£å¸¸â€æ—¶é—´åºåˆ—æ•°æ®é›†,950æ¡æ—¶é—´åºåˆ—.æ¯ä¸€ä¸ªåºåˆ—50ä¸ªæ—¶é—´æ­¥.æ¯ä¸€ä¸ªæ—¶é—´æ­¥æœ‰ä¸€ä¸ªå˜é‡.å…±47500ä¸ªæµ®ç‚¹æ•°
# ä»Žå‡å€¼0.5,æ ‡å‡†å·®0.1çš„æ­£å¤ªåˆ†å¸ƒä¸­éšæœºé‡‡æ ·
# [ 
#  seq0_t0_f0, seq0_t1_f0, ..., seq0_t49_f0,
#  seq1_t0_f0, seq1_t1_f0, ..., seq1_t49_f0,
#  ...
#  seq949_t0_f0, ..., seq949_t49_f0
# ]
# seqX è¡¨ç¤ºç¬¬ X æ¡æ—¶é—´åºåˆ—ï¼ˆå…± 950 æ¡ï¼‰
# tY è¡¨ç¤ºç¬¬ Y ä¸ªæ—¶é—´æ­¥ï¼ˆ0 åˆ° 49ï¼‰
# f0 è¡¨ç¤ºç¬¬ 0 ä¸ªç‰¹å¾ï¼ˆå› ä¸ºåªæœ‰ 1 ä¸ªç‰¹å¾ï¼‰
normal_data = np.random.normal(loc=0.5, scale=0.1, size=(950, 50, 1)) 
anomaly_data = np.random.normal(loc=0.9, scale=0.2, size=(50, 50, 1))  # high CPU
# normal_data:   [æ ·æœ¬0, æ ·æœ¬1, ..., æ ·æœ¬949]     â†’ 950 ä¸ªæ ·æœ¬
# anomaly_data:  [æ ·æœ¬0, æ ·æœ¬1, ..., æ ·æœ¬49]      â†’ 50 ä¸ªæ ·æœ¬
# ------------------------------------------------------------
# data:          [æ ·æœ¬0ï½ž949 (æ­£å¸¸), æ ·æœ¬0ï½ž49 (å¼‚å¸¸)] â†’ 1000 ä¸ªæ ·æœ¬
# è¿™æ˜¯ä¸ªæ‹¼æŽ¥åŠ¨ä½œ,np.concatenate([normal_data, anomaly_data], axis=0) æ—¶ï¼Œé™¤äº†æ‹¼æŽ¥è½´ï¼ˆaxis=0ï¼‰ä¹‹å¤–ï¼Œå…¶ä»–æ‰€æœ‰ç»´åº¦çš„å¤§å°å¿…é¡»å®Œå…¨ä¸€è‡´ã€‚
# åˆå¹¶ä¸¤ä¸ªNumPy æ•°ç»„,ç±»åž‹ä¸ºNumPy ndarray . å½¢çŠ¶(1000,50,1), æ•°æ®ç±»åž‹float64. å­˜å‚¨åœ¨RAMä¸­,ç”±NumPyç®¡ç†
data = np.concatenate([normal_data, anomaly_data], axis=0)
# ç”Ÿæˆä¸€ä¸ªtorch.tensor , ç±»åž‹ä¸ºtorch.float32 , ä¿å­˜åœ¨cpuä¸Š.
data = torch.tensor(data, dtype=torch.float32)

# Create model
# åˆ›å»ºä¸€ä¸ªç”¨äºŽæ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹çš„ LSTM è‡ªç¼–ç å™¨æ¨¡åž‹å®žä¾‹. 
# ä»–çš„forward å…ˆencode , ç„¶åŽdecode , æŠ˜è…¾å•¥?
model = LSTMAutoencoder(input_dim=1, hidden_dim=32)
# å®šä¹‰ä¸€ä¸ªå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æŸå¤±å‡½æ•°ï¼Œå¹¶è®¾ç½®å…¶ä¸è¿›è¡Œèšåˆï¼ˆå³ä¿ç•™æ¯ä¸ªå…ƒç´ çš„æŸå¤±å€¼ï¼‰
criterion = nn.MSELoss(reduction='none')  # per-timestep loss
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# Training loop (unsupervised)
    # å°†æ¨¡åž‹è®¾ä¸ºè®­ç»ƒæ¨¡å¼ã€‚
    # å½±å“å“ªäº›å±‚ï¼Ÿ
            # è™½ç„¶å½“å‰æ¨¡åž‹æ²¡æœ‰ Dropout æˆ– BatchNormï¼Œä½†è¿™æ˜¯æ ‡å‡†åšæ³•ã€‚
            # å¦‚æžœæœªæ¥åŠ å…¥è¿™äº›å±‚ï¼Œtrain() ä¼šå¯ç”¨ dropout å’Œ batch ç»Ÿè®¡ã€‚
    #å¯¹æ¯”ï¼šè¯„ä¼°æ—¶éœ€è°ƒç”¨ model.eval()ã€‚
model.train()
    # è¿›è¡Œ 50 è½®ï¼ˆepochsï¼‰è®­ç»ƒã€‚
    # epoch å«ä¹‰ï¼šéåŽ†æ•´ä¸ªæ•°æ®é›†ä¸€æ¬¡ä¸ºä¸€ä¸ª epochã€‚
    # ä¸ºä»€ä¹ˆ 50ï¼Ÿ
        # å¯¹äºŽç®€å•åˆæˆæ•°æ®ï¼Œ50 è½®é€šå¸¸è¶³å¤Ÿæ”¶æ•›ã€‚
        # å®žé™…é¡¹ç›®ä¸­å¯èƒ½éœ€è¦æ›´å¤šï¼ˆå¦‚ 100ï½ž500ï¼‰ï¼Œæˆ–é…åˆæ—©åœï¼ˆearly stoppingï¼‰ã€‚
for epoch in range(50):
        # åŠŸèƒ½ï¼šå°†æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦æ¸…é›¶ã€‚
        # ä¹ˆéœ€è¦ï¼Ÿ
            # Torch é»˜è®¤ç´¯åŠ æ¢¯åº¦ï¼ˆæ–¹ä¾¿ RNN ç­‰åœºæ™¯ï¼‰ã€‚
            # åå‘ä¼ æ’­å‰å¿…é¡»æ¸…é›¶ï¼Œå¦åˆ™æ¢¯åº¦ä¼šé”™è¯¯ç´¯ç§¯ã€‚
        # å†™æ³•ï¼šmodel.zero_grad()ï¼Œä½†æŽ¨èç”¨ optimizer.zero_grad()    
    optimizer.zero_grad()
        # å°†æ•°æ® data è¾“å…¥æ¨¡åž‹ï¼Œå¾—åˆ°é‡å»ºç»“æžœ reconã€‚
        # è¾“å…¥/è¾“å‡ºå½¢çŠ¶ï¼š
            # data: [1000, 50, 1]ï¼ˆ1000 æ¡åºåˆ—ï¼Œæ¯æ¡ 50 æ­¥ï¼Œå•å˜é‡ï¼‰
            # recon: [1000, 50, 1]ï¼ˆé‡å»ºåŽçš„åºåˆ—ï¼‰
        # æ³¨æ„ï¼š
            # è™½ç„¶ data åŒ…å«æ­£å¸¸+å¼‚å¸¸æ ·æœ¬ï¼Œä½†è‡ªç¼–ç å™¨åœ¨æ— ç›‘ç£è®­ç»ƒä¸­é€šå¸¸åªç”¨æ­£å¸¸æ•°æ®ã€‚
            # æ­¤å¤„ä¸ºäº†ç®€åŒ–æ¼”ç¤ºç”¨äº†å…¨éƒ¨æ•°æ®ï¼Œå®žé™…åº”åªç”¨ normal_data è®­ç»ƒï¼    
    recon = model(data)
        # è®¡ç®—é‡å»ºè¯¯å·®ï¼Œå¹¶å–å¹³å‡ä½œä¸ºæœ€ç»ˆæŸå¤±
        # åˆ†æ­¥è§£æžï¼š
            # criterion(recon, data) â†’ è¿”å›ž [1000, 50, 1] çš„é€å…ƒç´  MSE
            # .mean() â†’ å¯¹æ‰€æœ‰å…ƒç´ æ±‚å¹³å‡ï¼Œå¾—åˆ°æ ‡é‡ loss
    loss = criterion(recon, data).mean()
        # è‡ªåŠ¨è®¡ç®—æŸå¤±å¯¹æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ã€‚
    loss.backward()
        # æ ¹æ®å½“å‰æ¢¯åº¦æ›´æ–°æ¨¡åž‹å‚æ•°ã€‚
        # å†…éƒ¨æ“ä½œï¼ˆä»¥ Adam ä¸ºä¾‹ï¼‰ï¼š
            # è®¡ç®—ä¸€é˜¶/äºŒé˜¶åŠ¨é‡
            # è°ƒæ•´å­¦ä¹ çŽ‡
            # æ‰§è¡Œå‚æ•°æ›´æ–°ï¼šparam = param - lr * grad
         # ðŸ” å®Œæˆä¸€æ¬¡â€œå‰å‘ â†’ æŸå¤± â†’ åå‘ â†’ æ›´æ–°â€é—­çŽ¯ã€‚        
    optimizer.step()
    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.6f}")
        # æ•´ä½“æµç¨‹æ€»ç»“
        # è¡¨æ ¼
        # æ­¥éª¤	ä»£ç 	ç›®çš„
        # . åˆå§‹åŒ–	model, criterion, optimizer	æž„å»ºè®­ç»ƒç»„ä»¶
        # . è®­ç»ƒæ¨¡å¼	model.train()	å¯ç”¨è®­ç»ƒè¡Œä¸º
        # . å¾ªçŽ¯	for epoch in ...	å¤šè½®ä¼˜åŒ–
        # . æ¸…æ¢¯åº¦	optimizer.zero_grad()	é¿å…æ¢¯åº¦ç´¯ç§¯
        # . å‰å‘	recon = model(data)	èŽ·å–é‡å»ºç»“æžœ
        # . æŸå¤±	loss = ... .mean()	é‡åŒ–é‡å»ºè¯¯å·®
        # . åå‘	loss.backward()	è®¡ç®—æ¢¯åº¦
        # . æ›´æ–°	optimizer.step()	ä¼˜åŒ–å‚æ•°
        # . æ—¥å¿—	print(...)	ç›‘æŽ§è®­ç»ƒ



# Anomaly scoring: reconstruction error per sequence
    # å°†æ¨¡åž‹è®¾ä¸ºè¯„ä¼°ï¼ˆæŽ¨ç†ï¼‰æ¨¡å¼ã€‚
    # ä½œç”¨ï¼š
        # å¦‚æžœæ¨¡åž‹åŒ…å« Dropoutã€BatchNorm ç­‰å±‚ï¼Œä¼šç¦ç”¨å®ƒä»¬çš„è®­ç»ƒè¡Œä¸ºï¼ˆå¦‚ dropout éšæœºå¤±æ´»ã€BN ä½¿ç”¨ batch ç»Ÿè®¡ï¼‰ã€‚
        # è™½ç„¶å½“å‰ LSTMAutoencoder æ²¡æœ‰è¿™äº›å±‚ï¼Œä½†è¿™æ˜¯æ ‡å‡†ä¸”å¿…è¦çš„åšæ³•ã€‚
    # å¯¹æ¯”ï¼šè®­ç»ƒæ—¶ç”¨ model.train()ï¼Œè¯„ä¼°æ—¶å¿…é¡»ç”¨ model.eval()ã€‚
model.eval()
    # åœ¨è¯¥ä»£ç å—å†…ä¸è®¡ç®—æˆ–å­˜å‚¨æ¢¯åº¦ã€‚
    # ç›®çš„ï¼š
        # èŠ‚çœå†…å­˜ï¼šé¿å…æž„å»ºè®¡ç®—å›¾ï¼ˆcomputation graphï¼‰
        # åŠ é€ŸæŽ¨ç†ï¼šè·³è¿‡è‡ªåŠ¨å¾®åˆ†çš„å¼€é”€
        # é˜²æ­¢æ„å¤–æ›´æ–°ï¼šç¡®ä¿æ¨¡åž‹å‚æ•°ä¸ä¼šè¢«ä¿®æ”¹
    # é€‚ç”¨åœºæ™¯ï¼šæ‰€æœ‰æŽ¨ç†ï¼ˆinferenceï¼‰æˆ–è¯„ä¼°ï¼ˆevaluationï¼‰ é˜¶æ®µã€‚
        #âš ï¸ åœ¨è®­ç»ƒå¾ªçŽ¯å¤–åšé¢„æµ‹æ—¶ï¼ŒåŠ¡å¿…ä½¿ç”¨ torch.no_grad()ã€‚
with torch.no_grad():
    # å°†æ•´ä¸ªæ•°æ®é›† dataï¼ˆå«æ­£å¸¸+å¼‚å¸¸ï¼‰è¾“å…¥æ¨¡åž‹ï¼Œå¾—åˆ°é‡å»ºåºåˆ—ã€‚
    # è¾“å…¥/è¾“å‡ºå½¢çŠ¶ï¼š
        # data: [1000, 50, 1]ï¼ˆ950 æ­£å¸¸ + 50 å¼‚å¸¸ï¼‰
        # recon: [1000, 50, 1]ï¼ˆæ¨¡åž‹å¯¹æ¯æ¡åºåˆ—çš„é‡å»ºï¼‰
    # å…³é”®å‡è®¾ï¼š
        # æ¨¡åž‹åªåœ¨æ­£å¸¸æ•°æ®ä¸Šè®­ç»ƒè¿‡ â†’ èƒ½å¾ˆå¥½é‡å»ºæ­£å¸¸åºåˆ—
        # å¯¹å¼‚å¸¸åºåˆ—é‡å»ºæ•ˆæžœå·® â†’ é‡å»ºè¯¯å·®å¤§
    # ðŸ“Œ æ³¨æ„ï¼šè¿™é‡Œ data æ˜¯æ··åˆæ•°æ®ï¼Œç”¨äºŽæµ‹è¯•æ¨¡åž‹æ³›åŒ–èƒ½åŠ›ã€‚    
    recon = model(data)
    # è®¡ç®—æ¯æ¡æ—¶é—´åºåˆ—çš„å¹³å‡å¹³æ–¹é‡å»ºè¯¯å·®ã€‚
    # åˆ†æ­¥è§£æžï¼š
        # (recon - data) â†’ é€å…ƒç´ è¯¯å·®ï¼Œshape [1000, 50, 1]
        # ** 2 â†’ å¹³æ–¹è¯¯å·®ï¼ˆç­‰ä»·äºŽ MSE çš„åˆ†å­ï¼‰
        # torch.mean(..., dim=[1, 2]) â†’ å¯¹ æ—¶é—´æ­¥ï¼ˆdim=1ï¼‰å’Œç‰¹å¾ï¼ˆdim=2ï¼‰æ±‚å¹³å‡
    # ç»“æžœï¼š
        # seq_errors æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º 1000 çš„ä¸€ç»´å¼ é‡
        # seq_errors[i] è¡¨ç¤ºç¬¬ i æ¡åºåˆ—çš„æ•´ä½“é‡å»ºè¯¯å·®
    # âœ… è¿™å°±æ˜¯å¼‚å¸¸è¯„åˆ†ï¼ˆanomaly scoreï¼‰ï¼šè¯¯å·®è¶Šå¤§ï¼Œè¶Šå¯èƒ½æ˜¯å¼‚å¸¸ï¼    
    seq_errors = torch.mean((recon - data) ** 2, dim=[1, 2])  # [1000]

# Threshold: top 5% as anomalies
threshold = torch.quantile(seq_errors, 0.95)
anomaly_pred = seq_errors > threshold

print(f"Detected {anomaly_pred.sum().item()} anomalies (expected ï½ž50)")