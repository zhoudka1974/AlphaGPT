{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f65fe806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa390ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocab_size = 1000\n",
    "d_model = 512      # 嵌入维度，也是 Transformer 的模型维度\n",
    "nhead = 8          # 多头注意力头数（必须整除 d_model）\n",
    "seq_len = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0b0573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Embedding 层：将 token IDs 转为 dense vectors\n",
    "embedding = nn.Embedding(vocab_size, d_model)  # 输出: (batch, seq, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b19fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 输入 token IDs（假设已 padding）\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))  # (32, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62dc38e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = embedding(input_ids)  # shape: (32, 10, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c79f92ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=d_model,   # ← 必须等于 embedding 的 dim\n",
    "    nhead=nhead,       # ← 决定内部如何拆分注意力头\n",
    "    batch_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "553d6fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = encoder_layer(x)  # shape: (32, 10, 512) —— 维度不变！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59c449db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "631207e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd727201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1647, -0.3085,  1.0952,  ..., -0.4182,  0.6224,  0.9236],\n",
      "         [-0.5490, -0.3168, -0.8426,  ...,  0.2688, -0.2423, -0.3333],\n",
      "         [ 0.6672,  0.3188,  0.3514,  ...,  1.6089,  1.0639,  0.3676],\n",
      "         ...,\n",
      "         [ 0.4677, -1.6339, -0.2655,  ..., -0.4746, -0.6595, -0.5544],\n",
      "         [-0.4466,  0.0138, -3.4896,  ...,  1.0505, -0.8704, -1.1786],\n",
      "         [-0.0698, -1.0421,  1.4685,  ...,  1.1292,  0.5555,  0.2181]],\n",
      "\n",
      "        [[ 0.3979,  2.1153,  0.8561,  ..., -1.0947,  0.7962,  0.8197],\n",
      "         [ 0.6449, -0.0062,  0.6563,  ..., -0.8089,  1.1921,  0.0762],\n",
      "         [-0.1759,  0.7744, -0.0060,  ..., -0.5796,  0.1568,  0.3155],\n",
      "         ...,\n",
      "         [-1.2795,  0.8664,  0.0287,  ..., -1.6289,  0.7865,  0.1139],\n",
      "         [ 0.4173,  1.0536, -1.9627,  ..., -1.2910, -1.5000, -1.2034],\n",
      "         [ 1.1759, -0.3368, -0.6095,  ..., -0.8134, -1.5944,  1.0804]],\n",
      "\n",
      "        [[-0.1705,  0.3860,  1.5224,  ...,  0.5696, -0.3932, -0.5377],\n",
      "         [-1.3352,  0.4999, -0.4728,  ..., -1.1892, -1.1342, -1.3494],\n",
      "         [ 1.1260,  1.4937, -0.5930,  ..., -0.0462,  0.3977, -1.2844],\n",
      "         ...,\n",
      "         [-1.3281, -0.1322,  1.7451,  ..., -0.1288, -0.2465, -0.5264],\n",
      "         [-0.5047, -0.9707, -0.2057,  ..., -1.7834,  1.2935,  0.2431],\n",
      "         [-0.9428,  0.6050,  1.2566,  ..., -0.6258, -0.5169, -0.2051]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.7356,  1.5225,  0.2606,  ...,  0.1133,  0.1233,  0.2332],\n",
      "         [-0.3095,  0.4810,  0.1408,  ..., -1.0550,  1.5836, -2.3193],\n",
      "         [ 0.8469, -0.1491,  0.5732,  ..., -0.2771,  0.9892, -1.0674],\n",
      "         ...,\n",
      "         [-0.0514, -0.2637,  2.6024,  ...,  0.3364, -1.4220, -0.0858],\n",
      "         [ 0.7393, -0.9721,  1.2484,  ..., -0.7959,  0.8566, -0.2631],\n",
      "         [-1.3866, -0.9593, -2.4290,  ...,  0.6825,  0.1806,  1.0062]],\n",
      "\n",
      "        [[ 0.1039,  0.5842, -1.2747,  ...,  1.1234, -1.0306, -0.4419],\n",
      "         [-1.7056,  0.4825,  1.3957,  ..., -1.1553, -0.3188,  1.2819],\n",
      "         [ 0.3814,  1.1105,  0.2157,  ..., -0.4680, -0.1315, -0.0779],\n",
      "         ...,\n",
      "         [-0.1473, -0.2576, -0.6166,  ...,  0.2569,  0.6976, -0.8330],\n",
      "         [ 0.9741,  0.7218,  1.5759,  ..., -0.2672,  1.0493, -1.5656],\n",
      "         [ 0.5367, -0.1942,  0.4901,  ...,  0.1051, -0.4038,  0.2091]],\n",
      "\n",
      "        [[-0.3291,  0.8381,  0.2100,  ..., -0.3790,  0.2874,  0.5668],\n",
      "         [ 0.4506, -1.6470,  0.6543,  ..., -1.6707,  0.0779,  1.6600],\n",
      "         [-0.5020,  0.1338, -0.5007,  ...,  1.5349,  0.1191, -0.7321],\n",
      "         ...,\n",
      "         [ 0.6674,  0.4252,  0.6083,  ...,  1.1171,  0.3459,  0.8429],\n",
      "         [-0.2402, -0.4786,  1.6736,  ...,  1.0810, -1.0686, -0.2074],\n",
      "         [-0.4317,  0.9968,  1.4742,  ..., -0.4058, -1.8742,  2.5378]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c379c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding = nn.Parameter(torch.zeros(1, seq_len, d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0697b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(pos_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "001fb7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "print(pos_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e90473e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efdba940",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 训练前\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBefore training:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mmodel\u001b[49m.pos_encoding[\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, :\u001b[32m3\u001b[39m])  \u001b[38;5;66;03m# 前3维\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 输出: tensor([0., 0., 0.], grad_fn=<SliceBackward0>)\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 训练一个 batch\u001b[39;00m\n\u001b[32m      6\u001b[39m optimizer.zero_grad()\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# 训练前\n",
    "print(\"Before training:\", model.pos_encoding[0, 0, :3])  # 前3维\n",
    "# 输出: tensor([0., 0., 0.], grad_fn=<SliceBackward0>)\n",
    "\n",
    "# 训练一个 batch\n",
    "optimizer.zero_grad()\n",
    "logits = model(train_src[:2])\n",
    "loss = criterion(logits.view(-1, vocab_size), train_tgt[:2].view(-1))\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# 训练后\n",
    "print(\"After training:\", model.pos_encoding[0, 0, :3])\n",
    "# 输出: tensor([-0.0021, 0.0045, -0.0012], ...) ← 不再是 0！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3f64717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.1000, 15.2000],\n",
      "        [ 5.1000, 14.2000]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建线性层：输入3维 → 输出2维\n",
    "linear = nn.Linear(in_features=3, out_features=2, bias=True)\n",
    "\n",
    "# 手动设置参数（仅用于演示）\n",
    "with torch.no_grad():\n",
    "    linear.weight = nn.Parameter(torch.tensor([[1.0, 2.0, 3.0],\n",
    "                                              [4.0, 5.0, 6.0]]))  # shape [2, 3]\n",
    "    linear.bias = nn.Parameter(torch.tensor([0.1, 0.2]))            # shape [2]\n",
    "\n",
    "# 输入: [2, 3] （batch=2, feature=3）\n",
    "x = torch.tensor([[1.0, 1.0, 1.0],\n",
    "                  [2.0, 0.0, 1.0]])\n",
    "\n",
    "# 前向计算\n",
    "y = linear(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b6e269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(4, 2)\n",
    "x = torch.randn(4)          # shape: [4]\n",
    "y = linear(x)               # shape: [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab4d8acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0554, -0.0902,  0.9343, -1.6278])\n",
      "tensor([-1.0755, -0.3322], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e25099c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
