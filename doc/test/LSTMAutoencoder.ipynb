{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa051e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Autoencoder for multivariate time series anomaly detection.\n",
    "    \n",
    "    Architecture:\n",
    "        Encoder: Input -> LSTM -> Hidden State (bottleneck)\n",
    "        Decoder: Hidden State -> Repeat -> LSTM -> Reconstructed Output\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): Number of features (e.g., 1 for univariate, 5 for multivariate)\n",
    "        hidden_dim (int): Hidden size of LSTM (bottleneck dimension)\n",
    "        num_layers (int): Number of LSTM layers (default: 1)\n",
    "        dropout (float): Dropout rate (default: 0.0)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 64, num_layers: int = 1, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Encoder: compress time series into a latent vector\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        \n",
    "        # Decoder: reconstruct the sequence from latent vector\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        \n",
    "        # Output projection: map decoder output to original feature space\n",
    "        self.fc_out = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: reconstruct input sequence.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input time series of shape [B, T, D]\n",
    "                B = batch size\n",
    "                T = sequence length\n",
    "                D = number of features (input_dim)\n",
    "                \n",
    "        Returns:\n",
    "            recon_x (torch.Tensor): Reconstructed sequence of shape [B, T, D]\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        \n",
    "        # Encode: get final hidden state\n",
    "        _, (h_n, c_n) = self.encoder(x)  # h_n: [num_layers, B, hidden_dim]\n",
    "        \n",
    "        # Use last layer's hidden state as bottleneck\n",
    "        latent = h_n[-1]  # [B, hidden_dim]\n",
    "        \n",
    "        # Repeat latent vector to match sequence length\n",
    "        decoder_input = latent.unsqueeze(1).repeat(1, T, 1)  # [B, T, hidden_dim]\n",
    "        \n",
    "        # Decode\n",
    "        decoder_output, _ = self.decoder(decoder_input)\n",
    "        \n",
    "        # Project to original feature space\n",
    "        recon_x = self.fc_out(decoder_output)  # [B, T, D]\n",
    "        \n",
    "        return recon_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abdfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.338000\n",
      "Epoch 10, Loss: 0.182544\n",
      "Epoch 20, Loss: 0.037992\n",
      "Epoch 30, Loss: 0.032330\n",
      "Epoch 40, Loss: 0.025277\n",
      "Detected 50 anomalies (expected ～50)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Simulate 1000 sequences of 50-timestep CPU usage (normal + some anomalies)\n",
    "np.random.seed(42)\n",
    "#生成一个模拟的“正常”时间序列数据集,950条时间序列.每一个序列50个时间步.每一个时间步有一个变量.共47500个浮点数\n",
    "# 从均值0.5,标准差0.1的正太分布中随机采样\n",
    "# [ \n",
    "#  seq0_t0_f0, seq0_t1_f0, ..., seq0_t49_f0,\n",
    "#  seq1_t0_f0, seq1_t1_f0, ..., seq1_t49_f0,\n",
    "#  ...\n",
    "#  seq949_t0_f0, ..., seq949_t49_f0\n",
    "# ]\n",
    "# seqX 表示第 X 条时间序列（共 950 条）\n",
    "# tY 表示第 Y 个时间步（0 到 49）\n",
    "# f0 表示第 0 个特征（因为只有 1 个特征）\n",
    "normal_data = np.random.normal(loc=0.5, scale=0.1, size=(950, 50, 1)) \n",
    "anomaly_data = np.random.normal(loc=0.9, scale=0.2, size=(50, 50, 1))  # high CPU\n",
    "# normal_data:   [样本0, 样本1, ..., 样本949]     → 950 个样本\n",
    "# anomaly_data:  [样本0, 样本1, ..., 样本49]      → 50 个样本\n",
    "# ------------------------------------------------------------\n",
    "# data:          [样本0～949 (正常), 样本0～49 (异常)] → 1000 个样本\n",
    "# 这是个拼接动作,np.concatenate([normal_data, anomaly_data], axis=0) 时，除了拼接轴（axis=0）之外，其他所有维度的大小必须完全一致。\n",
    "# 合并两个NumPy 数组,类型为NumPy ndarray . 形状(1000,50,1), 数据类型float64. 存储在RAM中,由NumPy管理\n",
    "data = np.concatenate([normal_data, anomaly_data], axis=0)\n",
    "# 生成一个torch.tensor , 类型为torch.float32 , 保存在cpu上.\n",
    "data = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "# Create model\n",
    "model = LSTMAutoencoder(input_dim=1, hidden_dim=32)\n",
    "criterion = nn.MSELoss(reduction='none')  # per-timestep loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop (unsupervised)\n",
    "model.train()\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    recon = model(data)\n",
    "    loss = criterion(recon, data).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "# Anomaly scoring: reconstruction error per sequence\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    recon = model(data)\n",
    "    seq_errors = torch.mean((recon - data) ** 2, dim=[1, 2])  # [1000]\n",
    "\n",
    "# Threshold: top 5% as anomalies\n",
    "threshold = torch.quantile(seq_errors, 0.95)\n",
    "anomaly_pred = seq_errors > threshold\n",
    "\n",
    "print(f\"Detected {anomaly_pred.sum().item()} anomalies (expected ～50)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70be77d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(normal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14078c82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
