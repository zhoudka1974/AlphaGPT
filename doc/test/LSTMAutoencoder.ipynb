{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa051e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Autoencoder for multivariate time series anomaly detection.\n",
    "    \n",
    "    Architecture:\n",
    "        Encoder: Input -> LSTM -> Hidden State (bottleneck)\n",
    "        Decoder: Hidden State -> Repeat -> LSTM -> Reconstructed Output\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): Number of features (e.g., 1 for univariate, 5 for multivariate)\n",
    "        hidden_dim (int): Hidden size of LSTM (bottleneck dimension)\n",
    "        num_layers (int): Number of LSTM layers (default: 1)\n",
    "        dropout (float): Dropout rate (default: 0.0)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 64, num_layers: int = 1, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Encoder: compress time series into a latent vector\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        \n",
    "        # Decoder: reconstruct the sequence from latent vector\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        \n",
    "        # Output projection: map decoder output to original feature space\n",
    "        self.fc_out = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: reconstruct input sequence.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input time series of shape [B, T, D]\n",
    "                B = batch size\n",
    "                T = sequence length\n",
    "                D = number of features (input_dim)\n",
    "                \n",
    "        Returns:\n",
    "            recon_x (torch.Tensor): Reconstructed sequence of shape [B, T, D]\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        \n",
    "        # Encode: get final hidden state\n",
    "        _, (h_n, c_n) = self.encoder(x)  # h_n: [num_layers, B, hidden_dim]\n",
    "        \n",
    "        # Use last layer's hidden state as bottleneck\n",
    "        latent = h_n[-1]  # [B, hidden_dim]\n",
    "        \n",
    "        # Repeat latent vector to match sequence length\n",
    "        decoder_input = latent.unsqueeze(1).repeat(1, T, 1)  # [B, T, hidden_dim]\n",
    "        \n",
    "        # Decode\n",
    "        decoder_output, _ = self.decoder(decoder_input)\n",
    "        \n",
    "        # Project to original feature space\n",
    "        recon_x = self.fc_out(decoder_output)  # [B, T, D]\n",
    "        \n",
    "        return recon_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abdfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Simulate 1000 sequences of 50-timestep CPU usage (normal + some anomalies)\n",
    "np.random.seed(42)\n",
    "#ç”Ÿæˆä¸€ä¸ªæ¨¡æ‹Ÿçš„â€œæ­£å¸¸â€æ—¶é—´åºåˆ—æ•°æ®é›†,950æ¡æ—¶é—´åºåˆ—.æ¯ä¸€ä¸ªåºåˆ—50ä¸ªæ—¶é—´æ­¥.æ¯ä¸€ä¸ªæ—¶é—´æ­¥æœ‰ä¸€ä¸ªå˜é‡.å…±47500ä¸ªæµ®ç‚¹æ•°\n",
    "# ä»Žå‡å€¼0.5,æ ‡å‡†å·®0.1çš„æ­£å¤ªåˆ†å¸ƒä¸­éšæœºé‡‡æ ·\n",
    "# [ \n",
    "#  seq0_t0_f0, seq0_t1_f0, ..., seq0_t49_f0,\n",
    "#  seq1_t0_f0, seq1_t1_f0, ..., seq1_t49_f0,\n",
    "#  ...\n",
    "#  seq949_t0_f0, ..., seq949_t49_f0\n",
    "# ]\n",
    "# seqX è¡¨ç¤ºç¬¬ X æ¡æ—¶é—´åºåˆ—ï¼ˆå…± 950 æ¡ï¼‰\n",
    "# tY è¡¨ç¤ºç¬¬ Y ä¸ªæ—¶é—´æ­¥ï¼ˆ0 åˆ° 49ï¼‰\n",
    "# f0 è¡¨ç¤ºç¬¬ 0 ä¸ªç‰¹å¾ï¼ˆå› ä¸ºåªæœ‰ 1 ä¸ªç‰¹å¾ï¼‰\n",
    "normal_data = np.random.normal(loc=0.5, scale=0.1, size=(950, 50, 1)) \n",
    "anomaly_data = np.random.normal(loc=0.9, scale=0.2, size=(50, 50, 1))  # high CPU\n",
    "# normal_data:   [æ ·æœ¬0, æ ·æœ¬1, ..., æ ·æœ¬949]     â†’ 950 ä¸ªæ ·æœ¬\n",
    "# anomaly_data:  [æ ·æœ¬0, æ ·æœ¬1, ..., æ ·æœ¬49]      â†’ 50 ä¸ªæ ·æœ¬\n",
    "# ------------------------------------------------------------\n",
    "# data:          [æ ·æœ¬0ï½ž949 (æ­£å¸¸), æ ·æœ¬0ï½ž49 (å¼‚å¸¸)] â†’ 1000 ä¸ªæ ·æœ¬\n",
    "# è¿™æ˜¯ä¸ªæ‹¼æŽ¥åŠ¨ä½œ,np.concatenate([normal_data, anomaly_data], axis=0) æ—¶ï¼Œé™¤äº†æ‹¼æŽ¥è½´ï¼ˆaxis=0ï¼‰ä¹‹å¤–ï¼Œå…¶ä»–æ‰€æœ‰ç»´åº¦çš„å¤§å°å¿…é¡»å®Œå…¨ä¸€è‡´ã€‚\n",
    "# åˆå¹¶ä¸¤ä¸ªNumPy æ•°ç»„,ç±»åž‹ä¸ºNumPy ndarray . å½¢çŠ¶(1000,50,1), æ•°æ®ç±»åž‹float64. å­˜å‚¨åœ¨RAMä¸­,ç”±NumPyç®¡ç†\n",
    "data = np.concatenate([normal_data, anomaly_data], axis=0)\n",
    "# ç”Ÿæˆä¸€ä¸ªtorch.tensor , ç±»åž‹ä¸ºtorch.float32 , ä¿å­˜åœ¨cpuä¸Š.\n",
    "data = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "# Create model\n",
    "# åˆ›å»ºä¸€ä¸ªç”¨äºŽæ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹çš„ LSTM è‡ªç¼–ç å™¨æ¨¡åž‹å®žä¾‹. \n",
    "# ä»–çš„forward å…ˆencode , ç„¶åŽdecode , æŠ˜è…¾å•¥?\n",
    "model = LSTMAutoencoder(input_dim=1, hidden_dim=32)\n",
    "# å®šä¹‰ä¸€ä¸ªå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æŸå¤±å‡½æ•°ï¼Œå¹¶è®¾ç½®å…¶ä¸è¿›è¡Œèšåˆï¼ˆå³ä¿ç•™æ¯ä¸ªå…ƒç´ çš„æŸå¤±å€¼ï¼‰\n",
    "criterion = nn.MSELoss(reduction='none')  # per-timestep loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop (unsupervised)\n",
    "    # å°†æ¨¡åž‹è®¾ä¸ºè®­ç»ƒæ¨¡å¼ã€‚\n",
    "    # å½±å“å“ªäº›å±‚ï¼Ÿ\n",
    "            # è™½ç„¶å½“å‰æ¨¡åž‹æ²¡æœ‰ Dropout æˆ– BatchNormï¼Œä½†è¿™æ˜¯æ ‡å‡†åšæ³•ã€‚\n",
    "            # å¦‚æžœæœªæ¥åŠ å…¥è¿™äº›å±‚ï¼Œtrain() ä¼šå¯ç”¨ dropout å’Œ batch ç»Ÿè®¡ã€‚\n",
    "    #å¯¹æ¯”ï¼šè¯„ä¼°æ—¶éœ€è°ƒç”¨ model.eval()ã€‚\n",
    "model.train()\n",
    "    # è¿›è¡Œ 50 è½®ï¼ˆepochsï¼‰è®­ç»ƒã€‚\n",
    "    # epoch å«ä¹‰ï¼šéåŽ†æ•´ä¸ªæ•°æ®é›†ä¸€æ¬¡ä¸ºä¸€ä¸ª epochã€‚\n",
    "    # ä¸ºä»€ä¹ˆ 50ï¼Ÿ\n",
    "        # å¯¹äºŽç®€å•åˆæˆæ•°æ®ï¼Œ50 è½®é€šå¸¸è¶³å¤Ÿæ”¶æ•›ã€‚\n",
    "        # å®žé™…é¡¹ç›®ä¸­å¯èƒ½éœ€è¦æ›´å¤šï¼ˆå¦‚ 100ï½ž500ï¼‰ï¼Œæˆ–é…åˆæ—©åœï¼ˆearly stoppingï¼‰ã€‚\n",
    "for epoch in range(50):\n",
    "        # åŠŸèƒ½ï¼šå°†æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦æ¸…é›¶ã€‚\n",
    "        # ä¹ˆéœ€è¦ï¼Ÿ\n",
    "            # Torch é»˜è®¤ç´¯åŠ æ¢¯åº¦ï¼ˆæ–¹ä¾¿ RNN ç­‰åœºæ™¯ï¼‰ã€‚\n",
    "            # åå‘ä¼ æ’­å‰å¿…é¡»æ¸…é›¶ï¼Œå¦åˆ™æ¢¯åº¦ä¼šé”™è¯¯ç´¯ç§¯ã€‚\n",
    "        # å†™æ³•ï¼šmodel.zero_grad()ï¼Œä½†æŽ¨èç”¨ optimizer.zero_grad()    \n",
    "    optimizer.zero_grad()\n",
    "        # å°†æ•°æ® data è¾“å…¥æ¨¡åž‹ï¼Œå¾—åˆ°é‡å»ºç»“æžœ reconã€‚\n",
    "        # è¾“å…¥/è¾“å‡ºå½¢çŠ¶ï¼š\n",
    "            # data: [1000, 50, 1]ï¼ˆ1000 æ¡åºåˆ—ï¼Œæ¯æ¡ 50 æ­¥ï¼Œå•å˜é‡ï¼‰\n",
    "            # recon: [1000, 50, 1]ï¼ˆé‡å»ºåŽçš„åºåˆ—ï¼‰\n",
    "        # æ³¨æ„ï¼š\n",
    "            # è™½ç„¶ data åŒ…å«æ­£å¸¸+å¼‚å¸¸æ ·æœ¬ï¼Œä½†è‡ªç¼–ç å™¨åœ¨æ— ç›‘ç£è®­ç»ƒä¸­é€šå¸¸åªç”¨æ­£å¸¸æ•°æ®ã€‚\n",
    "            # æ­¤å¤„ä¸ºäº†ç®€åŒ–æ¼”ç¤ºç”¨äº†å…¨éƒ¨æ•°æ®ï¼Œå®žé™…åº”åªç”¨ normal_data è®­ç»ƒï¼    \n",
    "    recon = model(data)\n",
    "        # è®¡ç®—é‡å»ºè¯¯å·®ï¼Œå¹¶å–å¹³å‡ä½œä¸ºæœ€ç»ˆæŸå¤±\n",
    "        # åˆ†æ­¥è§£æžï¼š\n",
    "            # criterion(recon, data) â†’ è¿”å›ž [1000, 50, 1] çš„é€å…ƒç´  MSE\n",
    "            # .mean() â†’ å¯¹æ‰€æœ‰å…ƒç´ æ±‚å¹³å‡ï¼Œå¾—åˆ°æ ‡é‡ loss\n",
    "    loss = criterion(recon, data).mean()\n",
    "        # è‡ªåŠ¨è®¡ç®—æŸå¤±å¯¹æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ã€‚\n",
    "    loss.backward()\n",
    "        # æ ¹æ®å½“å‰æ¢¯åº¦æ›´æ–°æ¨¡åž‹å‚æ•°ã€‚\n",
    "        # å†…éƒ¨æ“ä½œï¼ˆä»¥ Adam ä¸ºä¾‹ï¼‰ï¼š\n",
    "            # è®¡ç®—ä¸€é˜¶/äºŒé˜¶åŠ¨é‡\n",
    "            # è°ƒæ•´å­¦ä¹ çŽ‡\n",
    "            # æ‰§è¡Œå‚æ•°æ›´æ–°ï¼šparam = param - lr * grad\n",
    "         # ðŸ” å®Œæˆä¸€æ¬¡â€œå‰å‘ â†’ æŸå¤± â†’ åå‘ â†’ æ›´æ–°â€é—­çŽ¯ã€‚        \n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
    "        # æ•´ä½“æµç¨‹æ€»ç»“\n",
    "        # è¡¨æ ¼\n",
    "        # æ­¥éª¤\tä»£ç \tç›®çš„\n",
    "        # . åˆå§‹åŒ–\tmodel, criterion, optimizer\tæž„å»ºè®­ç»ƒç»„ä»¶\n",
    "        # . è®­ç»ƒæ¨¡å¼\tmodel.train()\tå¯ç”¨è®­ç»ƒè¡Œä¸º\n",
    "        # . å¾ªçŽ¯\tfor epoch in ...\tå¤šè½®ä¼˜åŒ–\n",
    "        # . æ¸…æ¢¯åº¦\toptimizer.zero_grad()\té¿å…æ¢¯åº¦ç´¯ç§¯\n",
    "        # . å‰å‘\trecon = model(data)\tèŽ·å–é‡å»ºç»“æžœ\n",
    "        # . æŸå¤±\tloss = ... .mean()\té‡åŒ–é‡å»ºè¯¯å·®\n",
    "        # . åå‘\tloss.backward()\tè®¡ç®—æ¢¯åº¦\n",
    "        # . æ›´æ–°\toptimizer.step()\tä¼˜åŒ–å‚æ•°\n",
    "        # . æ—¥å¿—\tprint(...)\tç›‘æŽ§è®­ç»ƒ\n",
    "\n",
    "\n",
    "\n",
    "# Anomaly scoring: reconstruction error per sequence\n",
    "    # å°†æ¨¡åž‹è®¾ä¸ºè¯„ä¼°ï¼ˆæŽ¨ç†ï¼‰æ¨¡å¼ã€‚\n",
    "    # ä½œç”¨ï¼š\n",
    "        # å¦‚æžœæ¨¡åž‹åŒ…å« Dropoutã€BatchNorm ç­‰å±‚ï¼Œä¼šç¦ç”¨å®ƒä»¬çš„è®­ç»ƒè¡Œä¸ºï¼ˆå¦‚ dropout éšæœºå¤±æ´»ã€BN ä½¿ç”¨ batch ç»Ÿè®¡ï¼‰ã€‚\n",
    "        # è™½ç„¶å½“å‰ LSTMAutoencoder æ²¡æœ‰è¿™äº›å±‚ï¼Œä½†è¿™æ˜¯æ ‡å‡†ä¸”å¿…è¦çš„åšæ³•ã€‚\n",
    "    # å¯¹æ¯”ï¼šè®­ç»ƒæ—¶ç”¨ model.train()ï¼Œè¯„ä¼°æ—¶å¿…é¡»ç”¨ model.eval()ã€‚\n",
    "model.eval()\n",
    "    # åœ¨è¯¥ä»£ç å—å†…ä¸è®¡ç®—æˆ–å­˜å‚¨æ¢¯åº¦ã€‚\n",
    "    # ç›®çš„ï¼š\n",
    "        # èŠ‚çœå†…å­˜ï¼šé¿å…æž„å»ºè®¡ç®—å›¾ï¼ˆcomputation graphï¼‰\n",
    "        # åŠ é€ŸæŽ¨ç†ï¼šè·³è¿‡è‡ªåŠ¨å¾®åˆ†çš„å¼€é”€\n",
    "        # é˜²æ­¢æ„å¤–æ›´æ–°ï¼šç¡®ä¿æ¨¡åž‹å‚æ•°ä¸ä¼šè¢«ä¿®æ”¹\n",
    "    # é€‚ç”¨åœºæ™¯ï¼šæ‰€æœ‰æŽ¨ç†ï¼ˆinferenceï¼‰æˆ–è¯„ä¼°ï¼ˆevaluationï¼‰ é˜¶æ®µã€‚\n",
    "        #âš ï¸ åœ¨è®­ç»ƒå¾ªçŽ¯å¤–åšé¢„æµ‹æ—¶ï¼ŒåŠ¡å¿…ä½¿ç”¨ torch.no_grad()ã€‚\n",
    "with torch.no_grad():\n",
    "    # å°†æ•´ä¸ªæ•°æ®é›† dataï¼ˆå«æ­£å¸¸+å¼‚å¸¸ï¼‰è¾“å…¥æ¨¡åž‹ï¼Œå¾—åˆ°é‡å»ºåºåˆ—ã€‚\n",
    "    # è¾“å…¥/è¾“å‡ºå½¢çŠ¶ï¼š\n",
    "        # data: [1000, 50, 1]ï¼ˆ950 æ­£å¸¸ + 50 å¼‚å¸¸ï¼‰\n",
    "        # recon: [1000, 50, 1]ï¼ˆæ¨¡åž‹å¯¹æ¯æ¡åºåˆ—çš„é‡å»ºï¼‰\n",
    "    # å…³é”®å‡è®¾ï¼š\n",
    "        # æ¨¡åž‹åªåœ¨æ­£å¸¸æ•°æ®ä¸Šè®­ç»ƒè¿‡ â†’ èƒ½å¾ˆå¥½é‡å»ºæ­£å¸¸åºåˆ—\n",
    "        # å¯¹å¼‚å¸¸åºåˆ—é‡å»ºæ•ˆæžœå·® â†’ é‡å»ºè¯¯å·®å¤§\n",
    "    # ðŸ“Œ æ³¨æ„ï¼šè¿™é‡Œ data æ˜¯æ··åˆæ•°æ®ï¼Œç”¨äºŽæµ‹è¯•æ¨¡åž‹æ³›åŒ–èƒ½åŠ›ã€‚    \n",
    "    recon = model(data)\n",
    "    # è®¡ç®—æ¯æ¡æ—¶é—´åºåˆ—çš„å¹³å‡å¹³æ–¹é‡å»ºè¯¯å·®ã€‚\n",
    "    # åˆ†æ­¥è§£æžï¼š\n",
    "        # (recon - data) â†’ é€å…ƒç´ è¯¯å·®ï¼Œshape [1000, 50, 1]\n",
    "        # ** 2 â†’ å¹³æ–¹è¯¯å·®ï¼ˆç­‰ä»·äºŽ MSE çš„åˆ†å­ï¼‰\n",
    "        # torch.mean(..., dim=[1, 2]) â†’ å¯¹ æ—¶é—´æ­¥ï¼ˆdim=1ï¼‰å’Œç‰¹å¾ï¼ˆdim=2ï¼‰æ±‚å¹³å‡\n",
    "    # ç»“æžœï¼š\n",
    "        # seq_errors æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º 1000 çš„ä¸€ç»´å¼ é‡\n",
    "        # seq_errors[i] è¡¨ç¤ºç¬¬ i æ¡åºåˆ—çš„æ•´ä½“é‡å»ºè¯¯å·®\n",
    "    # âœ… è¿™å°±æ˜¯å¼‚å¸¸è¯„åˆ†ï¼ˆanomaly scoreï¼‰ï¼šè¯¯å·®è¶Šå¤§ï¼Œè¶Šå¯èƒ½æ˜¯å¼‚å¸¸ï¼    \n",
    "    seq_errors = torch.mean((recon - data) ** 2, dim=[1, 2])  # [1000]\n",
    "\n",
    "# Threshold: top 5% as anomalies\n",
    "threshold = torch.quantile(seq_errors, 0.95)\n",
    "anomaly_pred = seq_errors > threshold\n",
    "\n",
    "print(f\"Detected {anomaly_pred.sum().item()} anomalies (expected ï½ž50)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8751bd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0156, 0.0111, 0.0117, 0.0111, 0.0159, 0.0159, 0.0082, 0.0138, 0.0124,\n",
      "        0.0150, 0.0160, 0.0124, 0.0168, 0.0169, 0.0127, 0.0098, 0.0116, 0.0097,\n",
      "        0.0129, 0.0132, 0.0134, 0.0119, 0.0126, 0.0145, 0.0131, 0.0134, 0.0111,\n",
      "        0.0119, 0.0106, 0.0165, 0.0150, 0.0148, 0.0130, 0.0113, 0.0112, 0.0117,\n",
      "        0.0142, 0.0122, 0.0163, 0.0109, 0.0136, 0.0117, 0.0123, 0.0144, 0.0150,\n",
      "        0.0136, 0.0119, 0.0164, 0.0101, 0.0098, 0.0163, 0.0123, 0.0086, 0.0124,\n",
      "        0.0156, 0.0136, 0.0166, 0.0188, 0.0107, 0.0161, 0.0146, 0.0107, 0.0133,\n",
      "        0.0158, 0.0153, 0.0148, 0.0128, 0.0156, 0.0127, 0.0145, 0.0169, 0.0139,\n",
      "        0.0109, 0.0100, 0.0200, 0.0126, 0.0104, 0.0132, 0.0127, 0.0183, 0.0158,\n",
      "        0.0107, 0.0116, 0.0156, 0.0165, 0.0148, 0.0109, 0.0105, 0.0171, 0.0120,\n",
      "        0.0081, 0.0134, 0.0199, 0.0111, 0.0096, 0.0180, 0.0114, 0.0160, 0.0098,\n",
      "        0.0170, 0.0135, 0.0123, 0.0114, 0.0174, 0.0119, 0.0116, 0.0125, 0.0178,\n",
      "        0.0108, 0.0131, 0.0151, 0.0120, 0.0139, 0.0147, 0.0120, 0.0143, 0.0189,\n",
      "        0.0186, 0.0160, 0.0111, 0.0117, 0.0141, 0.0105, 0.0154, 0.0141, 0.0114,\n",
      "        0.0100, 0.0197, 0.0147, 0.0139, 0.0147, 0.0141, 0.0145, 0.0118, 0.0164,\n",
      "        0.0139, 0.0170, 0.0136, 0.0163, 0.0116, 0.0142, 0.0176, 0.0153, 0.0133,\n",
      "        0.0157, 0.0145, 0.0151, 0.0101, 0.0149, 0.0141, 0.0183, 0.0161, 0.0176,\n",
      "        0.0154, 0.0123, 0.0132, 0.0071, 0.0197, 0.0098, 0.0119, 0.0170, 0.0098,\n",
      "        0.0118, 0.0131, 0.0156, 0.0149, 0.0153, 0.0105, 0.0137, 0.0130, 0.0151,\n",
      "        0.0129, 0.0160, 0.0116, 0.0145, 0.0135, 0.0075, 0.0156, 0.0107, 0.0089,\n",
      "        0.0136, 0.0127, 0.0116, 0.0140, 0.0156, 0.0161, 0.0115, 0.0120, 0.0133,\n",
      "        0.0120, 0.0147, 0.0153, 0.0134, 0.0153, 0.0117, 0.0134, 0.0160, 0.0094,\n",
      "        0.0151, 0.0136, 0.0090, 0.0121, 0.0164, 0.0119, 0.0156, 0.0093, 0.0144,\n",
      "        0.0138, 0.0085, 0.0136, 0.0198, 0.0117, 0.0121, 0.0130, 0.0188, 0.0151,\n",
      "        0.0101, 0.0099, 0.0133, 0.0124, 0.0112, 0.0144, 0.0111, 0.0151, 0.0108,\n",
      "        0.0149, 0.0116, 0.0135, 0.0136, 0.0130, 0.0148, 0.0142, 0.0125, 0.0195,\n",
      "        0.0152, 0.0160, 0.0112, 0.0129, 0.0122, 0.0186, 0.0109, 0.0125, 0.0074,\n",
      "        0.0117, 0.0139, 0.0158, 0.0140, 0.0105, 0.0144, 0.0122, 0.0099, 0.0146,\n",
      "        0.0136, 0.0134, 0.0139, 0.0147, 0.0129, 0.0153, 0.0168, 0.0152, 0.0121,\n",
      "        0.0123, 0.0123, 0.0162, 0.0188, 0.0074, 0.0146, 0.0105, 0.0123, 0.0105,\n",
      "        0.0156, 0.0169, 0.0178, 0.0145, 0.0132, 0.0140, 0.0103, 0.0101, 0.0222,\n",
      "        0.0106, 0.0152, 0.0114, 0.0184, 0.0134, 0.0074, 0.0165, 0.0148, 0.0139,\n",
      "        0.0125, 0.0149, 0.0113, 0.0178, 0.0195, 0.0145, 0.0152, 0.0130, 0.0097,\n",
      "        0.0066, 0.0138, 0.0132, 0.0120, 0.0155, 0.0136, 0.0170, 0.0113, 0.0166,\n",
      "        0.0160, 0.0119, 0.0106, 0.0163, 0.0152, 0.0197, 0.0151, 0.0117, 0.0166,\n",
      "        0.0135, 0.0156, 0.0142, 0.0126, 0.0127, 0.0120, 0.0148, 0.0134, 0.0140,\n",
      "        0.0167, 0.0122, 0.0132, 0.0111, 0.0171, 0.0131, 0.0134, 0.0173, 0.0130,\n",
      "        0.0134, 0.0126, 0.0152, 0.0170, 0.0155, 0.0111, 0.0166, 0.0079, 0.0144,\n",
      "        0.0149, 0.0156, 0.0107, 0.0083, 0.0103, 0.0158, 0.0174, 0.0155, 0.0130,\n",
      "        0.0135, 0.0136, 0.0132, 0.0155, 0.0146, 0.0133, 0.0162, 0.0097, 0.0138,\n",
      "        0.0147, 0.0098, 0.0141, 0.0119, 0.0154, 0.0154, 0.0107, 0.0137, 0.0117,\n",
      "        0.0117, 0.0121, 0.0144, 0.0130, 0.0150, 0.0164, 0.0101, 0.0147, 0.0209,\n",
      "        0.0134, 0.0092, 0.0128, 0.0134, 0.0171, 0.0144, 0.0115, 0.0122, 0.0139,\n",
      "        0.0134, 0.0136, 0.0139, 0.0127, 0.0142, 0.0099, 0.0164, 0.0118, 0.0120,\n",
      "        0.0156, 0.0144, 0.0144, 0.0131, 0.0161, 0.0130, 0.0132, 0.0107, 0.0147,\n",
      "        0.0151, 0.0153, 0.0155, 0.0151, 0.0141, 0.0126, 0.0134, 0.0109, 0.0148,\n",
      "        0.0159, 0.0142, 0.0144, 0.0126, 0.0108, 0.0143, 0.0146, 0.0181, 0.0125,\n",
      "        0.0115, 0.0131, 0.0114, 0.0174, 0.0135, 0.0152, 0.0127, 0.0127, 0.0123,\n",
      "        0.0125, 0.0141, 0.0127, 0.0120, 0.0104, 0.0136, 0.0098, 0.0114, 0.0129,\n",
      "        0.0149, 0.0108, 0.0112, 0.0145, 0.0143, 0.0126, 0.0165, 0.0151, 0.0183,\n",
      "        0.0147, 0.0098, 0.0149, 0.0170, 0.0133, 0.0174, 0.0170, 0.0119, 0.0144,\n",
      "        0.0099, 0.0176, 0.0144, 0.0173, 0.0097, 0.0157, 0.0140, 0.0143, 0.0181,\n",
      "        0.0100, 0.0126, 0.0112, 0.0112, 0.0099, 0.0143, 0.0149, 0.0115, 0.0122,\n",
      "        0.0110, 0.0154, 0.0083, 0.0112, 0.0180, 0.0064, 0.0136, 0.0126, 0.0133,\n",
      "        0.0109, 0.0150, 0.0162, 0.0126, 0.0154, 0.0102, 0.0155, 0.0133, 0.0142,\n",
      "        0.0081, 0.0137, 0.0113, 0.0115, 0.0128, 0.0124, 0.0097, 0.0138, 0.0144,\n",
      "        0.0104, 0.0140, 0.0128, 0.0140, 0.0151, 0.0098, 0.0148, 0.0118, 0.0143,\n",
      "        0.0094, 0.0141, 0.0149, 0.0180, 0.0085, 0.0112, 0.0126, 0.0087, 0.0176,\n",
      "        0.0169, 0.0126, 0.0129, 0.0093, 0.0127, 0.0129, 0.0155, 0.0141, 0.0120,\n",
      "        0.0124, 0.0104, 0.0164, 0.0144, 0.0145, 0.0162, 0.0140, 0.0115, 0.0166,\n",
      "        0.0109, 0.0158, 0.0181, 0.0128, 0.0106, 0.0145, 0.0149, 0.0116, 0.0118,\n",
      "        0.0193, 0.0146, 0.0114, 0.0147, 0.0135, 0.0113, 0.0119, 0.0140, 0.0113,\n",
      "        0.0145, 0.0116, 0.0161, 0.0178, 0.0104, 0.0157, 0.0119, 0.0153, 0.0162,\n",
      "        0.0119, 0.0131, 0.0129, 0.0111, 0.0176, 0.0139, 0.0133, 0.0140, 0.0143,\n",
      "        0.0127, 0.0141, 0.0119, 0.0131, 0.0170, 0.0118, 0.0137, 0.0136, 0.0117,\n",
      "        0.0147, 0.0125, 0.0126, 0.0154, 0.0156, 0.0177, 0.0150, 0.0132, 0.0133,\n",
      "        0.0105, 0.0141, 0.0176, 0.0143, 0.0123, 0.0169, 0.0123, 0.0109, 0.0142,\n",
      "        0.0096, 0.0130, 0.0106, 0.0135, 0.0115, 0.0143, 0.0147, 0.0141, 0.0149,\n",
      "        0.0145, 0.0143, 0.0192, 0.0120, 0.0136, 0.0122, 0.0147, 0.0135, 0.0141,\n",
      "        0.0190, 0.0121, 0.0104, 0.0134, 0.0126, 0.0128, 0.0131, 0.0123, 0.0100,\n",
      "        0.0126, 0.0164, 0.0116, 0.0117, 0.0108, 0.0132, 0.0158, 0.0114, 0.0108,\n",
      "        0.0170, 0.0155, 0.0145, 0.0192, 0.0181, 0.0108, 0.0146, 0.0113, 0.0244,\n",
      "        0.0112, 0.0106, 0.0196, 0.0131, 0.0173, 0.0137, 0.0119, 0.0175, 0.0155,\n",
      "        0.0140, 0.0130, 0.0176, 0.0142, 0.0121, 0.0088, 0.0127, 0.0147, 0.0126,\n",
      "        0.0154, 0.0108, 0.0120, 0.0107, 0.0140, 0.0151, 0.0198, 0.0196, 0.0148,\n",
      "        0.0192, 0.0146, 0.0113, 0.0124, 0.0155, 0.0179, 0.0113, 0.0180, 0.0140,\n",
      "        0.0116, 0.0119, 0.0160, 0.0147, 0.0166, 0.0199, 0.0145, 0.0180, 0.0148,\n",
      "        0.0167, 0.0143, 0.0119, 0.0163, 0.0118, 0.0220, 0.0124, 0.0140, 0.0147,\n",
      "        0.0141, 0.0160, 0.0146, 0.0162, 0.0144, 0.0112, 0.0115, 0.0157, 0.0154,\n",
      "        0.0120, 0.0115, 0.0176, 0.0131, 0.0110, 0.0143, 0.0142, 0.0174, 0.0152,\n",
      "        0.0119, 0.0106, 0.0081, 0.0136, 0.0133, 0.0154, 0.0141, 0.0126, 0.0114,\n",
      "        0.0126, 0.0169, 0.0154, 0.0147, 0.0114, 0.0098, 0.0169, 0.0114, 0.0146,\n",
      "        0.0170, 0.0144, 0.0132, 0.0159, 0.0108, 0.0123, 0.0106, 0.0125, 0.0120,\n",
      "        0.0115, 0.0104, 0.0131, 0.0132, 0.0139, 0.0135, 0.0126, 0.0132, 0.0134,\n",
      "        0.0090, 0.0139, 0.0138, 0.0122, 0.0121, 0.0141, 0.0123, 0.0142, 0.0103,\n",
      "        0.0135, 0.0153, 0.0172, 0.0148, 0.0087, 0.0197, 0.0147, 0.0167, 0.0143,\n",
      "        0.0104, 0.0123, 0.0123, 0.0128, 0.0149, 0.0143, 0.0125, 0.0150, 0.0150,\n",
      "        0.0149, 0.0103, 0.0159, 0.0151, 0.0148, 0.0145, 0.0113, 0.0130, 0.0112,\n",
      "        0.0178, 0.0144, 0.0125, 0.0097, 0.0100, 0.0130, 0.0124, 0.0120, 0.0158,\n",
      "        0.0171, 0.0120, 0.0156, 0.0167, 0.0122, 0.0138, 0.0123, 0.0099, 0.0128,\n",
      "        0.0114, 0.0116, 0.0153, 0.0124, 0.0091, 0.0160, 0.0138, 0.0153, 0.0154,\n",
      "        0.0102, 0.0152, 0.0133, 0.0110, 0.0154, 0.0118, 0.0145, 0.0115, 0.0110,\n",
      "        0.0178, 0.0138, 0.0140, 0.0172, 0.0167, 0.0155, 0.0186, 0.0096, 0.0140,\n",
      "        0.0172, 0.0132, 0.0100, 0.0127, 0.0103, 0.0145, 0.0181, 0.0208, 0.0123,\n",
      "        0.0108, 0.0134, 0.0165, 0.0120, 0.0148, 0.0221, 0.0146, 0.0114, 0.0174,\n",
      "        0.0155, 0.0106, 0.0165, 0.0115, 0.0179, 0.0138, 0.0155, 0.0176, 0.0146,\n",
      "        0.0149, 0.0144, 0.0146, 0.0121, 0.0146, 0.0111, 0.0134, 0.0163, 0.0184,\n",
      "        0.0083, 0.0124, 0.0116, 0.0181, 0.0124, 0.0095, 0.0181, 0.0146, 0.0067,\n",
      "        0.0091, 0.0137, 0.0087, 0.0115, 0.0100, 0.0117, 0.0146, 0.0138, 0.0170,\n",
      "        0.0130, 0.0131, 0.0135, 0.0137, 0.0107, 0.0131, 0.0120, 0.0119, 0.0118,\n",
      "        0.0147, 0.0112, 0.0152, 0.0104, 0.0088, 0.0108, 0.0144, 0.0147, 0.0127,\n",
      "        0.0162, 0.0150, 0.0165, 0.0125, 0.0152, 0.0134, 0.0119, 0.0156, 0.0121,\n",
      "        0.0128, 0.0156, 0.0117, 0.0176, 0.0106, 0.0166, 0.0100, 0.0166, 0.0177,\n",
      "        0.0104, 0.0131, 0.0154, 0.0128, 0.0177, 0.0180, 0.0153, 0.0211, 0.0134,\n",
      "        0.0112, 0.0131, 0.0151, 0.0161, 0.0106, 0.0126, 0.0117, 0.0126, 0.0158,\n",
      "        0.0146, 0.0159, 0.0139, 0.0168, 0.0128, 0.1733, 0.1684, 0.2017, 0.1564,\n",
      "        0.1703, 0.1544, 0.1644, 0.1791, 0.1539, 0.1016, 0.1564, 0.1525, 0.2111,\n",
      "        0.1735, 0.1555, 0.1275, 0.1413, 0.1405, 0.1694, 0.1741, 0.1151, 0.1773,\n",
      "        0.1700, 0.1689, 0.2022, 0.1527, 0.1627, 0.1676, 0.1491, 0.1665, 0.2052,\n",
      "        0.1516, 0.1676, 0.1789, 0.1438, 0.1457, 0.1690, 0.1485, 0.1621, 0.1517,\n",
      "        0.1663, 0.1844, 0.1413, 0.1545, 0.1174, 0.1699, 0.1456, 0.1279, 0.2069,\n",
      "        0.1547])\n"
     ]
    }
   ],
   "source": [
    "print(seq_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01f2250b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n"
     ]
    }
   ],
   "source": [
    "print(anomaly_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c9932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
