import torch
import json
import ipdb
import os
import pandas as pd
import torch.nn as nn
import sqlalchemy
import torch.nn.functional as F

from torch.distributions import Categorical
from tqdm import tqdm
from dotenv import load_dotenv

load_dotenv()

# config.py
class ModelConfig:
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    DB_URL = f"postgresql://{os.getenv('DB_USER','postgres')}:{os.getenv('DB_PASSWORD','password')}@{os.getenv('DB_HOST','localhost')}:5432/{os.getenv('DB_NAME','crypto_quant')}"
    BATCH_SIZE = 8192
    TRAIN_STEPS = 1000
    MAX_FORMULA_LEN = 12
    TRADE_SIZE_USD = 1000.0
    MIN_LIQUIDITY = 5000.0 # ä½äºæ­¤æµåŠ¨æ€§è§†ä¸ºå½’é›¶/æ— æ³•äº¤æ˜“
    BASE_FEE = 0.005 # åŸºç¡€è´¹ç‡ 0.5% (Swap + Gas + Jito Tip)
    INPUT_DIM = 6

#ops.py
"""
    ç”¨äºæ—¶é—´åºåˆ—ä¿¡å·å¤„ç†æˆ–è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹(å¦‚é‡åŒ–å› å­æŒ–æ˜ã€é—ä¼ ç¼–ç¨‹)çš„åŸå­æ“ä½œ(primitive operations),
    å¹¶åˆ©ç”¨ PyTorch çš„ torch.jit.script ç¼–è¯‘ä¼˜åŒ–ä»¥æå‡è¿è¡Œæ•ˆç‡ã€‚
    æ•´ä½“è®¾è®¡å¸¸è§äºè‡ªåŠ¨åŒ– alpha æŒ–æ˜ã€ç¨‹åºåˆæˆæˆ–ç¥ç»ç¬¦å·ç³»ç»Ÿä¸­ã€‚
"""
@torch.jit.script
def _ts_delay(x: torch.Tensor, d: int) -> torch.Tensor:
    """
        åŠŸèƒ½:æ—¶é—´åºåˆ—å»¶è¿Ÿ(lag)æ“ä½œ
        è¿”å› x å‘åç§»åŠ¨ d æœŸçš„ç»“æœ(å³ x[t-d] å¯¹åº”è¾“å‡º t æ—¶åˆ»)
        è¾¹ç•Œå¤„ç†:å‰ d ä¸ªæ—¶é—´æ­¥ç”¨ 0 å¡«å……(å› æœ padding,æ— æœªæ¥ä¿¡æ¯æ³„éœ²)
    """
    if d == 0: return x
    pad = torch.zeros((x.shape[0], d), device=x.device)
    result = torch.cat([pad, x[:, :-d]], dim=1)
    # import ipdb ; ipdb.set_trace()
    return result

@torch.jit.script
def _op_gate(condition: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
    """
        åŠŸèƒ½:æ¡ä»¶é€‰æ‹©é—¨(ç±»ä¼¼ if-else)
        - è‹¥ condition > 0,è¾“å‡º xï¼›å¦åˆ™è¾“å‡º y
        - å‘é‡åŒ–å®ç°:é¿å… Python æ§åˆ¶æµ,æ”¯æŒ GPU å¹¶è¡Œ
        æ•°å­¦ç­‰ä»·äº:
        output={
                 x if condition>0
                 y otherwise
                }
        âœ… ç”¨äºæ„å»ºæ¡ä»¶é€»è¾‘(å¦‚â€œå¦‚æœæ³¢åŠ¨ç‡é«˜,åˆ™ç”¨ä¿å®ˆç­–ç•¥â€)
    """
    mask = (condition > 0).float()
    return mask * x + (1.0 - mask) * y

@torch.jit.script
def _op_jump(x: torch.Tensor) -> torch.Tensor:
    """
        åŠŸèƒ½:æ£€æµ‹â€œè·³è·ƒâ€æˆ–æç«¯å¼‚å¸¸å€¼
        è®¡ç®—æ¯è¡Œ(æ¯ä¸ªä»£å¸)çš„ Z-score:   z=(x - Î¼)/Ïƒ
        è‹¥   z>3 (å³åç¦»å‡å€¼ 3 ä¸ªæ ‡å‡†å·®ä»¥ä¸Š),è¾“å‡º z - 3 ï¼›å¦åˆ™è¾“å‡º 0
        æ„ä¹‰:è¯†åˆ«ä»·æ ¼/æŒ‡æ ‡çš„çªå‘æ€§æš´æ¶¨æš´è·Œ(meme å¸å¸¸è§)
        ğŸ“Œ è¾“å‡ºä¸ºéè´Ÿå€¼,è¶Šå¤§è¡¨ç¤ºâ€œè·³å¾—è¶ŠçŒ›â€
    """    
    mean = x.mean(dim=1, keepdim=True)
    std = x.std(dim=1, keepdim=True) + 1e-6
    z = (x - mean) / std
    return torch.relu(z - 3.0)

@torch.jit.script
def _op_decay(x: torch.Tensor) -> torch.Tensor:
    """
    åŠŸèƒ½:æŒ‡æ•°è¡°å‡åŠ æƒ(è¿‘ä¼¼)
        å½“å‰å€¼æƒé‡ 1.0,æ˜¨æ—¥ 0.8,å‰æ—¥ 0.6
        è™½éä¸¥æ ¼æŒ‡æ•°è¡°å‡(å¦‚ EWMA),ä½†å®ç°äº†â€œè¿‘æœŸæ›´é‡è¦â€çš„æ€æƒ³
        ç”¨é€”:å¹³æ»‘ä¿¡å·ã€èµ‹äºˆè¿‘æœŸæ•°æ®æ›´é«˜ä¼˜å…ˆçº§
        ğŸ’¡ å¯è§†ä¸ºä¸€ä¸ªç®€å•çš„ FIR æ»¤æ³¢å™¨
    """
    return x + 0.8 * _ts_delay(x, 1) + 0.6 * _ts_delay(x, 2)

OPS_CONFIG = [
    #    è¿™æ˜¯ä¸€ä¸ª æ“ä½œåŸè¯­åº“(primitive set),æ¯ä¸ªå…ƒç´ æ˜¯ä¸‰å…ƒç»„ (name, function, arity):
    #    å­—æ®µ	å«ä¹‰
    #    name	æ“ä½œåç§°(å­—ç¬¦ä¸²æ ‡è¯†)
    #    function	å¯è°ƒç”¨çš„å‡½æ•°(lambda æˆ– JIT å‡½æ•°)
    #    arity	æ‰€éœ€è¾“å…¥å‚æ•°ä¸ªæ•°(1=ä¸€å…ƒ,2=äºŒå…ƒ,3=ä¸‰å…ƒ)
    ('ADD', lambda x, y: x + y, 2),
    ('SUB', lambda x, y: x - y, 2),
    ('MUL', lambda x, y: x * y, 2),
    ('DIV', lambda x, y: x / (y + 1e-6), 2),
    ('NEG', lambda x: -x, 1),
    ('ABS', torch.abs, 1),
    ('SIGN', torch.sign, 1),
    ('GATE', _op_gate, 3),
    ('JUMP', _op_jump, 1),
    ('DECAY', _op_decay, 1),
    ('DELAY1', lambda x: _ts_delay(x, 1), 1),
    ('MAX3', lambda x: torch.max(x, torch.max(_ts_delay(x,1), _ts_delay(x,2))), 1)
]

# factors.py
class RMSNormFactor(nn.Module):
    """RMSNorm for factor normalization
        RMS å½’ä¸€åŒ–å±‚
    """
    def __init__(self, d_model, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(d_model))
    
    def forward(self, x):
        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)
        return (x / rms) * self.weight


class MemeIndicators:
    @staticmethod
    def liquidity_health(liquidity, fdv):
        """
            è¡¡é‡æµåŠ¨æ€§å¥åº·åº¦ â€”â€” æµåŠ¨æ€§æ± æ·±åº¦ç›¸å¯¹äºå®Œå…¨ç¨€é‡Šå¸‚å€¼(FDV)çš„æ¯”ä¾‹ã€‚
            é€»è¾‘:
                FDV è¶Šé«˜,é¡¹ç›®â€œç†è®ºå¸‚å€¼â€è¶Šå¤§
                è‹¥æµåŠ¨æ€§è¿œå°äº FDV(å¦‚ FDV= 1B,æµåŠ¨æ€§= 10k),åˆ™ææ˜“è¢«ç ¸ç›˜(æ»‘ç‚¹å·¨å¤§)
            å½’ä¸€åŒ–:
                ratio * 4.0:å‡è®¾ç†æƒ³ ratio â‰ˆ 0.25(å³æµåŠ¨æ€§å  FDV çš„ 25%),æ­¤æ—¶è¾“å‡ºä¸º 1.0
                clamp(0, 1):é™åˆ¶åœ¨ [0,1] åŒºé—´,0=æå·®,1=æå¥½
                âœ… è¾“å‡ºå¯ä½œä¸ºâ€œæŠ—ç ¸ç›˜èƒ½åŠ›â€è¯„åˆ†
            """
        ratio = liquidity / (fdv + 1e-6)
        return torch.clamp(ratio * 4.0, 0.0, 1.0)

    @staticmethod
    def buy_sell_imbalance(close, open_, high, low):
        """
            è¡¡é‡å•æ ¹Kçº¿çš„ä¹°å–åŠ›é‡ä¸å¹³è¡¡ç¨‹åº¦(ç±»ä¼¼"å®ä½“å æ¯”")ã€‚
            è®¡ç®—:
                body = close - open_:é˜³çº¿ä¸ºæ­£,é˜´çº¿ä¸ºè´Ÿ
                range_hl = high - low:æ•´æ ¹Kçº¿æ³¢åŠ¨èŒƒå›´
                strength = body / range_hl:å®ä½“å æ€»æŒ¯å¹…çš„æ¯”ä¾‹(âˆˆ [-1, 1])
            éçº¿æ€§å‹ç¼©:
                tanh(strength * 3):æ”¾å¤§ä¸­ç­‰å¼ºåº¦ä¿¡å·,æŠ‘åˆ¶æç«¯å€¼(å¹³æ»‘)
            âœ… æ­£å€¼è¡¨ç¤ºä¹°æ–¹å¼ºåŠ¿,è´Ÿå€¼è¡¨ç¤ºå–æ–¹å¼ºåŠ¿
        """
        range_hl = high - low + 1e-9
        body = close - open_
        strength = body / range_hl
        return torch.tanh(strength * 3.0)

    @staticmethod
    def fomo_acceleration(volume, window=5):
        """
            æ£€æµ‹FOMO(é”™å¤±ææƒ§)æƒ…ç»ªçš„åŠ é€Ÿåº¦ â€”â€” æˆäº¤é‡å¢é•¿æ˜¯å¦åœ¨åŠ é€Ÿã€‚
            æ­¥éª¤:
                1, vol_chg:æˆäº¤é‡ç¯æ¯”å˜åŒ–ç‡(æ˜¨æ—¥ä¸ºåˆ†æ¯,+1 é˜²é™¤é›¶)
                2, acc = Î”(vol_chg):æˆäº¤é‡å˜åŒ–ç‡çš„äºŒé˜¶å¯¼æ•°(åŠ é€Ÿåº¦)
            æ„ä¹‰:
                1, acc > 0:FOMO åœ¨åŠ å‰§(è¶Šæ¥è¶Šå¤šäººå†²å…¥)
                2, acc < 0:FOMO å‡é€€(çƒ­åº¦ä¸‹é™)
            è£å‰ª:clamp(-5, 5) é˜²æ­¢å¼‚å¸¸å€¼å¹²æ‰°
            âš ï¸ æ³¨æ„:torch.roll ä¼šå¯¼è‡´é¦–åˆ—ä½¿ç”¨æœ€åä¸€åˆ—æ•°æ®(å¾ªç¯ç§»ä½),å®é™…åº” paddingã€‚ä½†æ­¤å¤„å¯èƒ½æ¥å—è¾¹ç•Œè¯¯å·®ã€‚
        """
        vol_prev = torch.roll(volume, 1, dims=1)
        vol_chg = (volume - vol_prev) / (vol_prev + 1.0)
        acc = vol_chg - torch.roll(vol_chg, 1, dims=1)
        return torch.clamp(acc, -5.0, 5.0)

    @staticmethod
    def pump_deviation(close, window=20):
        """
            è¡¡é‡ä»·æ ¼ç›¸å¯¹äºç§»åŠ¨å¹³å‡çº¿çš„åç¦»ç¨‹åº¦(åˆ¤æ–­æ˜¯å¦â€œæš´æ¶¨â€æˆ–â€œè¶…ä¹°â€)ã€‚
            å®ç°ç»†èŠ‚:
                æ‰‹åŠ¨ padding(å‰è¡¥ 0)â†’ è®¡ç®—ä»ç¬¬ window å¤©å¼€å§‹çš„ MA
                unfold(1, window, 1):æ»‘åŠ¨çª—å£æå–å­åºåˆ—
                dev = (price - MA) / MA:ç›¸å¯¹åç¦»ç‡(ç±»ä¼¼å¸ƒæ—å¸¦ z-score)
            ç”¨é€”:
                dev >> 0:ä»·æ ¼è¿œé«˜äºå‡çº¿ â†’ å¯èƒ½è¿‡çƒ­
                dev << 0:ä»·æ ¼è¿œä½äºå‡çº¿ â†’ å¯èƒ½è¶…å–
            âœ… å…¸å‹â€œè¿½æ¶¨æ€è·Œâ€ä¿¡å·æº
        """
        pad = torch.zeros((close.shape[0], window-1), device=close.device)
        c_pad = torch.cat([pad, close], dim=1)
        ma = c_pad.unfold(1, window, 1).mean(dim=-1)
        dev = (close - ma) / (ma + 1e-9)
        return dev

    @staticmethod
    def volatility_clustering(close, window=10):
        """Detect volatility clustering patterns
            æ£€æµ‹æ³¢åŠ¨ç‡èšé›†æ•ˆåº”(é‡‘èç»å…¸ç°è±¡:é«˜æ³¢åŠ¨åå¾€å¾€æ¥é«˜æ³¢åŠ¨)ã€‚
            æ­¥éª¤:
                1, è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡ ret = log(P_t / P_{t-1})
                2, å¹³æ–¹æ”¶ç›Šç‡ ret_sq â†’ ä»£è¡¨ç¬æ—¶æ³¢åŠ¨ç‡
                3, å¯¹ ret_sq åšç§»åŠ¨å¹³å‡ â†’ å¾—åˆ°å±€éƒ¨æ³¢åŠ¨ç‡ä¼°è®¡
                4, å¼€æ–¹ â†’ è¿‘ä¼¼æ ‡å‡†å·®(æ³¢åŠ¨ç‡)
            è¾“å‡º:æ»šåŠ¨çª—å£å†…çš„å†å²æ³¢åŠ¨ç‡
            âœ… ç”¨äºè¯†åˆ«â€œé«˜æ³¢åŠ¨æœŸâ€,å¯èƒ½ä¼´éš meme å¸å‰§çƒˆç‚’ä½œ
        """
        ret = torch.log(close / (torch.roll(close, 1, dims=1) + 1e-9))
        ret_sq = ret ** 2
        
        pad = torch.zeros((ret_sq.shape[0], window-1), device=close.device)
        ret_sq_pad = torch.cat([pad, ret_sq], dim=1)
        vol_ma = ret_sq_pad.unfold(1, window, 1).mean(dim=-1)
        
        return torch.sqrt(vol_ma + 1e-9)

    @staticmethod
    def momentum_reversal(close, window=5):
        """Capture momentum reversal signals
            æ£€æµ‹åŠ¨é‡åè½¬ä¿¡å· â€”â€” ä¸Šæ¶¨/ä¸‹è·Œè¶‹åŠ¿æ˜¯å¦çªç„¶è½¬å‘ã€‚
            é€»è¾‘:
                1, mom:è¿‡å» window å¤©çš„ç´¯è®¡æ”¶ç›Šç‡(åŠ¨é‡æ–¹å‘)
                2, mom_prev:å‰ä¸€å¤©çš„åŠ¨é‡
                3, mom * mom_prev < 0:ç¬¦å·ç›¸å â†’ å‘ç”ŸåŠ¨é‡åè½¬
            è¾“å‡º:äºŒå€¼ä¿¡å·(1=åè½¬,0=æ— åè½¬)
            âœ… ç”¨äºæ•æ‰â€œè§é¡¶å›è½â€æˆ–â€œæ­¢è·Œåå¼¹â€çš„è½¬æŠ˜ç‚¹
        """
        ret = torch.log(close / (torch.roll(close, 1, dims=1) + 1e-9))
        
        pad = torch.zeros((ret.shape[0], window-1), device=close.device)
        ret_pad = torch.cat([pad, ret], dim=1)
        mom = ret_pad.unfold(1, window, 1).sum(dim=-1)
        
        # Detect reversals
        mom_prev = torch.roll(mom, 1, dims=1)
        reversal = (mom * mom_prev < 0).float()
        
        return reversal

    @staticmethod
    def relative_strength(close, high, low, window=14):
        """RSI-like indicator for strength detection
            å®ç° RSI(ç›¸å¯¹å¼ºå¼±æŒ‡æ•°) çš„å˜ä½“,ç”¨äºåˆ¤æ–­è¶…ä¹°/è¶…å–ã€‚
            æ ‡å‡† RSI æ­¥éª¤:
                1, è®¡ç®—æ¯æ—¥æ¶¨è·Œ(gains, losses)
                2, è®¡ç®— window æ—¥å¹³å‡æ¶¨å¹…/è·Œå¹…
                3, RS = avg_gain / avg_loss
                4, RSI = 100 - 100/(1+RS)
            å½’ä¸€åŒ–:
                (RSI - 50) / 50 â†’ å°† [0,100] æ˜ å°„åˆ° [-1, 1]
                    -1:æç«¯è¶…å–
                    0:ä¸­æ€§
                    +1:æç«¯è¶…ä¹°
            âœ… ç»å…¸éœ‡è¡æŒ‡æ ‡,é€‚ç”¨äº meme å¸çš„é«˜æ³¢åŠ¨ç¯å¢ƒ        
        """
        ret = close - torch.roll(close, 1, dims=1)
        
        gains = torch.relu(ret)
        losses = torch.relu(-ret)
        
        pad = torch.zeros((gains.shape[0], window-1), device=close.device)
        gains_pad = torch.cat([pad, gains], dim=1)
        losses_pad = torch.cat([pad, losses], dim=1)
        
        avg_gain = gains_pad.unfold(1, window, 1).mean(dim=-1)
        avg_loss = losses_pad.unfold(1, window, 1).mean(dim=-1)
        
        rs = (avg_gain + 1e-9) / (avg_loss + 1e-9)
        rsi = 100 - (100 / (1 + rs))
        
        return (rsi - 50) / 50  # Normalize


class AdvancedFactorEngineer:
    """Advanced feature engineering with multiple factor types
        ä»åŸå§‹å¸‚åœºæ•°æ®ä¸­æ„å»ºä¸€ä¸ª12ç»´çš„é«˜çº§ç‰¹å¾ç©ºé—´(feature space),ç‰¹åˆ«é’ˆå¯¹é«˜æ³¢åŠ¨æ€§èµ„äº§(å¦‚ meme å¸)è®¾è®¡ã€‚
        å®ƒç»“åˆäº†åŸºç¡€æŠ€æœ¯æŒ‡æ ‡ã€è¡Œä¸ºé‡‘èä¿¡å·å’Œç¨³å¥å½’ä¸€åŒ–æ–¹æ³•,
        è¾“å‡ºå¯ç›´æ¥ç”¨äºæœºå™¨å­¦ä¹ æ¨¡å‹(å¦‚é¢„æµ‹ã€åˆ†ç±»æˆ–å¼ºåŒ–å­¦ä¹ )çš„æ ‡å‡†åŒ–ç‰¹å¾å¼ é‡.
    """
    def __init__(self):
        self.rms_norm = RMSNormFactor(1)
    
    def robust_norm(self, t):
        """Robust normalization using median absolute deviation
            ç¨³å¥å½’ä¸€åŒ–(åŸºäºä¸­ä½æ•°å’Œ MAD)
            å¯¹è¾“å…¥å¼ é‡ t(shape: [num_tokens, time_steps])è¿›è¡Œç¨³å¥æ ‡å‡†åŒ–,é¿å…å—æç«¯å€¼(outliers)å½±å“ã€‚
            æ­¥éª¤:
                1,è®¡ç®—æ¯è¡Œ(æ¯ä¸ªä»£å¸)çš„ä¸­ä½æ•° â†’ æ¯”å‡å€¼æ›´æŠ—å¼‚å¸¸å€¼ã€‚
                2, è®¡ç®— MAD(Median Absolute Deviation):
                    MAD = median(|Xi - median(X)|)
                3,æ ‡å‡†åŒ–:(x - median) / MAD
            è£å‰ª:é™åˆ¶åœ¨ [-5, 5],é˜²æ­¢æ®‹ä½™å¼‚å¸¸å€¼å¹²æ‰°æ¨¡å‹ã€‚
            âœ… ä¸ºä»€ä¹ˆç”¨ MAD?
            Meme å¸ä»·æ ¼å¸¸å‡ºç°æš´æ¶¨æš´è·Œ(å¦‚ 100x),å‡å€¼å’Œæ ‡å‡†å·®ä¼šè¢«ä¸¥é‡æ‰­æ›²ã€‚MAD å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ,æ›´é€‚åˆæ­¤ç±»æ•°æ®ã€‚            
        """
        median = torch.nanmedian(t, dim=1, keepdim=True)[0]
        mad = torch.nanmedian(torch.abs(t - median), dim=1, keepdim=True)[0] + 1e-6
        norm = (t - median) / mad
        return torch.clamp(norm, -5.0, 5.0)
    
    def compute_advanced_features(self, raw_dict):
        """Compute 12-dimensional feature space with advanced factors æ„å»º12ç»´ç‰¹å¾
        """
        c = raw_dict['close']
        o = raw_dict['open']
        h = raw_dict['high']
        l = raw_dict['low']
        v = raw_dict['volume']
        liq = raw_dict['liquidity']
        fdv = raw_dict['fdv']
        
        # Basic factors è®¡ç®—åŸºç¡€å› å­(Basic Factors)
        ret = torch.log(c / (torch.roll(c, 1, dims=1) + 1e-9)) # å¯¹æ•°æ”¶ç›Šç‡
        liq_score = MemeIndicators.liquidity_health(liq, fdv)   # æµåŠ¨æ€§å¥åº·åº¦
        pressure = MemeIndicators.buy_sell_imbalance(c, o, h, l) # ä¹°å–å‹åŠ›
        fomo = MemeIndicators.fomo_acceleration(v) # fomo åŠ é€Ÿåº¦
        dev = MemeIndicators.pump_deviation(c)  # ä»·æ ¼åç¦»å‡çº¿ç¨‹åº¦
        log_vol = torch.log1p(v) # torch.log1p(v) = log(1 + v),é¿å… v=0 æ—¶ log(0) é—®é¢˜,åŒæ—¶å‹ç¼©å¤§æˆäº¤é‡ã€‚
        
        # Advanced factors
        vol_cluster = MemeIndicators.volatility_clustering(c) # æ³¢åŠ¨ç‡èšé›†(å±€éƒ¨æ³¢åŠ¨ç‡)
        momentum_rev = MemeIndicators.momentum_reversal(c) # åŠ¨é‡åè½¬(0 æˆ– 1)
        rel_strength = MemeIndicators.relative_strength(c, h, l) # ç›¸å¯¹å¼ºå¼±(RSI å˜ä½“,-1ï½1)
        
        # High-low range æ—¥å†…æŒ¯å¹…å æ”¶ç›˜ä»·æ¯”ä¾‹
        hl_range = (h - l) / (c + 1e-9)
        
        # Close position in range æ”¶ç›˜ä»·åœ¨æ—¥å†…åŒºé—´çš„ä½ç½®(0ï½1)
        close_pos = (c - l) / (h - l + 1e-9)
            # close_pos æ˜¯ç»å…¸â€œKçº¿ä½ç½®â€æŒ‡æ ‡ï¼šæ¥è¿‘ 1 è¡¨ç¤ºå¼ºåŠ¿æ”¶æ¶¨,æ¥è¿‘ 0 è¡¨ç¤ºå¼±åŠ¿æ”¶è·Œã€‚
        
        # Volume trend æˆäº¤é‡å˜åŒ–ç‡
        vol_prev = torch.roll(v, 1, dims=1)
        vol_trend = (v - vol_prev) / (vol_prev + 1.0)
        
        # å †å æ‰€æœ‰ç‰¹å¾ 
        features = torch.stack([
            self.robust_norm(ret),
            liq_score,
            pressure,
            self.robust_norm(fomo),
            self.robust_norm(dev),
            self.robust_norm(log_vol),
            self.robust_norm(vol_cluster),
            momentum_rev,
            self.robust_norm(rel_strength),
            self.robust_norm(hl_range),
            close_pos,
            self.robust_norm(vol_trend)
        ], dim=1)
        # torch.stack(..., dim=1) å°† 12 ä¸ª [B, T] å¼ é‡å †å æˆ [B, 12, T],
        # è¿™æ˜¯æ—¶é—´åºåˆ—æ¨¡å‹(å¦‚ Transformerã€CNN)çš„æ ‡å‡†è¾“å…¥æ ¼å¼ã€‚
        
        return features


class FeatureEngineer:
    """
        ä»åŸå§‹å¸‚åœºæ•°æ®(å¦‚ K çº¿ã€æµåŠ¨æ€§ç­‰)ä¸­é«˜æ•ˆæå–ä¸€ç»„æ ‡å‡†åŒ–çš„ 6 ç»´æŠ€æœ¯ç‰¹å¾(factors),
        ä¸“ä¸ºé«˜æ³¢åŠ¨æ€§åŠ å¯†èµ„äº§(å¦‚ meme å¸)è®¾è®¡ã€‚
        å…¶æ ¸å¿ƒç›®æ ‡æ˜¯å°†åŸå§‹è¡Œæƒ…æ•°æ®è½¬æ¢ä¸ºæ•°å€¼ç¨³å®šã€é‡çº²ç»Ÿä¸€ã€æŠ—å¼‚å¸¸å€¼çš„ç‰¹å¾å¼ é‡,å¯ç›´æ¥è¾“å…¥æœºå™¨å­¦ä¹ æ¨¡å‹(å¦‚é¢„æµ‹ç½‘ç»œã€å¼ºåŒ–å­¦ä¹ ç­–ç•¥ç­‰)ã€‚
    """
    INPUT_DIM = 6

    @staticmethod
    def compute_features(raw_dict):
        c = raw_dict['close']
        o = raw_dict['open']
        h = raw_dict['high']
        l = raw_dict['low']
        v = raw_dict['volume']
        liq = raw_dict['liquidity']
        fdv = raw_dict['fdv']
        
        ret = torch.log(c / (torch.roll(c, 1, dims=1) + 1e-9)) #  å¯¹æ•°æ”¶ç›Šç‡(Log Return)
        liq_score = MemeIndicators.liquidity_health(liq, fdv) # æµåŠ¨æ€§å¥åº·åº¦
        pressure = MemeIndicators.buy_sell_imbalance(c, o, h, l) # ä¹°å–å‹åŠ›(Kçº¿å®ä½“å¼ºåº¦)
        fomo = MemeIndicators.fomo_acceleration(v) # FOMO æƒ…ç»ªåŠ é€Ÿåº¦
        dev = MemeIndicators.pump_deviation(c) # ä»·æ ¼åç¦»åº¦(æ˜¯å¦æš´æ¶¨ï¼Ÿ)
        log_vol = torch.log1p(v) # å¯¹æ•°æˆäº¤é‡
        
        def robust_norm(t):
            median = torch.nanmedian(t, dim=1, keepdim=True)[0]
            mad = torch.nanmedian(torch.abs(t - median), dim=1, keepdim=True)[0] + 1e-6
            norm = (t - median) / mad
            return torch.clamp(norm, -5.0, 5.0)

        features = torch.stack([
            robust_norm(ret),
            liq_score,
            pressure,
            robust_norm(fomo),
            robust_norm(dev),
            robust_norm(log_vol)
        ], dim=1)
        
        return features
# data_loader.py
class CryptoDataLoader:
    def __init__(self):
        self.engine = sqlalchemy.create_engine(ModelConfig.DB_URL)
        self.feat_tensor = None
        self.raw_data_cache = None
        self.target_ret = None
        
    def load_data(self, limit_tokens=500):
        print("Loading data from SQL...")
        top_query = f"""
        SELECT address FROM tokens 
        LIMIT {limit_tokens} 
        """
        addrs = pd.read_sql(top_query, self.engine)['address'].tolist()
        if not addrs: raise ValueError("No tokens found.")
        addr_str = "'" + "','".join(addrs) + "'"
        data_query = f"""
        SELECT time, address, open, high, low, close, volume, liquidity, fdv
        FROM ohlcv
        WHERE address IN ({addr_str})
        ORDER BY time ASC
        """
        df = pd.read_sql(data_query, self.engine)
        def to_tensor(col):
            pivot = df.pivot(index='time', columns='address', values=col)
            pivot = pivot.fillna(0.0)
            return torch.tensor(pivot.values.T, dtype=torch.float32, device=ModelConfig.DEVICE)
        self.raw_data_cache = {
            'open': to_tensor('open'),
            'high': to_tensor('high'),
            'low': to_tensor('low'),
            'close': to_tensor('close'),
            'volume': to_tensor('volume'),
            'liquidity': to_tensor('liquidity'),
            'fdv': to_tensor('fdv')
        }
        self.feat_tensor = FeatureEngineer.compute_features(self.raw_data_cache)
        op = self.raw_data_cache['open']
        t1 = torch.roll(op, -1, dims=1)
        t2 = torch.roll(op, -2, dims=1)
        self.target_ret = torch.log(t2 / (t1 + 1e-9))
        self.target_ret[:, -2:] = 0.0
        print(f"Data Ready. Shape: {self.feat_tensor.shape}")

# alphagpt.py
class NewtonSchulzLowRankDecay:
    """
    Low-Rank Decay (LoRD) using Newton-Schulz iteration.
    
    A more efficient regularization method that targets low-rank structure
    in attention and key parameters. Uses Newton-Schulz iteration to compute
    the minimum singular vectors without explicit SVD.
    
    Args:
        named_parameters: Model's named parameters
        decay_rate: Strength of low-rank decay
        num_iterations: Number of Newton-Schulz iterations (default: 5)
        target_keywords: If specified, only decay parameters matching these keywords

    åä¸º AlphaGPT çš„ç¥ç»ç½‘ç»œæ¨¡å‹,ä¸“ä¸ºè‡ªåŠ¨ç”Ÿæˆäº¤æ˜“ç­–ç•¥å…¬å¼(å¦‚é‡åŒ–å› å­) è€Œè®¾è®¡ã€‚
        å®ƒç»“åˆäº†å¤šç§å…ˆè¿›æ¶æ„æŠ€æœ¯(å¦‚ Looped Transformerã€QK-Normã€SwiGLUã€MTP Head ç­‰),
        å¹¶å¼•å…¥äº†ä½ç§©æ­£åˆ™åŒ–(LoRD) å’Œç¨³å®šç§©ç›‘æ§æœºåˆ¶,
        ä»¥æå‡æ¨¡å‹åœ¨å°æ ·æœ¬ã€é«˜å™ªå£°é‡‘èæ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›
    """
    def __init__(self, named_parameters, decay_rate=1e-3, num_iterations=5, target_keywords=None):
        self.decay_rate = decay_rate
        self.num_iterations = num_iterations
        self.target_keywords = target_keywords or ["qk_norm", "attention"]
        self.params_to_decay = []
        
        for name, param in named_parameters:
            if not param.requires_grad or param.ndim != 2:
                continue
            if not any(k in name for k in self.target_keywords):
                continue
            self.params_to_decay.append((name, param))
    
    @torch.no_grad()
    def step(self):
        """Apply Newton-Schulz low-rank decay to attention parameters."""
        for name, W in self.params_to_decay:
            orig_dtype = W.dtype
            X = W.float()
            r, c = X.shape
            
            # Transpose if needed for efficiency
            transposed = False
            if r > c:
                X = X.T
                transposed = True
            
            # Normalize by spectral norm
            norm = X.norm() + 1e-8
            X = X / norm
            
            # Initialize Y for Newton-Schulz iteration
            Y = X
            I = torch.eye(X.shape[-1], device=X.device, dtype=X.dtype)
            
            # Newton-Schulz iteration: Y_{k+1} = 0.5 * Y_k * (3*I - Y_k^T * Y_k)
            # This converges to the orthogonal matrix with same singular vectors
            for _ in range(self.num_iterations):
                A = Y.T @ Y
                Y = 0.5 * Y @ (3.0 * I - A)
            
            if transposed:
                Y = Y.T
            
            # Apply low-rank decay
            W.sub_(self.decay_rate * Y.to(orig_dtype))


class StableRankMonitor:
    """Monitor the effective rank (stable rank) of model parameters."""
    def __init__(self, model, target_keywords=None):
        self.model = model
        self.target_keywords = target_keywords or ["q_proj", "k_proj", "attention"]
        self.history = []
    
    @torch.no_grad()
    def compute(self):
        """Compute average stable rank of target parameters."""
        ranks = []
        for name, param in self.model.named_parameters():
            if param.ndim != 2:
                continue
            if not any(k in name for k in self.target_keywords):
                continue
            
            W = param.detach().float()
            S = torch.linalg.svdvals(W)
            # Stable Rank = ||W||_F^2 / ||W||_2^2
            stable_rank = (S.norm() ** 2) / (S[0] ** 2 + 1e-9)
            ranks.append(stable_rank.item())
        
        avg_rank = sum(ranks) / len(ranks) if ranks else 0.0
        self.history.append(avg_rank)
        return avg_rank


class RMSNorm(nn.Module):
    """Root Mean Square Layer Normalization"""
    def __init__(self, d_model, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(d_model))
    
    def forward(self, x):
        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)
        return (x / rms) * self.weight


class QKNorm(nn.Module):
    """Query-Key Normalization for Attention"""
    def __init__(self, d_model, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.scale = nn.Parameter(torch.ones(1, 1, 1, d_model) * (d_model ** -0.5))
    
    def forward(self, q, k):
        # Normalize Q and K independently
        q_norm = F.normalize(q, p=2, dim=-1)
        k_norm = F.normalize(k, p=2, dim=-1)
        return q_norm * self.scale, k_norm * self.scale


class SwiGLU(nn.Module):
    """Swish GLU activation function"""
    def __init__(self, d_in, d_ff):
        super().__init__()
        self.w = nn.Linear(d_in, d_ff * 2)
        self.fc = nn.Linear(d_ff, d_in)
    
    def forward(self, x):
        x_glu = self.w(x)
        x, gate = x_glu.chunk(2, dim=-1)
        x = x * F.silu(gate)  # Swish activation
        return self.fc(x)


class MTPHead(nn.Module):
    """Multi-Task Pooling Head for multi-objective learning"""
    def __init__(self, d_model, vocab_size, num_tasks=3):
        super().__init__()
        self.num_tasks = num_tasks
        self.task_heads = nn.ModuleList([
            nn.Linear(d_model, vocab_size) for _ in range(num_tasks)
        ])
        self.task_weights = nn.Parameter(torch.ones(num_tasks) / num_tasks)
        self.task_router = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Linear(d_model // 2, num_tasks)
        )
    
    def forward(self, x):
        # Route to appropriate task heads
        task_logits = self.task_router(x)
        task_probs = F.softmax(task_logits, dim=-1)
        
        # Compute all task outputs
        task_outputs = [head(x) for head in self.task_heads]
        task_outputs = torch.stack(task_outputs, dim=1)  # [B, num_tasks, vocab_size]
        
        # Weighted combination
        weighted = (task_probs.unsqueeze(-1) * task_outputs).sum(dim=1)
        return weighted, task_probs


class LoopedTransformerLayer(nn.Module):
    """Looped Transformer Layer - recurrent processing within a layer"""
    def __init__(self, d_model, nhead, dim_feedforward, num_loops=3, dropout=0.1):
        super().__init__()
        self.num_loops = num_loops
        self.d_model = d_model
        self.nhead = nhead
        
        # QK-Norm attention
        self.qk_norm = QKNorm(d_model // nhead)
        
        # Standard attention components
        self.attention = nn.MultiheadAttention(d_model, nhead, batch_first=True, dropout=dropout)
        
        # RMSNorm instead of LayerNorm
        self.norm1 = RMSNorm(d_model)
        self.norm2 = RMSNorm(d_model)
        
        # SwiGLU FFN instead of standard FFN
        self.ffn = SwiGLU(d_model, dim_feedforward)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None, is_causal=False):
        # Looped processing - recurrent refinement
        for _ in range(self.num_loops):
            # Self-attention with residual
            x_norm = self.norm1(x)
            attn_out, _ = self.attention(x_norm, x_norm, x_norm, attn_mask=mask, is_causal=is_causal)
            x = x + self.dropout(attn_out)
            
            # FFN with residual
            x_norm = self.norm2(x)
            ffn_out = self.ffn(x_norm)
            x = x + self.dropout(ffn_out)
        
        return x


class LoopedTransformer(nn.Module):
    """Looped Transformer Encoder with multiple loop iterations"""
    def __init__(self, d_model, nhead, num_layers, dim_feedforward, num_loops=3, dropout=0.1):
        super().__init__()
        self.layers = nn.ModuleList([
            LoopedTransformerLayer(d_model, nhead, dim_feedforward, num_loops, dropout)
            for _ in range(num_layers)
        ])
    
    def forward(self, x, mask=None, is_causal=False):
        for layer in self.layers:
            x = layer(x, mask=mask, is_causal=is_causal)
        return x


class AlphaGPT(nn.Module):
    def __init__(self):
        super().__init__()
        self.d_model = 64
        self.features_list = ['RET', 'VOL', 'V_CHG', 'PV', 'TREND']
        self.ops_list = [cfg[0] for cfg in OPS_CONFIG]
        
        self.vocab = self.features_list + self.ops_list
        self.vocab_size = len(self.vocab)
        
        # Embedding
        self.token_emb = nn.Embedding(self.vocab_size, self.d_model)
        self.pos_emb = nn.Parameter(torch.zeros(1, ModelConfig.MAX_FORMULA_LEN + 1, self.d_model))
        
        # Enhanced Transformer with Looped Transformer
        self.blocks = LoopedTransformer(
            d_model=self.d_model,
            nhead=4,
            num_layers=2,
            dim_feedforward=128,
            num_loops=3,
            dropout=0.1
        )
        
        # RMSNorm instead of LayerNorm
        self.ln_f = RMSNorm(self.d_model)
        
        # MTPHead for multi-task output
        self.mtp_head = MTPHead(self.d_model, self.vocab_size, num_tasks=3)
        self.head_critic = nn.Linear(self.d_model, 1)

    def forward(self, idx):
        # idx: [Batch, SeqLen]
        B, T = idx.size()
        
        x = self.token_emb(idx) + self.pos_emb[:, :T, :]
        
        # Causal Mask
        mask = nn.Transformer.generate_square_subsequent_mask(T).to(idx.device)
        
        # Process through looped transformer
        x = self.blocks(x, mask=mask, is_causal=True)
        x = self.ln_f(x)
        
        last_emb = x[:, -1, :]
        
        # Multi-task pooling head for logits
        logits, task_probs = self.mtp_head(last_emb)
        value = self.head_critic(last_emb)
        
        return logits, value, task_probs
# vm.py

class StackVM:
    def __init__(self):
        """
        feat_offset = 6
            åˆå§‹åŒ–æ˜ å°„è¡¨
            token 0~5 : è¾“å…¥ç‰¹å¾
            token 6+ æ“ä½œç¬¦
        """
        self.feat_offset = FeatureEngineer.INPUT_DIM
        self.op_map = {i + self.feat_offset: cfg[1] for i, cfg in enumerate(OPS_CONFIG)}
        # å°†æ“ä½œæ•°tokenæ˜ å°„åˆ°å‡½æ•°
        self.arity_map = {i + self.feat_offset: cfg[2] for i, cfg in enumerate(OPS_CONFIG)}
        # è®°å½•æ¯ä¸ªæ“ä½œç¬¦éœ€è¦å‡ ä¸ªå‚æ•°

    def execute(self, formula_tokens, feat_tensor): # æ‰§è¡Œå¼•æ“
        """            
            è¾“å…¥:
                formula_tokens: List[int] , è¡¨ç¤ºå…¬å¼çš„token åºåˆ—(åç¼€è¡¨è¾¾å¼/é€†æ³¢å…°è¡¨è¾¾å¼)
                feat_tensor:  Tensor, shape[B,6,T], ç”±Feature Engineer.compute_feature() ç”Ÿæˆ.
            è¾“å‡º:
                æˆåŠŸ: Tensor , shape[B,T] (æ¯ä¸€ä¸ªä»£å¸çš„æ—¶é—´åºåˆ—ä¿¡å·)
                å¤±è´¥: None(è¯­æ³•é”™è¯¯,æ ˆä¸åŒ¹é…,NaNç­‰)
            å…³é”®ç‚¹ï¼šä½¿ç”¨åç¼€è¡¨è¾¾å¼(RPN),å¤©ç„¶é€‚åˆæ ˆæœºæ‰§è¡Œ,æ— éœ€æ‹¬å·ã€‚
        """
        stack = []
        try:
            for token in formula_tokens:
                token = int(token)
                if token < self.feat_offset:
                    stack.append(feat_tensor[:, token, :])
                elif token in self.op_map:
                    arity = self.arity_map[token]
                    if len(stack) < arity: return None
                    args = []
                    for _ in range(arity):
                        args.append(stack.pop())
                    args.reverse()
                    func = self.op_map[token]
                    res = func(*args)
                    if torch.isnan(res).any() or torch.isinf(res).any():
                        res = torch.nan_to_num(res, nan=0.0, posinf=1.0, neginf=-1.0)
                    stack.append(res)
                else:
                    return None
            if len(stack) == 1:
                return stack[0]
            else:
                return None
        except Exception:
            return None
# backtest.py
class MemeBacktest:
    """
        ç”¨äºå¯¹ meme å¸(é«˜æ³¢åŠ¨æ€§åŠ å¯†è´§å¸)äº¤æ˜“ç­–ç•¥è¿›è¡Œå›æµ‹è¯„ä¼°ã€‚
        å®ƒæ¥æ”¶æ¨¡å‹ç”Ÿæˆçš„ä¿¡å·(factors)ã€åŸå§‹å¸‚åœºæ•°æ®(raw_data)å’Œç›®æ ‡æ”¶ç›Šç‡(target_ret),
        æ¨¡æ‹ŸçœŸå®äº¤æ˜“ç¯å¢ƒ(åŒ…æ‹¬æµåŠ¨æ€§é™åˆ¶ã€æ»‘ç‚¹ã€æ‰‹ç»­è´¹ç­‰),
        å¹¶è¾“å‡ºä¸€ä¸ªç»¼åˆé€‚åº”åº¦åˆ†æ•°(fitness score),å¸¸ç”¨äºè¿›åŒ–ç®—æ³•æˆ–å¼ºåŒ–å­¦ä¹ ä¸­çš„ç­–ç•¥è¯„ä¼°.
    """
    def __init__(self):
        self.trade_size = 1000.0
        self.min_liq = 500000.0
        self.base_fee = 0.0060

    def evaluate(self, factors, raw_data, target_ret):
        """
            å‚æ•°        ç±»å‹            shape           è§£é‡Š
            factors:    torch.Tensor    [B,T]           æ¨¡å‹è¾“å‡ºçš„åŸå§‹ä¿¡å·
            raw_data:   dict            --              åŒ…å«'liquidity'ç­‰åŸå§‹æ•°æ®
            target_ret: torch.Tensor    [B,T]           ç›®æ ‡æŒæœ‰æ”¶ç›Šç‡
        """
        # ç”Ÿæˆäº¤æ˜“ä¿¡æ¯
        liquidity = raw_data['liquidity']   # æµåŠ¨æ€§æŒ‡æ ‡
        signal = torch.sigmoid(factors)     # å°†ä¿¡å·å‹ç¼©åˆ°(0,1)
        is_safe = (liquidity > self.min_liq).float()    # æµåŠ¨æ€§è¾¾æ ‡? 1= å®‰å…¨,0=å±é™©
        position = (signal > 0.85).float() * is_safe    #ä»…å½“ä¿¡å·å¼ºçš„ä¼é¹…å®‰å…¨çš„æ—¶å€™æŒä»“
        # è®¡ç®—äº¤æ˜“æˆæœ¬(æ»‘ç‚¹+æ‰‹ç»­è´¹)
        impact_slippage = self.trade_size / (liquidity + 1e-9)
        impact_slippage = torch.clamp(impact_slippage, 0.0, 0.05) #æœ€å¤§æ»‘ç‚¹ 5%
        total_slippage_one_way = self.base_fee + impact_slippage
        # è®¡ç®—æ¢æ‰‹ç‡å’Œäº¤æ˜“æˆæœ¬
        prev_pos = torch.roll(position, 1, dims=1)
        prev_pos[:, 0] = 0 # ç¬¬ä¸€æ—¶é—´æ­¥æ— å‰åºæŒä»“
        turnover = torch.abs(position - prev_pos)   # 0->1 æˆ– 1->0 è¡¨ç¤ºäº¤æ˜“
        tx_cost = turnover * total_slippage_one_way
        # è®¡ç®—ç›ˆäº
        gross_pnl = position * target_ret # æŒä»“æœŸé—´æ¯›æ”¶ç›Š
        net_pnl = gross_pnl - tx_cost       # æ‰£é™¤äº¤æ˜“æˆæœ¬åçš„å‡€æ”¶ç›Š
        # æ„å»ºç»¼åˆè¯„åˆ†(fitness score)
        cum_ret = net_pnl.sum(dim=1)    # æ¯ä¸ªä»£å¸çš„ç´¯è®¡å‡€æ”¶ç›Š
        big_drawdowns = (net_pnl < -0.05).float().sum(dim=1)    # å•æœŸäºæŸ>5% çš„æ¬¡æ•°
        score = cum_ret - (big_drawdowns * 2.0) #æƒ©ç½šå¤§å›æ’¤
        # è¿‡æ»¤ä½æ´»è·ƒåº¦ç­–ç•¥
        activity = position.sum(dim=1) # æ€»äº¤æ˜“æ¬¡æ•°
        score = torch.where(activity < 5, torch.tensor(-10.0, device=score.device), score)
        # æœ€ç»ˆé€‚åº”åº¦
        final_fitness = torch.median(score)
        return final_fitness, cum_ret.mean().item()        
# engine.py
class AlphaEngine:
    def __init__(self, use_lord_regularization=True, lord_decay_rate=1e-3, lord_num_iterations=5):
        """
        Initialize AlphaGPT training engine.
        
        Args:
            use_lord_regularization: Enable Low-Rank Decay (LoRD) regularization
            lord_decay_rate: Strength of LoRD regularization
            lord_num_iterations: Number of Newton-Schulz iterations per step
        """
        self.loader = CryptoDataLoader()
        self.loader.load_data()
        
        self.model = AlphaGPT().to(ModelConfig.DEVICE)
        
        # Standard optimizer
        self.opt = torch.optim.AdamW(self.model.parameters(), lr=1e-3)
        # ipdb.set_trace()
        
        # Low-Rank Decay regularizer
        self.use_lord = use_lord_regularization
        if self.use_lord:
            self.lord_opt = NewtonSchulzLowRankDecay(
                self.model.named_parameters(),
                decay_rate=lord_decay_rate,
                num_iterations=lord_num_iterations,
                target_keywords=["q_proj", "k_proj", "attention", "qk_norm"]
            )
            self.rank_monitor = StableRankMonitor(
                self.model,
                target_keywords=["q_proj", "k_proj"]
            )
        else:
            self.lord_opt = None
            self.rank_monitor = None
        
        self.vm = StackVM()
        self.bt = MemeBacktest()
        
        self.best_score = -float('inf')
        self.best_formula = None
        self.training_history = {
            'step': [],
            'avg_reward': [],
            'best_score': [],
            'stable_rank': []
        }
    def train(self):
        print("ğŸš€ Starting Meme Alpha Mining with LoRD Regularization..." if self.use_lord else "ğŸš€ Starting Meme Alpha Mining...")
        if self.use_lord:
            print(f"   LoRD Regularization enabled")
            print(f"   Target keywords: ['q_proj', 'k_proj', 'attention', 'qk_norm']")
        
        pbar = tqdm(range(ModelConfig.TRAIN_STEPS))
        # ipdb.set_trace()
        
        for step in pbar:
            # ipdb.set_trace()
            bs = ModelConfig.BATCH_SIZE
            inp = torch.zeros((bs, 1), dtype=torch.long, device=ModelConfig.DEVICE)
            
            log_probs = []
            tokens_list = []
            
            for _ in range(ModelConfig.MAX_FORMULA_LEN):
                logits, _, _ = self.model(inp)
                dist = Categorical(logits=logits)
                action = dist.sample()
                
                log_probs.append(dist.log_prob(action))
                tokens_list.append(action)
                inp = torch.cat([inp, action.unsqueeze(1)], dim=1)
            
            seqs = torch.stack(tokens_list, dim=1)
            
            rewards = torch.zeros(bs, device=ModelConfig.DEVICE)
            
            for i in range(bs):
                formula = seqs[i].tolist()
                
                res = self.vm.execute(formula, self.loader.feat_tensor)
                
                if res is None:
                    rewards[i] = -5.0
                    continue
                
                if res.std() < 1e-4:
                    rewards[i] = -2.0
                    continue
                
                score, ret_val = self.bt.evaluate(res, self.loader.raw_data_cache, self.loader.target_ret)
                rewards[i] = score
                
                if score.item() > self.best_score:
                    self.best_score = score.item()
                    self.best_formula = formula
                    tqdm.write(f"[!] New King: Score {score:.2f} | Ret {ret_val:.2%} | Formula {formula}")
            
            # Normalize rewards
            adv = (rewards - rewards.mean()) / (rewards.std() + 1e-5)
            
            loss = 0
            for t in range(len(log_probs)):
                loss += -log_probs[t] * adv
            
            loss = loss.mean()
            
            # Gradient step
            self.opt.zero_grad()
            loss.backward()
            self.opt.step()
            
            # Apply Low-Rank Decay regularization
            if self.use_lord:
                self.lord_opt.step()
            
            # Logging
            avg_reward = rewards.mean().item()
            postfix_dict = {'AvgRew': f"{avg_reward:.3f}", 'BestScore': f"{self.best_score:.3f}"}
            
            if self.use_lord and step % 100 == 0:
                stable_rank = self.rank_monitor.compute()
                postfix_dict['Rank'] = f"{stable_rank:.2f}"
                self.training_history['stable_rank'].append(stable_rank)
            
            self.training_history['step'].append(step)
            self.training_history['avg_reward'].append(avg_reward)
            self.training_history['best_score'].append(self.best_score)
            
            pbar.set_postfix(postfix_dict)

        # Save best formula
        with open("best_meme_strategy.json", "w") as f:
            json.dump(self.best_formula, f)
        
        # Save training history
        import json as js
        with open("training_history.json", "w") as f:
            js.dump(self.training_history, f)
        
        print(f"\nâœ“ Training completed!")
        print(f"  Best score: {self.best_score:.4f}")
        print(f"  Best formula: {self.best_formula}")

if __name__ == "__main__":
    eng = AlphaEngine(use_lord_regularization=True)
    eng.train()
