{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import ipdb\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import sqlalchemy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "print(\"import success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e8274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "class ModelConfig:\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    DB_URL = f\"postgresql://{os.getenv('DB_USER','postgres')}:{os.getenv('DB_PASSWORD','password')}@{os.getenv('DB_HOST','localhost')}:5432/{os.getenv('DB_NAME','crypto_quant')}\"\n",
    "    BATCH_SIZE = 8192\n",
    "    TRAIN_STEPS = 1000\n",
    "    MAX_FORMULA_LEN = 12\n",
    "    TRADE_SIZE_USD = 1000.0\n",
    "    MIN_LIQUIDITY = 5000.0 # ä½äºæ­¤æµåŠ¨æ€§è§†ä¸ºå½’é›¶/æ— æ³•äº¤æ˜“\n",
    "    BASE_FEE = 0.005 # åŸºç¡€è´¹ç‡ 0.5% (Swap + Gas + Jito Tip)\n",
    "    INPUT_DIM = 6\n",
    "print(\"success config.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf48b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ops.py\n",
    "\"\"\"\n",
    "    ç”¨äºæ—¶é—´åºåˆ—ä¿¡å·å¤„ç†æˆ–è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹(å¦‚é‡åŒ–å› å­æŒ–æ˜ã€é—ä¼ ç¼–ç¨‹)çš„åŸå­æ“ä½œ(primitive operations),\n",
    "    å¹¶åˆ©ç”¨ PyTorch çš„ torch.jit.script ç¼–è¯‘ä¼˜åŒ–ä»¥æå‡è¿è¡Œæ•ˆç‡ã€‚\n",
    "    æ•´ä½“è®¾è®¡å¸¸è§äºè‡ªåŠ¨åŒ– alpha æŒ–æ˜ã€ç¨‹åºåˆæˆæˆ–ç¥ç»ç¬¦å·ç³»ç»Ÿä¸­ã€‚\n",
    "\"\"\"\n",
    "@torch.jit.script\n",
    "def _ts_delay(x: torch.Tensor, d: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "        åŠŸèƒ½:æ—¶é—´åºåˆ—å»¶è¿Ÿ(lag)æ“ä½œ\n",
    "        è¿”å› x å‘åç§»åŠ¨ d æœŸçš„ç»“æœ(å³ x[t-d] å¯¹åº”è¾“å‡º t æ—¶åˆ»)\n",
    "        è¾¹ç•Œå¤„ç†:å‰ d ä¸ªæ—¶é—´æ­¥ç”¨ 0 å¡«å……(å› æœ padding,æ— æœªæ¥ä¿¡æ¯æ³„éœ²)\n",
    "    \"\"\"\n",
    "    if d == 0: return x\n",
    "    pad = torch.zeros((x.shape[0], d), device=x.device)\n",
    "    result = torch.cat([pad, x[:, :-d]], dim=1)\n",
    "    # import ipdb ; ipdb.set_trace()\n",
    "    return result\n",
    "\n",
    "@torch.jit.script\n",
    "def _op_gate(condition: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "        åŠŸèƒ½:æ¡ä»¶é€‰æ‹©é—¨(ç±»ä¼¼ if-else)\n",
    "        - è‹¥ condition > 0,è¾“å‡º xï¼›å¦åˆ™è¾“å‡º y\n",
    "        - å‘é‡åŒ–å®ç°:é¿å… Python æ§åˆ¶æµ,æ”¯æŒ GPU å¹¶è¡Œ\n",
    "        æ•°å­¦ç­‰ä»·äº:\n",
    "        output={\n",
    "                 x if condition>0\n",
    "                 y otherwise\n",
    "                }\n",
    "        âœ… ç”¨äºæ„å»ºæ¡ä»¶é€»è¾‘(å¦‚â€œå¦‚æœæ³¢åŠ¨ç‡é«˜,åˆ™ç”¨ä¿å®ˆç­–ç•¥â€)\n",
    "    \"\"\"\n",
    "    mask = (condition > 0).float()\n",
    "    return mask * x + (1.0 - mask) * y\n",
    "\n",
    "@torch.jit.script\n",
    "def _op_jump(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "        åŠŸèƒ½:æ£€æµ‹â€œè·³è·ƒâ€æˆ–æç«¯å¼‚å¸¸å€¼\n",
    "        è®¡ç®—æ¯è¡Œ(æ¯ä¸ªä»£å¸)çš„ Z-score:   z=(x - Î¼)/Ïƒ\n",
    "        è‹¥   z>3 (å³åç¦»å‡å€¼ 3 ä¸ªæ ‡å‡†å·®ä»¥ä¸Š),è¾“å‡º z - 3 ï¼›å¦åˆ™è¾“å‡º 0\n",
    "        æ„ä¹‰:è¯†åˆ«ä»·æ ¼/æŒ‡æ ‡çš„çªå‘æ€§æš´æ¶¨æš´è·Œ(meme å¸å¸¸è§)\n",
    "        ğŸ“Œ è¾“å‡ºä¸ºéè´Ÿå€¼,è¶Šå¤§è¡¨ç¤ºâ€œè·³å¾—è¶ŠçŒ›â€\n",
    "    \"\"\"    \n",
    "    mean = x.mean(dim=1, keepdim=True)\n",
    "    std = x.std(dim=1, keepdim=True) + 1e-6\n",
    "    z = (x - mean) / std\n",
    "    return torch.relu(z - 3.0)\n",
    "\n",
    "@torch.jit.script\n",
    "def _op_decay(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    åŠŸèƒ½:æŒ‡æ•°è¡°å‡åŠ æƒ(è¿‘ä¼¼)\n",
    "        å½“å‰å€¼æƒé‡ 1.0,æ˜¨æ—¥ 0.8,å‰æ—¥ 0.6\n",
    "        è™½éä¸¥æ ¼æŒ‡æ•°è¡°å‡(å¦‚ EWMA),ä½†å®ç°äº†â€œè¿‘æœŸæ›´é‡è¦â€çš„æ€æƒ³\n",
    "        ç”¨é€”:å¹³æ»‘ä¿¡å·ã€èµ‹äºˆè¿‘æœŸæ•°æ®æ›´é«˜ä¼˜å…ˆçº§\n",
    "        ğŸ’¡ å¯è§†ä¸ºä¸€ä¸ªç®€å•çš„ FIR æ»¤æ³¢å™¨\n",
    "    \"\"\"\n",
    "    return x + 0.8 * _ts_delay(x, 1) + 0.6 * _ts_delay(x, 2)\n",
    "\n",
    "OPS_CONFIG = [\n",
    "    #    è¿™æ˜¯ä¸€ä¸ª æ“ä½œåŸè¯­åº“(primitive set),æ¯ä¸ªå…ƒç´ æ˜¯ä¸‰å…ƒç»„ (name, function, arity):\n",
    "    #    å­—æ®µ\tå«ä¹‰\n",
    "    #    name\tæ“ä½œåç§°(å­—ç¬¦ä¸²æ ‡è¯†)\n",
    "    #    function\tå¯è°ƒç”¨çš„å‡½æ•°(lambda æˆ– JIT å‡½æ•°)\n",
    "    #    arity\tæ‰€éœ€è¾“å…¥å‚æ•°ä¸ªæ•°(1=ä¸€å…ƒ,2=äºŒå…ƒ,3=ä¸‰å…ƒ)\n",
    "    ('ADD', lambda x, y: x + y, 2),\n",
    "    ('SUB', lambda x, y: x - y, 2),\n",
    "    ('MUL', lambda x, y: x * y, 2),\n",
    "    ('DIV', lambda x, y: x / (y + 1e-6), 2),\n",
    "    ('NEG', lambda x: -x, 1),\n",
    "    ('ABS', torch.abs, 1),\n",
    "    ('SIGN', torch.sign, 1),\n",
    "    ('GATE', _op_gate, 3),\n",
    "    ('JUMP', _op_jump, 1),\n",
    "    ('DECAY', _op_decay, 1),\n",
    "    ('DELAY1', lambda x: _ts_delay(x, 1), 1),\n",
    "    ('MAX3', lambda x: torch.max(x, torch.max(_ts_delay(x,1), _ts_delay(x,2))), 1)\n",
    "]\n",
    "print(\"success ops.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# factors.py\n",
    "class RMSNormFactor(nn.Module):\n",
    "    \"\"\"RMSNorm for factor normalization\n",
    "        RMS å½’ä¸€åŒ–å±‚\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        return (x / rms) * self.weight\n",
    "\n",
    "\n",
    "class MemeIndicators:\n",
    "    @staticmethod\n",
    "    def liquidity_health(liquidity, fdv):\n",
    "        \"\"\"\n",
    "            è¡¡é‡æµåŠ¨æ€§å¥åº·åº¦ â€”â€” æµåŠ¨æ€§æ± æ·±åº¦ç›¸å¯¹äºå®Œå…¨ç¨€é‡Šå¸‚å€¼(FDV)çš„æ¯”ä¾‹ã€‚\n",
    "            é€»è¾‘:\n",
    "                FDV è¶Šé«˜,é¡¹ç›®â€œç†è®ºå¸‚å€¼â€è¶Šå¤§\n",
    "                è‹¥æµåŠ¨æ€§è¿œå°äº FDV(å¦‚ FDV= 1B,æµåŠ¨æ€§= 10k),åˆ™ææ˜“è¢«ç ¸ç›˜(æ»‘ç‚¹å·¨å¤§)\n",
    "            å½’ä¸€åŒ–:\n",
    "                ratio * 4.0:å‡è®¾ç†æƒ³ ratio â‰ˆ 0.25(å³æµåŠ¨æ€§å  FDV çš„ 25%),æ­¤æ—¶è¾“å‡ºä¸º 1.0\n",
    "                clamp(0, 1):é™åˆ¶åœ¨ [0,1] åŒºé—´,0=æå·®,1=æå¥½\n",
    "                âœ… è¾“å‡ºå¯ä½œä¸ºâ€œæŠ—ç ¸ç›˜èƒ½åŠ›â€è¯„åˆ†\n",
    "            \"\"\"\n",
    "        ratio = liquidity / (fdv + 1e-6)\n",
    "        return torch.clamp(ratio * 4.0, 0.0, 1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def buy_sell_imbalance(close, open_, high, low):\n",
    "        \"\"\"\n",
    "            è¡¡é‡å•æ ¹Kçº¿çš„ä¹°å–åŠ›é‡ä¸å¹³è¡¡ç¨‹åº¦(ç±»ä¼¼\"å®ä½“å æ¯”\")ã€‚\n",
    "            è®¡ç®—:\n",
    "                body = close - open_:é˜³çº¿ä¸ºæ­£,é˜´çº¿ä¸ºè´Ÿ\n",
    "                range_hl = high - low:æ•´æ ¹Kçº¿æ³¢åŠ¨èŒƒå›´\n",
    "                strength = body / range_hl:å®ä½“å æ€»æŒ¯å¹…çš„æ¯”ä¾‹(âˆˆ [-1, 1])\n",
    "            éçº¿æ€§å‹ç¼©:\n",
    "                tanh(strength * 3):æ”¾å¤§ä¸­ç­‰å¼ºåº¦ä¿¡å·,æŠ‘åˆ¶æç«¯å€¼(å¹³æ»‘)\n",
    "            âœ… æ­£å€¼è¡¨ç¤ºä¹°æ–¹å¼ºåŠ¿,è´Ÿå€¼è¡¨ç¤ºå–æ–¹å¼ºåŠ¿\n",
    "        \"\"\"\n",
    "        range_hl = high - low + 1e-9\n",
    "        body = close - open_\n",
    "        strength = body / range_hl\n",
    "        return torch.tanh(strength * 3.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def fomo_acceleration(volume, window=5):\n",
    "        \"\"\"\n",
    "            æ£€æµ‹FOMO(é”™å¤±ææƒ§)æƒ…ç»ªçš„åŠ é€Ÿåº¦ â€”â€” æˆäº¤é‡å¢é•¿æ˜¯å¦åœ¨åŠ é€Ÿã€‚\n",
    "            æ­¥éª¤:\n",
    "                1, vol_chg:æˆäº¤é‡ç¯æ¯”å˜åŒ–ç‡(æ˜¨æ—¥ä¸ºåˆ†æ¯,+1 é˜²é™¤é›¶)\n",
    "                2, acc = Î”(vol_chg):æˆäº¤é‡å˜åŒ–ç‡çš„äºŒé˜¶å¯¼æ•°(åŠ é€Ÿåº¦)\n",
    "            æ„ä¹‰:\n",
    "                1, acc > 0:FOMO åœ¨åŠ å‰§(è¶Šæ¥è¶Šå¤šäººå†²å…¥)\n",
    "                2, acc < 0:FOMO å‡é€€(çƒ­åº¦ä¸‹é™)\n",
    "            è£å‰ª:clamp(-5, 5) é˜²æ­¢å¼‚å¸¸å€¼å¹²æ‰°\n",
    "            âš ï¸ æ³¨æ„:torch.roll ä¼šå¯¼è‡´é¦–åˆ—ä½¿ç”¨æœ€åä¸€åˆ—æ•°æ®(å¾ªç¯ç§»ä½),å®é™…åº” paddingã€‚ä½†æ­¤å¤„å¯èƒ½æ¥å—è¾¹ç•Œè¯¯å·®ã€‚\n",
    "        \"\"\"\n",
    "        vol_prev = torch.roll(volume, 1, dims=1)\n",
    "        vol_chg = (volume - vol_prev) / (vol_prev + 1.0)\n",
    "        acc = vol_chg - torch.roll(vol_chg, 1, dims=1)\n",
    "        return torch.clamp(acc, -5.0, 5.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def pump_deviation(close, window=20):\n",
    "        \"\"\"\n",
    "            è¡¡é‡ä»·æ ¼ç›¸å¯¹äºç§»åŠ¨å¹³å‡çº¿çš„åç¦»ç¨‹åº¦(åˆ¤æ–­æ˜¯å¦â€œæš´æ¶¨â€æˆ–â€œè¶…ä¹°â€)ã€‚\n",
    "            å®ç°ç»†èŠ‚:\n",
    "                æ‰‹åŠ¨ padding(å‰è¡¥ 0)â†’ è®¡ç®—ä»ç¬¬ window å¤©å¼€å§‹çš„ MA\n",
    "                unfold(1, window, 1):æ»‘åŠ¨çª—å£æå–å­åºåˆ—\n",
    "                dev = (price - MA) / MA:ç›¸å¯¹åç¦»ç‡(ç±»ä¼¼å¸ƒæ—å¸¦ z-score)\n",
    "            ç”¨é€”:\n",
    "                dev >> 0:ä»·æ ¼è¿œé«˜äºå‡çº¿ â†’ å¯èƒ½è¿‡çƒ­\n",
    "                dev << 0:ä»·æ ¼è¿œä½äºå‡çº¿ â†’ å¯èƒ½è¶…å–\n",
    "            âœ… å…¸å‹â€œè¿½æ¶¨æ€è·Œâ€ä¿¡å·æº\n",
    "        \"\"\"\n",
    "        pad = torch.zeros((close.shape[0], window-1), device=close.device)\n",
    "        c_pad = torch.cat([pad, close], dim=1)\n",
    "        ma = c_pad.unfold(1, window, 1).mean(dim=-1)\n",
    "        dev = (close - ma) / (ma + 1e-9)\n",
    "        return dev\n",
    "\n",
    "    @staticmethod\n",
    "    def volatility_clustering(close, window=10):\n",
    "        \"\"\"Detect volatility clustering patterns\n",
    "            æ£€æµ‹æ³¢åŠ¨ç‡èšé›†æ•ˆåº”(é‡‘èç»å…¸ç°è±¡:é«˜æ³¢åŠ¨åå¾€å¾€æ¥é«˜æ³¢åŠ¨)ã€‚\n",
    "            æ­¥éª¤:\n",
    "                1, è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡ ret = log(P_t / P_{t-1})\n",
    "                2, å¹³æ–¹æ”¶ç›Šç‡ ret_sq â†’ ä»£è¡¨ç¬æ—¶æ³¢åŠ¨ç‡\n",
    "                3, å¯¹ ret_sq åšç§»åŠ¨å¹³å‡ â†’ å¾—åˆ°å±€éƒ¨æ³¢åŠ¨ç‡ä¼°è®¡\n",
    "                4, å¼€æ–¹ â†’ è¿‘ä¼¼æ ‡å‡†å·®(æ³¢åŠ¨ç‡)\n",
    "            è¾“å‡º:æ»šåŠ¨çª—å£å†…çš„å†å²æ³¢åŠ¨ç‡\n",
    "            âœ… ç”¨äºè¯†åˆ«â€œé«˜æ³¢åŠ¨æœŸâ€,å¯èƒ½ä¼´éš meme å¸å‰§çƒˆç‚’ä½œ\n",
    "        \"\"\"\n",
    "        ret = torch.log(close / (torch.roll(close, 1, dims=1) + 1e-9))\n",
    "        ret_sq = ret ** 2\n",
    "        \n",
    "        pad = torch.zeros((ret_sq.shape[0], window-1), device=close.device)\n",
    "        ret_sq_pad = torch.cat([pad, ret_sq], dim=1)\n",
    "        vol_ma = ret_sq_pad.unfold(1, window, 1).mean(dim=-1)\n",
    "        \n",
    "        return torch.sqrt(vol_ma + 1e-9)\n",
    "\n",
    "    @staticmethod\n",
    "    def momentum_reversal(close, window=5):\n",
    "        \"\"\"Capture momentum reversal signals\n",
    "            æ£€æµ‹åŠ¨é‡åè½¬ä¿¡å· â€”â€” ä¸Šæ¶¨/ä¸‹è·Œè¶‹åŠ¿æ˜¯å¦çªç„¶è½¬å‘ã€‚\n",
    "            é€»è¾‘:\n",
    "                1, mom:è¿‡å» window å¤©çš„ç´¯è®¡æ”¶ç›Šç‡(åŠ¨é‡æ–¹å‘)\n",
    "                2, mom_prev:å‰ä¸€å¤©çš„åŠ¨é‡\n",
    "                3, mom * mom_prev < 0:ç¬¦å·ç›¸å â†’ å‘ç”ŸåŠ¨é‡åè½¬\n",
    "            è¾“å‡º:äºŒå€¼ä¿¡å·(1=åè½¬,0=æ— åè½¬)\n",
    "            âœ… ç”¨äºæ•æ‰â€œè§é¡¶å›è½â€æˆ–â€œæ­¢è·Œåå¼¹â€çš„è½¬æŠ˜ç‚¹\n",
    "        \"\"\"\n",
    "        ret = torch.log(close / (torch.roll(close, 1, dims=1) + 1e-9))\n",
    "        \n",
    "        pad = torch.zeros((ret.shape[0], window-1), device=close.device)\n",
    "        ret_pad = torch.cat([pad, ret], dim=1)\n",
    "        mom = ret_pad.unfold(1, window, 1).sum(dim=-1)\n",
    "        \n",
    "        # Detect reversals\n",
    "        mom_prev = torch.roll(mom, 1, dims=1)\n",
    "        reversal = (mom * mom_prev < 0).float()\n",
    "        \n",
    "        return reversal\n",
    "\n",
    "    @staticmethod\n",
    "    def relative_strength(close, high, low, window=14):\n",
    "        \"\"\"RSI-like indicator for strength detection\n",
    "            å®ç° RSI(ç›¸å¯¹å¼ºå¼±æŒ‡æ•°) çš„å˜ä½“,ç”¨äºåˆ¤æ–­è¶…ä¹°/è¶…å–ã€‚\n",
    "            æ ‡å‡† RSI æ­¥éª¤:\n",
    "                1, è®¡ç®—æ¯æ—¥æ¶¨è·Œ(gains, losses)\n",
    "                2, è®¡ç®— window æ—¥å¹³å‡æ¶¨å¹…/è·Œå¹…\n",
    "                3, RS = avg_gain / avg_loss\n",
    "                4, RSI = 100 - 100/(1+RS)\n",
    "            å½’ä¸€åŒ–:\n",
    "                (RSI - 50) / 50 â†’ å°† [0,100] æ˜ å°„åˆ° [-1, 1]\n",
    "                    -1:æç«¯è¶…å–\n",
    "                    0:ä¸­æ€§\n",
    "                    +1:æç«¯è¶…ä¹°\n",
    "            âœ… ç»å…¸éœ‡è¡æŒ‡æ ‡,é€‚ç”¨äº meme å¸çš„é«˜æ³¢åŠ¨ç¯å¢ƒ        \n",
    "        \"\"\"\n",
    "        ret = close - torch.roll(close, 1, dims=1)\n",
    "        \n",
    "        gains = torch.relu(ret)\n",
    "        losses = torch.relu(-ret)\n",
    "        \n",
    "        pad = torch.zeros((gains.shape[0], window-1), device=close.device)\n",
    "        gains_pad = torch.cat([pad, gains], dim=1)\n",
    "        losses_pad = torch.cat([pad, losses], dim=1)\n",
    "        \n",
    "        avg_gain = gains_pad.unfold(1, window, 1).mean(dim=-1)\n",
    "        avg_loss = losses_pad.unfold(1, window, 1).mean(dim=-1)\n",
    "        \n",
    "        rs = (avg_gain + 1e-9) / (avg_loss + 1e-9)\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        return (rsi - 50) / 50  # Normalize\n",
    "\n",
    "\n",
    "class AdvancedFactorEngineer:\n",
    "    \"\"\"Advanced feature engineering with multiple factor types\n",
    "        ä»åŸå§‹å¸‚åœºæ•°æ®ä¸­æ„å»ºä¸€ä¸ª12ç»´çš„é«˜çº§ç‰¹å¾ç©ºé—´(feature space),ç‰¹åˆ«é’ˆå¯¹é«˜æ³¢åŠ¨æ€§èµ„äº§(å¦‚ meme å¸)è®¾è®¡ã€‚\n",
    "        å®ƒç»“åˆäº†åŸºç¡€æŠ€æœ¯æŒ‡æ ‡ã€è¡Œä¸ºé‡‘èä¿¡å·å’Œç¨³å¥å½’ä¸€åŒ–æ–¹æ³•,\n",
    "        è¾“å‡ºå¯ç›´æ¥ç”¨äºæœºå™¨å­¦ä¹ æ¨¡å‹(å¦‚é¢„æµ‹ã€åˆ†ç±»æˆ–å¼ºåŒ–å­¦ä¹ )çš„æ ‡å‡†åŒ–ç‰¹å¾å¼ é‡.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.rms_norm = RMSNormFactor(1)\n",
    "    \n",
    "    def robust_norm(self, t):\n",
    "        \"\"\"Robust normalization using median absolute deviation\n",
    "            ç¨³å¥å½’ä¸€åŒ–(åŸºäºä¸­ä½æ•°å’Œ MAD)\n",
    "            å¯¹è¾“å…¥å¼ é‡ t(shape: [num_tokens, time_steps])è¿›è¡Œç¨³å¥æ ‡å‡†åŒ–,é¿å…å—æç«¯å€¼(outliers)å½±å“ã€‚\n",
    "            æ­¥éª¤:\n",
    "                1,è®¡ç®—æ¯è¡Œ(æ¯ä¸ªä»£å¸)çš„ä¸­ä½æ•° â†’ æ¯”å‡å€¼æ›´æŠ—å¼‚å¸¸å€¼ã€‚\n",
    "                2, è®¡ç®— MAD(Median Absolute Deviation):\n",
    "                    MAD = median(|Xi - median(X)|)\n",
    "                3,æ ‡å‡†åŒ–:(x - median) / MAD\n",
    "            è£å‰ª:é™åˆ¶åœ¨ [-5, 5],é˜²æ­¢æ®‹ä½™å¼‚å¸¸å€¼å¹²æ‰°æ¨¡å‹ã€‚\n",
    "            âœ… ä¸ºä»€ä¹ˆç”¨ MAD?\n",
    "            Meme å¸ä»·æ ¼å¸¸å‡ºç°æš´æ¶¨æš´è·Œ(å¦‚ 100x),å‡å€¼å’Œæ ‡å‡†å·®ä¼šè¢«ä¸¥é‡æ‰­æ›²ã€‚MAD å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ,æ›´é€‚åˆæ­¤ç±»æ•°æ®ã€‚            \n",
    "        \"\"\"\n",
    "        median = torch.nanmedian(t, dim=1, keepdim=True)[0]\n",
    "        mad = torch.nanmedian(torch.abs(t - median), dim=1, keepdim=True)[0] + 1e-6\n",
    "        norm = (t - median) / mad\n",
    "        return torch.clamp(norm, -5.0, 5.0)\n",
    "    \n",
    "    def compute_advanced_features(self, raw_dict):\n",
    "        \"\"\"Compute 12-dimensional feature space with advanced factors æ„å»º12ç»´ç‰¹å¾\n",
    "        \"\"\"\n",
    "        c = raw_dict['close']\n",
    "        o = raw_dict['open']\n",
    "        h = raw_dict['high']\n",
    "        l = raw_dict['low']\n",
    "        v = raw_dict['volume']\n",
    "        liq = raw_dict['liquidity']\n",
    "        fdv = raw_dict['fdv']\n",
    "        \n",
    "        # Basic factors è®¡ç®—åŸºç¡€å› å­(Basic Factors)\n",
    "        ret = torch.log(c / (torch.roll(c, 1, dims=1) + 1e-9)) # å¯¹æ•°æ”¶ç›Šç‡\n",
    "        liq_score = MemeIndicators.liquidity_health(liq, fdv)   # æµåŠ¨æ€§å¥åº·åº¦\n",
    "        pressure = MemeIndicators.buy_sell_imbalance(c, o, h, l) # ä¹°å–å‹åŠ›\n",
    "        fomo = MemeIndicators.fomo_acceleration(v) # fomo åŠ é€Ÿåº¦\n",
    "        dev = MemeIndicators.pump_deviation(c)  # ä»·æ ¼åç¦»å‡çº¿ç¨‹åº¦\n",
    "        log_vol = torch.log1p(v) # torch.log1p(v) = log(1 + v),é¿å… v=0 æ—¶ log(0) é—®é¢˜,åŒæ—¶å‹ç¼©å¤§æˆäº¤é‡ã€‚\n",
    "        \n",
    "        # Advanced factors\n",
    "        vol_cluster = MemeIndicators.volatility_clustering(c) # æ³¢åŠ¨ç‡èšé›†(å±€éƒ¨æ³¢åŠ¨ç‡)\n",
    "        momentum_rev = MemeIndicators.momentum_reversal(c) # åŠ¨é‡åè½¬(0 æˆ– 1)\n",
    "        rel_strength = MemeIndicators.relative_strength(c, h, l) # ç›¸å¯¹å¼ºå¼±(RSI å˜ä½“,-1ï½1)\n",
    "        \n",
    "        # High-low range æ—¥å†…æŒ¯å¹…å æ”¶ç›˜ä»·æ¯”ä¾‹\n",
    "        hl_range = (h - l) / (c + 1e-9)\n",
    "        \n",
    "        # Close position in range æ”¶ç›˜ä»·åœ¨æ—¥å†…åŒºé—´çš„ä½ç½®(0ï½1)\n",
    "        close_pos = (c - l) / (h - l + 1e-9)\n",
    "            # close_pos æ˜¯ç»å…¸â€œKçº¿ä½ç½®â€æŒ‡æ ‡ï¼šæ¥è¿‘ 1 è¡¨ç¤ºå¼ºåŠ¿æ”¶æ¶¨,æ¥è¿‘ 0 è¡¨ç¤ºå¼±åŠ¿æ”¶è·Œã€‚\n",
    "        \n",
    "        # Volume trend æˆäº¤é‡å˜åŒ–ç‡\n",
    "        vol_prev = torch.roll(v, 1, dims=1)\n",
    "        vol_trend = (v - vol_prev) / (vol_prev + 1.0)\n",
    "        \n",
    "        # å †å æ‰€æœ‰ç‰¹å¾ \n",
    "        features = torch.stack([\n",
    "            self.robust_norm(ret),\n",
    "            liq_score,\n",
    "            pressure,\n",
    "            self.robust_norm(fomo),\n",
    "            self.robust_norm(dev),\n",
    "            self.robust_norm(log_vol),\n",
    "            self.robust_norm(vol_cluster),\n",
    "            momentum_rev,\n",
    "            self.robust_norm(rel_strength),\n",
    "            self.robust_norm(hl_range),\n",
    "            close_pos,\n",
    "            self.robust_norm(vol_trend)\n",
    "        ], dim=1)\n",
    "        # torch.stack(..., dim=1) å°† 12 ä¸ª [B, T] å¼ é‡å †å æˆ [B, 12, T],\n",
    "        # è¿™æ˜¯æ—¶é—´åºåˆ—æ¨¡å‹(å¦‚ Transformerã€CNN)çš„æ ‡å‡†è¾“å…¥æ ¼å¼ã€‚\n",
    "        \n",
    "        return features\n",
    "\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "        ä»åŸå§‹å¸‚åœºæ•°æ®(å¦‚ K çº¿ã€æµåŠ¨æ€§ç­‰)ä¸­é«˜æ•ˆæå–ä¸€ç»„æ ‡å‡†åŒ–çš„ 6 ç»´æŠ€æœ¯ç‰¹å¾(factors),\n",
    "        ä¸“ä¸ºé«˜æ³¢åŠ¨æ€§åŠ å¯†èµ„äº§(å¦‚ meme å¸)è®¾è®¡ã€‚\n",
    "        å…¶æ ¸å¿ƒç›®æ ‡æ˜¯å°†åŸå§‹è¡Œæƒ…æ•°æ®è½¬æ¢ä¸ºæ•°å€¼ç¨³å®šã€é‡çº²ç»Ÿä¸€ã€æŠ—å¼‚å¸¸å€¼çš„ç‰¹å¾å¼ é‡,å¯ç›´æ¥è¾“å…¥æœºå™¨å­¦ä¹ æ¨¡å‹(å¦‚é¢„æµ‹ç½‘ç»œã€å¼ºåŒ–å­¦ä¹ ç­–ç•¥ç­‰)ã€‚\n",
    "    \"\"\"\n",
    "    INPUT_DIM = 6\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_features(raw_dict):\n",
    "        c = raw_dict['close']\n",
    "        o = raw_dict['open']\n",
    "        h = raw_dict['high']\n",
    "        l = raw_dict['low']\n",
    "        v = raw_dict['volume']\n",
    "        liq = raw_dict['liquidity']\n",
    "        fdv = raw_dict['fdv']\n",
    "        \n",
    "        ret = torch.log(c / (torch.roll(c, 1, dims=1) + 1e-9)) #  å¯¹æ•°æ”¶ç›Šç‡(Log Return)\n",
    "        liq_score = MemeIndicators.liquidity_health(liq, fdv) # æµåŠ¨æ€§å¥åº·åº¦\n",
    "        pressure = MemeIndicators.buy_sell_imbalance(c, o, h, l) # ä¹°å–å‹åŠ›(Kçº¿å®ä½“å¼ºåº¦)\n",
    "        fomo = MemeIndicators.fomo_acceleration(v) # FOMO æƒ…ç»ªåŠ é€Ÿåº¦\n",
    "        dev = MemeIndicators.pump_deviation(c) # ä»·æ ¼åç¦»åº¦(æ˜¯å¦æš´æ¶¨ï¼Ÿ)\n",
    "        log_vol = torch.log1p(v) # å¯¹æ•°æˆäº¤é‡\n",
    "        \n",
    "        def robust_norm(t):\n",
    "            median = torch.nanmedian(t, dim=1, keepdim=True)[0]\n",
    "            mad = torch.nanmedian(torch.abs(t - median), dim=1, keepdim=True)[0] + 1e-6\n",
    "            norm = (t - median) / mad\n",
    "            return torch.clamp(norm, -5.0, 5.0)\n",
    "\n",
    "        features = torch.stack([\n",
    "            robust_norm(ret),\n",
    "            liq_score,\n",
    "            pressure,\n",
    "            robust_norm(fomo),\n",
    "            robust_norm(dev),\n",
    "            robust_norm(log_vol)\n",
    "        ], dim=1)\n",
    "        \n",
    "        return features\n",
    "print(\"success factors.py\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cf9819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader.py\n",
    "class CryptoDataLoader:\n",
    "    def __init__(self):\n",
    "        self.engine = sqlalchemy.create_engine(ModelConfig.DB_URL)\n",
    "        self.feat_tensor = None\n",
    "        self.raw_data_cache = None\n",
    "        self.target_ret = None\n",
    "        \n",
    "    def load_data(self, limit_tokens=500):\n",
    "        print(\"Loading data from SQL...\")\n",
    "        top_query = f\"\"\"\n",
    "        SELECT address FROM tokens \n",
    "        LIMIT {limit_tokens} \n",
    "        \"\"\"\n",
    "        addrs = pd.read_sql(top_query, self.engine)['address'].tolist()\n",
    "        if not addrs: raise ValueError(\"No tokens found.\")\n",
    "        addr_str = \"'\" + \"','\".join(addrs) + \"'\"\n",
    "        data_query = f\"\"\"\n",
    "        SELECT time, address, open, high, low, close, volume, liquidity, fdv\n",
    "        FROM ohlcv\n",
    "        WHERE address IN ({addr_str})\n",
    "        ORDER BY time ASC\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(data_query, self.engine)\n",
    "        def to_tensor(col):\n",
    "            pivot = df.pivot(index='time', columns='address', values=col)\n",
    "            pivot = pivot.fillna(0.0)\n",
    "            return torch.tensor(pivot.values.T, dtype=torch.float32, device=ModelConfig.DEVICE)\n",
    "        self.raw_data_cache = {\n",
    "            'open': to_tensor('open'),\n",
    "            'high': to_tensor('high'),\n",
    "            'low': to_tensor('low'),\n",
    "            'close': to_tensor('close'),\n",
    "            'volume': to_tensor('volume'),\n",
    "            'liquidity': to_tensor('liquidity'),\n",
    "            'fdv': to_tensor('fdv')\n",
    "        }\n",
    "        self.feat_tensor = FeatureEngineer.compute_features(self.raw_data_cache)\n",
    "        op = self.raw_data_cache['open']\n",
    "        t1 = torch.roll(op, -1, dims=1)\n",
    "        t2 = torch.roll(op, -2, dims=1)\n",
    "        self.target_ret = torch.log(t2 / (t1 + 1e-9))\n",
    "        self.target_ret[:, -2:] = 0.0\n",
    "        print(f\"Data Ready. Shape: {self.feat_tensor.shape}\")\n",
    "\n",
    "print(\"success data_loader.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08069adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphagpt.py\n",
    "class NewtonSchulzLowRankDecay:\n",
    "    \"\"\"\n",
    "    Low-Rank Decay (LoRD) using Newton-Schulz iteration.\n",
    "    \n",
    "    A more efficient regularization method that targets low-rank structure\n",
    "    in attention and key parameters. Uses Newton-Schulz iteration to compute\n",
    "    the minimum singular vectors without explicit SVD.\n",
    "    \n",
    "    Args:\n",
    "        named_parameters: Model's named parameters\n",
    "        decay_rate: Strength of low-rank decay\n",
    "        num_iterations: Number of Newton-Schulz iterations (default: 5)\n",
    "        target_keywords: If specified, only decay parameters matching these keywords\n",
    "\n",
    "    åä¸º AlphaGPT çš„ç¥ç»ç½‘ç»œæ¨¡å‹,ä¸“ä¸ºè‡ªåŠ¨ç”Ÿæˆäº¤æ˜“ç­–ç•¥å…¬å¼(å¦‚é‡åŒ–å› å­) è€Œè®¾è®¡ã€‚\n",
    "        å®ƒç»“åˆäº†å¤šç§å…ˆè¿›æ¶æ„æŠ€æœ¯(å¦‚ Looped Transformerã€QK-Normã€SwiGLUã€MTP Head ç­‰),\n",
    "        å¹¶å¼•å…¥äº†ä½ç§©æ­£åˆ™åŒ–(LoRD) å’Œç¨³å®šç§©ç›‘æ§æœºåˆ¶,\n",
    "        ä»¥æå‡æ¨¡å‹åœ¨å°æ ·æœ¬ã€é«˜å™ªå£°é‡‘èæ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›\n",
    "    \"\"\"\n",
    "    def __init__(self, named_parameters, decay_rate=1e-3, num_iterations=5, target_keywords=None):\n",
    "        self.decay_rate = decay_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.target_keywords = target_keywords or [\"qk_norm\", \"attention\"]\n",
    "        self.params_to_decay = []\n",
    "        \n",
    "        for name, param in named_parameters:\n",
    "            if not param.requires_grad or param.ndim != 2:\n",
    "                continue\n",
    "            if not any(k in name for k in self.target_keywords):\n",
    "                continue\n",
    "            self.params_to_decay.append((name, param))\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Apply Newton-Schulz low-rank decay to attention parameters.\"\"\"\n",
    "        for name, W in self.params_to_decay:\n",
    "            orig_dtype = W.dtype\n",
    "            X = W.float()\n",
    "            r, c = X.shape\n",
    "            \n",
    "            # Transpose if needed for efficiency\n",
    "            transposed = False\n",
    "            if r > c:\n",
    "                X = X.T\n",
    "                transposed = True\n",
    "            \n",
    "            # Normalize by spectral norm\n",
    "            norm = X.norm() + 1e-8\n",
    "            X = X / norm\n",
    "            \n",
    "            # Initialize Y for Newton-Schulz iteration\n",
    "            Y = X\n",
    "            I = torch.eye(X.shape[-1], device=X.device, dtype=X.dtype)\n",
    "            \n",
    "            # Newton-Schulz iteration: Y_{k+1} = 0.5 * Y_k * (3*I - Y_k^T * Y_k)\n",
    "            # This converges to the orthogonal matrix with same singular vectors\n",
    "            for _ in range(self.num_iterations):\n",
    "                A = Y.T @ Y\n",
    "                Y = 0.5 * Y @ (3.0 * I - A)\n",
    "            \n",
    "            if transposed:\n",
    "                Y = Y.T\n",
    "            \n",
    "            # Apply low-rank decay\n",
    "            W.sub_(self.decay_rate * Y.to(orig_dtype))\n",
    "\n",
    "\n",
    "class StableRankMonitor:\n",
    "    \"\"\"Monitor the effective rank (stable rank) of model parameters.\"\"\"\n",
    "    def __init__(self, model, target_keywords=None):\n",
    "        self.model = model\n",
    "        self.target_keywords = target_keywords or [\"q_proj\", \"k_proj\", \"attention\"]\n",
    "        self.history = []\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def compute(self):\n",
    "        \"\"\"Compute average stable rank of target parameters.\"\"\"\n",
    "        ranks = []\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.ndim != 2:\n",
    "                continue\n",
    "            if not any(k in name for k in self.target_keywords):\n",
    "                continue\n",
    "            \n",
    "            W = param.detach().float()\n",
    "            S = torch.linalg.svdvals(W)\n",
    "            # Stable Rank = ||W||_F^2 / ||W||_2^2\n",
    "            stable_rank = (S.norm() ** 2) / (S[0] ** 2 + 1e-9)\n",
    "            ranks.append(stable_rank.item())\n",
    "        \n",
    "        avg_rank = sum(ranks) / len(ranks) if ranks else 0.0\n",
    "        self.history.append(avg_rank)\n",
    "        return avg_rank\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization\"\"\"\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        return (x / rms) * self.weight\n",
    "\n",
    "\n",
    "class QKNorm(nn.Module):\n",
    "    \"\"\"Query-Key Normalization for Attention\"\"\"\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(1, 1, 1, d_model) * (d_model ** -0.5))\n",
    "    \n",
    "    def forward(self, q, k):\n",
    "        # Normalize Q and K independently\n",
    "        q_norm = F.normalize(q, p=2, dim=-1)\n",
    "        k_norm = F.normalize(k, p=2, dim=-1)\n",
    "        return q_norm * self.scale, k_norm * self.scale\n",
    "\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"Swish GLU activation function\"\"\"\n",
    "    def __init__(self, d_in, d_ff):\n",
    "        super().__init__()\n",
    "        self.w = nn.Linear(d_in, d_ff * 2)\n",
    "        self.fc = nn.Linear(d_ff, d_in)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_glu = self.w(x)\n",
    "        x, gate = x_glu.chunk(2, dim=-1)\n",
    "        x = x * F.silu(gate)  # Swish activation\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class MTPHead(nn.Module):\n",
    "    \"\"\"Multi-Task Pooling Head for multi-objective learning\"\"\"\n",
    "    def __init__(self, d_model, vocab_size, num_tasks=3):\n",
    "        super().__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "        self.task_heads = nn.ModuleList([\n",
    "            nn.Linear(d_model, vocab_size) for _ in range(num_tasks)\n",
    "        ])\n",
    "        self.task_weights = nn.Parameter(torch.ones(num_tasks) / num_tasks)\n",
    "        self.task_router = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model // 2, num_tasks)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Route to appropriate task heads\n",
    "        task_logits = self.task_router(x)\n",
    "        task_probs = F.softmax(task_logits, dim=-1)\n",
    "        \n",
    "        # Compute all task outputs\n",
    "        task_outputs = [head(x) for head in self.task_heads]\n",
    "        task_outputs = torch.stack(task_outputs, dim=1)  # [B, num_tasks, vocab_size]\n",
    "        \n",
    "        # Weighted combination\n",
    "        weighted = (task_probs.unsqueeze(-1) * task_outputs).sum(dim=1)\n",
    "        return weighted, task_probs\n",
    "\n",
    "\n",
    "class LoopedTransformerLayer(nn.Module):\n",
    "    \"\"\"Looped Transformer Layer - recurrent processing within a layer\"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, num_loops=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_loops = num_loops\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        \n",
    "        # QK-Norm attention\n",
    "        self.qk_norm = QKNorm(d_model // nhead)\n",
    "        \n",
    "        # Standard attention components\n",
    "        self.attention = nn.MultiheadAttention(d_model, nhead, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # RMSNorm instead of LayerNorm\n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "        \n",
    "        # SwiGLU FFN instead of standard FFN\n",
    "        self.ffn = SwiGLU(d_model, dim_feedforward)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None, is_causal=False):\n",
    "        # Looped processing - recurrent refinement\n",
    "        for _ in range(self.num_loops):\n",
    "            # Self-attention with residual\n",
    "            x_norm = self.norm1(x)\n",
    "            attn_out, _ = self.attention(x_norm, x_norm, x_norm, attn_mask=mask, is_causal=is_causal)\n",
    "            x = x + self.dropout(attn_out)\n",
    "            \n",
    "            # FFN with residual\n",
    "            x_norm = self.norm2(x)\n",
    "            ffn_out = self.ffn(x_norm)\n",
    "            x = x + self.dropout(ffn_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class LoopedTransformer(nn.Module):\n",
    "    \"\"\"Looped Transformer Encoder with multiple loop iterations\"\"\"\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, num_loops=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            LoopedTransformerLayer(d_model, nhead, dim_feedforward, num_loops, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, mask=None, is_causal=False):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask, is_causal=is_causal)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AlphaGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d_model = 64\n",
    "        self.features_list = ['RET', 'VOL', 'V_CHG', 'PV', 'TREND']\n",
    "        self.ops_list = [cfg[0] for cfg in OPS_CONFIG]\n",
    "        \n",
    "        self.vocab = self.features_list + self.ops_list\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        # Embedding\n",
    "        self.token_emb = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, ModelConfig.MAX_FORMULA_LEN + 1, self.d_model))\n",
    "        \n",
    "        # Enhanced Transformer with Looped Transformer\n",
    "        self.blocks = LoopedTransformer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=4,\n",
    "            num_layers=2,\n",
    "            dim_feedforward=128,\n",
    "            num_loops=3,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # RMSNorm instead of LayerNorm\n",
    "        self.ln_f = RMSNorm(self.d_model)\n",
    "        \n",
    "        # MTPHead for multi-task output\n",
    "        self.mtp_head = MTPHead(self.d_model, self.vocab_size, num_tasks=3)\n",
    "        self.head_critic = nn.Linear(self.d_model, 1)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # idx: [Batch, SeqLen]\n",
    "        B, T = idx.size()\n",
    "        \n",
    "        x = self.token_emb(idx) + self.pos_emb[:, :T, :]\n",
    "        \n",
    "        # Causal Mask\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(T).to(idx.device)\n",
    "        \n",
    "        # Process through looped transformer\n",
    "        x = self.blocks(x, mask=mask, is_causal=True)\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        last_emb = x[:, -1, :]\n",
    "        \n",
    "        # Multi-task pooling head for logits\n",
    "        logits, task_probs = self.mtp_head(last_emb)\n",
    "        value = self.head_critic(last_emb)\n",
    "        \n",
    "        return logits, value, task_probs\n",
    "print(\"success alphagpt.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b11c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vm.py\n",
    "\n",
    "class StackVM:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        feat_offset = 6\n",
    "            åˆå§‹åŒ–æ˜ å°„è¡¨\n",
    "            token 0~5 : è¾“å…¥ç‰¹å¾\n",
    "            token 6+ æ“ä½œç¬¦\n",
    "        \"\"\"\n",
    "        self.feat_offset = FeatureEngineer.INPUT_DIM\n",
    "        self.op_map = {i + self.feat_offset: cfg[1] for i, cfg in enumerate(OPS_CONFIG)}\n",
    "        # å°†æ“ä½œæ•°tokenæ˜ å°„åˆ°å‡½æ•°\n",
    "        self.arity_map = {i + self.feat_offset: cfg[2] for i, cfg in enumerate(OPS_CONFIG)}\n",
    "        # è®°å½•æ¯ä¸ªæ“ä½œç¬¦éœ€è¦å‡ ä¸ªå‚æ•°\n",
    "\n",
    "    def execute(self, formula_tokens, feat_tensor): # æ‰§è¡Œå¼•æ“\n",
    "        \"\"\"            \n",
    "            è¾“å…¥:\n",
    "                formula_tokens: List[int] , è¡¨ç¤ºå…¬å¼çš„token åºåˆ—(åç¼€è¡¨è¾¾å¼/é€†æ³¢å…°è¡¨è¾¾å¼)\n",
    "                feat_tensor:  Tensor, shape[B,6,T], ç”±Feature Engineer.compute_feature() ç”Ÿæˆ.\n",
    "            è¾“å‡º:\n",
    "                æˆåŠŸ: Tensor , shape[B,T] (æ¯ä¸€ä¸ªä»£å¸çš„æ—¶é—´åºåˆ—ä¿¡å·)\n",
    "                å¤±è´¥: None(è¯­æ³•é”™è¯¯,æ ˆä¸åŒ¹é…,NaNç­‰)\n",
    "            å…³é”®ç‚¹ï¼šä½¿ç”¨åç¼€è¡¨è¾¾å¼(RPN),å¤©ç„¶é€‚åˆæ ˆæœºæ‰§è¡Œ,æ— éœ€æ‹¬å·ã€‚\n",
    "        \"\"\"\n",
    "        stack = []\n",
    "        try:\n",
    "            for token in formula_tokens:\n",
    "                token = int(token)\n",
    "                if token < self.feat_offset:\n",
    "                    stack.append(feat_tensor[:, token, :])\n",
    "                elif token in self.op_map:\n",
    "                    arity = self.arity_map[token]\n",
    "                    if len(stack) < arity: return None\n",
    "                    args = []\n",
    "                    for _ in range(arity):\n",
    "                        args.append(stack.pop())\n",
    "                    args.reverse()\n",
    "                    func = self.op_map[token]\n",
    "                    res = func(*args)\n",
    "                    if torch.isnan(res).any() or torch.isinf(res).any():\n",
    "                        res = torch.nan_to_num(res, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "                    stack.append(res)\n",
    "                else:\n",
    "                    return None\n",
    "            if len(stack) == 1:\n",
    "                return stack[0]\n",
    "            else:\n",
    "                return None\n",
    "        except Exception:\n",
    "            return None\n",
    "print(\"success vm.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb945e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backtest.py\n",
    "class MemeBacktest:\n",
    "    \"\"\"\n",
    "        ç”¨äºå¯¹ meme å¸(é«˜æ³¢åŠ¨æ€§åŠ å¯†è´§å¸)äº¤æ˜“ç­–ç•¥è¿›è¡Œå›æµ‹è¯„ä¼°ã€‚\n",
    "        å®ƒæ¥æ”¶æ¨¡å‹ç”Ÿæˆçš„ä¿¡å·(factors)ã€åŸå§‹å¸‚åœºæ•°æ®(raw_data)å’Œç›®æ ‡æ”¶ç›Šç‡(target_ret),\n",
    "        æ¨¡æ‹ŸçœŸå®äº¤æ˜“ç¯å¢ƒ(åŒ…æ‹¬æµåŠ¨æ€§é™åˆ¶ã€æ»‘ç‚¹ã€æ‰‹ç»­è´¹ç­‰),\n",
    "        å¹¶è¾“å‡ºä¸€ä¸ªç»¼åˆé€‚åº”åº¦åˆ†æ•°(fitness score),å¸¸ç”¨äºè¿›åŒ–ç®—æ³•æˆ–å¼ºåŒ–å­¦ä¹ ä¸­çš„ç­–ç•¥è¯„ä¼°.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.trade_size = 1000.0\n",
    "        self.min_liq = 500000.0\n",
    "        self.base_fee = 0.0060\n",
    "\n",
    "    def evaluate(self, factors, raw_data, target_ret):\n",
    "        \"\"\"\n",
    "            å‚æ•°        ç±»å‹            shape           è§£é‡Š\n",
    "            factors:    torch.Tensor    [B,T]           æ¨¡å‹è¾“å‡ºçš„åŸå§‹ä¿¡å·\n",
    "            raw_data:   dict            --              åŒ…å«'liquidity'ç­‰åŸå§‹æ•°æ®\n",
    "            target_ret: torch.Tensor    [B,T]           ç›®æ ‡æŒæœ‰æ”¶ç›Šç‡\n",
    "        \"\"\"\n",
    "        # ç”Ÿæˆäº¤æ˜“ä¿¡æ¯\n",
    "        liquidity = raw_data['liquidity']   # æµåŠ¨æ€§æŒ‡æ ‡\n",
    "        signal = torch.sigmoid(factors)     # å°†ä¿¡å·å‹ç¼©åˆ°(0,1)\n",
    "        is_safe = (liquidity > self.min_liq).float()    # æµåŠ¨æ€§è¾¾æ ‡? 1= å®‰å…¨,0=å±é™©\n",
    "        position = (signal > 0.85).float() * is_safe    #ä»…å½“ä¿¡å·å¼ºçš„ä¼é¹…å®‰å…¨çš„æ—¶å€™æŒä»“\n",
    "        # è®¡ç®—äº¤æ˜“æˆæœ¬(æ»‘ç‚¹+æ‰‹ç»­è´¹)\n",
    "        impact_slippage = self.trade_size / (liquidity + 1e-9)\n",
    "        impact_slippage = torch.clamp(impact_slippage, 0.0, 0.05) #æœ€å¤§æ»‘ç‚¹ 5%\n",
    "        total_slippage_one_way = self.base_fee + impact_slippage\n",
    "        # è®¡ç®—æ¢æ‰‹ç‡å’Œäº¤æ˜“æˆæœ¬\n",
    "        prev_pos = torch.roll(position, 1, dims=1)\n",
    "        prev_pos[:, 0] = 0 # ç¬¬ä¸€æ—¶é—´æ­¥æ— å‰åºæŒä»“\n",
    "        turnover = torch.abs(position - prev_pos)   # 0->1 æˆ– 1->0 è¡¨ç¤ºäº¤æ˜“\n",
    "        tx_cost = turnover * total_slippage_one_way\n",
    "        # è®¡ç®—ç›ˆäº\n",
    "        gross_pnl = position * target_ret # æŒä»“æœŸé—´æ¯›æ”¶ç›Š\n",
    "        net_pnl = gross_pnl - tx_cost       # æ‰£é™¤äº¤æ˜“æˆæœ¬åçš„å‡€æ”¶ç›Š\n",
    "        # æ„å»ºç»¼åˆè¯„åˆ†(fitness score)\n",
    "        cum_ret = net_pnl.sum(dim=1)    # æ¯ä¸ªä»£å¸çš„ç´¯è®¡å‡€æ”¶ç›Š\n",
    "        big_drawdowns = (net_pnl < -0.05).float().sum(dim=1)    # å•æœŸäºæŸ>5% çš„æ¬¡æ•°\n",
    "        score = cum_ret - (big_drawdowns * 2.0) #æƒ©ç½šå¤§å›æ’¤\n",
    "        # è¿‡æ»¤ä½æ´»è·ƒåº¦ç­–ç•¥\n",
    "        activity = position.sum(dim=1) # æ€»äº¤æ˜“æ¬¡æ•°\n",
    "        score = torch.where(activity < 5, torch.tensor(-10.0, device=score.device), score)\n",
    "        # æœ€ç»ˆé€‚åº”åº¦\n",
    "        final_fitness = torch.median(score)\n",
    "        return final_fitness, cum_ret.mean().item()        \n",
    "print(\"success backtest.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3978cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine.py\n",
    "class AlphaEngine:\n",
    "    def __init__(self, use_lord_regularization=True, lord_decay_rate=1e-3, lord_num_iterations=5):\n",
    "        \"\"\"\n",
    "        Initialize AlphaGPT training engine.\n",
    "        \n",
    "        Args:\n",
    "            use_lord_regularization: Enable Low-Rank Decay (LoRD) regularization\n",
    "            lord_decay_rate: Strength of LoRD regularization\n",
    "            lord_num_iterations: Number of Newton-Schulz iterations per step\n",
    "        \"\"\"\n",
    "        self.loader = CryptoDataLoader()\n",
    "        self.loader.load_data()\n",
    "        \n",
    "        self.model = AlphaGPT().to(ModelConfig.DEVICE)\n",
    "        \n",
    "        # Standard optimizer\n",
    "        self.opt = torch.optim.AdamW(self.model.parameters(), lr=1e-3)\n",
    "        # ipdb.set_trace()\n",
    "        \n",
    "        # Low-Rank Decay regularizer\n",
    "        self.use_lord = use_lord_regularization\n",
    "        if self.use_lord:\n",
    "            self.lord_opt = NewtonSchulzLowRankDecay(\n",
    "                self.model.named_parameters(),\n",
    "                decay_rate=lord_decay_rate,\n",
    "                num_iterations=lord_num_iterations,\n",
    "                target_keywords=[\"q_proj\", \"k_proj\", \"attention\", \"qk_norm\"]\n",
    "            )\n",
    "            self.rank_monitor = StableRankMonitor(\n",
    "                self.model,\n",
    "                target_keywords=[\"q_proj\", \"k_proj\"]\n",
    "            )\n",
    "        else:\n",
    "            self.lord_opt = None\n",
    "            self.rank_monitor = None\n",
    "        \n",
    "        self.vm = StackVM()\n",
    "        self.bt = MemeBacktest()\n",
    "        \n",
    "        self.best_score = -float('inf')\n",
    "        self.best_formula = None\n",
    "        self.training_history = {\n",
    "            'step': [],\n",
    "            'avg_reward': [],\n",
    "            'best_score': [],\n",
    "            'stable_rank': []\n",
    "        }\n",
    "print(\"success class AlphaEngine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5403eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1000 [05:05<7:20:14, 26.68s/it, AvgRew=-4.982, BestScore=-10.000]"
     ]
    }
   ],
   "source": [
    "def train(self):\n",
    "    print(\"ğŸš€ Starting Meme Alpha Mining with LoRD Regularization...\" if self.use_lord else \"ğŸš€ Starting Meme Alpha Mining...\")\n",
    "    if self.use_lord:\n",
    "        print(f\"   LoRD Regularization enabled\")\n",
    "        print(f\"   Target keywords: ['q_proj', 'k_proj', 'attention', 'qk_norm']\")\n",
    "    \n",
    "    pbar = tqdm(range(ModelConfig.TRAIN_STEPS))\n",
    "    # ipdb.set_trace()\n",
    "    \n",
    "    for step in pbar:\n",
    "        # ipdb.set_trace()\n",
    "        bs = ModelConfig.BATCH_SIZE\n",
    "        inp = torch.zeros((bs, 1), dtype=torch.long, device=ModelConfig.DEVICE)\n",
    "        \n",
    "        log_probs = []\n",
    "        tokens_list = []\n",
    "        \n",
    "        for _ in range(ModelConfig.MAX_FORMULA_LEN):\n",
    "            logits, _, _ = self.model(inp)\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            \n",
    "            log_probs.append(dist.log_prob(action))\n",
    "            tokens_list.append(action)\n",
    "            inp = torch.cat([inp, action.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        seqs = torch.stack(tokens_list, dim=1)\n",
    "        \n",
    "        rewards = torch.zeros(bs, device=ModelConfig.DEVICE)\n",
    "        \n",
    "        for i in range(bs):\n",
    "            formula = seqs[i].tolist()\n",
    "            \n",
    "            res = self.vm.execute(formula, self.loader.feat_tensor)\n",
    "            \n",
    "            if res is None:\n",
    "                rewards[i] = -5.0\n",
    "                continue\n",
    "            \n",
    "            if res.std() < 1e-4:\n",
    "                rewards[i] = -2.0\n",
    "                continue\n",
    "            \n",
    "            score, ret_val = self.bt.evaluate(res, self.loader.raw_data_cache, self.loader.target_ret)\n",
    "            rewards[i] = score\n",
    "            \n",
    "            if score.item() > self.best_score:\n",
    "                self.best_score = score.item()\n",
    "                self.best_formula = formula\n",
    "                tqdm.write(f\"[!] New King: Score {score:.2f} | Ret {ret_val:.2%} | Formula {formula}\")\n",
    "        \n",
    "        # Normalize rewards\n",
    "        adv = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        \n",
    "        loss = 0\n",
    "        for t in range(len(log_probs)):\n",
    "            loss += -log_probs[t] * adv\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        # Gradient step\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        \n",
    "        # Apply Low-Rank Decay regularization\n",
    "        if self.use_lord:\n",
    "            self.lord_opt.step()\n",
    "        \n",
    "        # Logging\n",
    "        avg_reward = rewards.mean().item()\n",
    "        postfix_dict = {'AvgRew': f\"{avg_reward:.3f}\", 'BestScore': f\"{self.best_score:.3f}\"}\n",
    "        \n",
    "        if self.use_lord and step % 100 == 0:\n",
    "            stable_rank = self.rank_monitor.compute()\n",
    "            postfix_dict['Rank'] = f\"{stable_rank:.2f}\"\n",
    "            self.training_history['stable_rank'].append(stable_rank)\n",
    "        \n",
    "        self.training_history['step'].append(step)\n",
    "        self.training_history['avg_reward'].append(avg_reward)\n",
    "        self.training_history['best_score'].append(self.best_score)\n",
    "        \n",
    "        pbar.set_postfix(postfix_dict)\n",
    "\n",
    "    # Save best formula\n",
    "    with open(\"best_meme_strategy.json\", \"w\") as f:\n",
    "        json.dump(self.best_formula, f)\n",
    "    \n",
    "    # Save training history\n",
    "    import json as js\n",
    "    with open(\"training_history.json\", \"w\") as f:\n",
    "        js.dump(self.training_history, f)\n",
    "    \n",
    "    print(f\"\\nâœ“ Training completed!\")\n",
    "    print(f\"  Best score: {self.best_score:.4f}\")\n",
    "    print(f\"  Best formula: {self.best_formula}\")\n",
    "\n",
    "AlphaEngine.train=train\n",
    "if __name__ == \"__main__\":\n",
    "    eng = AlphaEngine(use_lord_regularization=True)\n",
    "    eng.train()\n",
    "print(\"success engin.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
