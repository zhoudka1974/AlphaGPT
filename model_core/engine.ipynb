{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e21fd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import success\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import ipdb\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import sqlalchemy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "print(\"import success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e8274a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success config.py\n"
     ]
    }
   ],
   "source": [
    "# config.py\n",
    "class ModelConfig:\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    DB_URL = f\"postgresql://{os.getenv('DB_USER','postgres')}:{os.getenv('DB_PASSWORD','password')}@{os.getenv('DB_HOST','localhost')}:5432/{os.getenv('DB_NAME','crypto_quant')}\"\n",
    "    BATCH_SIZE = 8192\n",
    "    TRAIN_STEPS = 1000\n",
    "    MAX_FORMULA_LEN = 12\n",
    "    TRADE_SIZE_USD = 1000.0\n",
    "    MIN_LIQUIDITY = 5000.0 # ä½äºæ­¤æµåŠ¨æ€§è§†ä¸ºå½’é›¶/æ— æ³•äº¤æ˜“\n",
    "    BASE_FEE = 0.005 # åŸºç¡€è´¹ç‡ 0.5% (Swap + Gas + Jito Tip)\n",
    "    INPUT_DIM = 6\n",
    "print(\"success config.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaf48b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success ops.py\n"
     ]
    }
   ],
   "source": [
    "#ops.py\n",
    "\"\"\"\n",
    "    ç”¨äºæ—¶é—´åºåˆ—ä¿¡å·å¤„ç†æˆ–è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹(å¦‚é‡åŒ–å› å­æŒ–æ˜ã€é—ä¼ ç¼–ç¨‹)çš„åŸå­æ“ä½œ(primitive operations),\n",
    "    å¹¶åˆ©ç”¨ PyTorch çš„ torch.jit.script ç¼–è¯‘ä¼˜åŒ–ä»¥æå‡è¿è¡Œæ•ˆç‡ã€‚\n",
    "    æ•´ä½“è®¾è®¡å¸¸è§äºè‡ªåŠ¨åŒ– alpha æŒ–æ˜ã€ç¨‹åºåˆæˆæˆ–ç¥ç»ç¬¦å·ç³»ç»Ÿä¸­ã€‚\n",
    "\"\"\"\n",
    "@torch.jit.script\n",
    "def _ts_delay(x: torch.Tensor, d: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "        åŠŸèƒ½:æ—¶é—´åºåˆ—å»¶è¿Ÿ(lag)æ“ä½œ\n",
    "        è¿”å› x å‘åç§»åŠ¨ d æœŸçš„ç»“æœ(å³ x[t-d] å¯¹åº”è¾“å‡º t æ—¶åˆ»)\n",
    "        è¾¹ç•Œå¤„ç†:å‰ d ä¸ªæ—¶é—´æ­¥ç”¨ 0 å¡«å……(å› æœ padding,æ— æœªæ¥ä¿¡æ¯æ³„éœ²)\n",
    "    \"\"\"\n",
    "    if d == 0: return x\n",
    "    pad = torch.zeros((x.shape[0], d), device=x.device)\n",
    "    result = torch.cat([pad, x[:, :-d]], dim=1)\n",
    "    # import ipdb ; ipdb.set_trace()\n",
    "    return result\n",
    "\n",
    "@torch.jit.script\n",
    "def _op_gate(condition: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "        åŠŸèƒ½:æ¡ä»¶é€‰æ‹©é—¨(ç±»ä¼¼ if-else)\n",
    "        - è‹¥ condition > 0,è¾“å‡º xï¼›å¦åˆ™è¾“å‡º y\n",
    "        - å‘é‡åŒ–å®ç°:é¿å… Python æ§åˆ¶æµ,æ”¯æŒ GPU å¹¶è¡Œ\n",
    "        æ•°å­¦ç­‰ä»·äº:\n",
    "        output={\n",
    "                 x if condition>0\n",
    "                 y otherwise\n",
    "                }\n",
    "        âœ… ç”¨äºæ„å»ºæ¡ä»¶é€»è¾‘(å¦‚â€œå¦‚æœæ³¢åŠ¨ç‡é«˜,åˆ™ç”¨ä¿å®ˆç­–ç•¥â€)\n",
    "    \"\"\"\n",
    "    mask = (condition > 0).float()\n",
    "    return mask * x + (1.0 - mask) * y\n",
    "\n",
    "@torch.jit.script\n",
    "def _op_jump(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "        åŠŸèƒ½:æ£€æµ‹â€œè·³è·ƒâ€æˆ–æç«¯å¼‚å¸¸å€¼\n",
    "        è®¡ç®—æ¯è¡Œ(æ¯ä¸ªä»£å¸)çš„ Z-score:   z=(x - Î¼)/Ïƒ\n",
    "        è‹¥   z>3 (å³åç¦»å‡å€¼ 3 ä¸ªæ ‡å‡†å·®ä»¥ä¸Š),è¾“å‡º z - 3 ï¼›å¦åˆ™è¾“å‡º 0\n",
    "        æ„ä¹‰:è¯†åˆ«ä»·æ ¼/æŒ‡æ ‡çš„çªå‘æ€§æš´æ¶¨æš´è·Œ(meme å¸å¸¸è§)\n",
    "        ğŸ“Œ è¾“å‡ºä¸ºéè´Ÿå€¼,è¶Šå¤§è¡¨ç¤ºâ€œè·³å¾—è¶ŠçŒ›â€\n",
    "    \"\"\"    \n",
    "    mean = x.mean(dim=1, keepdim=True)\n",
    "    std = x.std(dim=1, keepdim=True) + 1e-6\n",
    "    z = (x - mean) / std\n",
    "    return torch.relu(z - 3.0)\n",
    "\n",
    "@torch.jit.script\n",
    "def _op_decay(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    åŠŸèƒ½:æŒ‡æ•°è¡°å‡åŠ æƒ(è¿‘ä¼¼)\n",
    "        å½“å‰å€¼æƒé‡ 1.0,æ˜¨æ—¥ 0.8,å‰æ—¥ 0.6\n",
    "        è™½éä¸¥æ ¼æŒ‡æ•°è¡°å‡(å¦‚ EWMA),ä½†å®ç°äº†â€œè¿‘æœŸæ›´é‡è¦â€çš„æ€æƒ³\n",
    "        ç”¨é€”:å¹³æ»‘ä¿¡å·ã€èµ‹äºˆè¿‘æœŸæ•°æ®æ›´é«˜ä¼˜å…ˆçº§\n",
    "        ğŸ’¡ å¯è§†ä¸ºä¸€ä¸ªç®€å•çš„ FIR æ»¤æ³¢å™¨\n",
    "    \"\"\"\n",
    "    return x + 0.8 * _ts_delay(x, 1) + 0.6 * _ts_delay(x, 2)\n",
    "\n",
    "OPS_CONFIG = [\n",
    "    #    è¿™æ˜¯ä¸€ä¸ª æ“ä½œåŸè¯­åº“(primitive set),æ¯ä¸ªå…ƒç´ æ˜¯ä¸‰å…ƒç»„ (name, function, arity):\n",
    "    #    å­—æ®µ\tå«ä¹‰\n",
    "    #    name\tæ“ä½œåç§°(å­—ç¬¦ä¸²æ ‡è¯†)\n",
    "    #    function\tå¯è°ƒç”¨çš„å‡½æ•°(lambda æˆ– JIT å‡½æ•°)\n",
    "    #    arity\tæ‰€éœ€è¾“å…¥å‚æ•°ä¸ªæ•°(1=ä¸€å…ƒ,2=äºŒå…ƒ,3=ä¸‰å…ƒ)\n",
    "    ('ADD', lambda x, y: x + y, 2),\n",
    "    ('SUB', lambda x, y: x - y, 2),\n",
    "    ('MUL', lambda x, y: x * y, 2),\n",
    "    ('DIV', lambda x, y: x / (y + 1e-6), 2),\n",
    "    ('NEG', lambda x: -x, 1),\n",
    "    ('ABS', torch.abs, 1),\n",
    "    ('SIGN', torch.sign, 1),\n",
    "    ('GATE', _op_gate, 3),\n",
    "    ('JUMP', _op_jump, 1),\n",
    "    ('DECAY', _op_decay, 1),\n",
    "    ('DELAY1', lambda x: _ts_delay(x, 1), 1),\n",
    "    ('MAX3', lambda x: torch.max(x, torch.max(_ts_delay(x,1), _ts_delay(x,2))), 1)\n",
    "]\n",
    "print(\"success ops.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff07d837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success factors.py\n"
     ]
    }
   ],
   "source": [
    "# factors.py\n",
    "class RMSNormFactor(nn.Module):\n",
    "    \"\"\"RMSNorm for factor normalization\n",
    "        RMS å½’ä¸€åŒ–å±‚\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        return (x / rms) * self.weight\n",
    "\n",
    "\n",
    "class MemeIndicators:\n",
    "    @staticmethod\n",
    "    def liquidity_health(liquidity, fdv):\n",
    "        \"\"\"\n",
    "            è¡¡é‡æµåŠ¨æ€§å¥åº·åº¦ â€”â€” æµåŠ¨æ€§æ± æ·±åº¦ç›¸å¯¹äºå®Œå…¨ç¨€é‡Šå¸‚å€¼(FDV)çš„æ¯”ä¾‹ã€‚\n",
    "            é€»è¾‘:\n",
    "                FDV è¶Šé«˜,é¡¹ç›®â€œç†è®ºå¸‚å€¼â€è¶Šå¤§\n",
    "                è‹¥æµåŠ¨æ€§è¿œå°äº FDV(å¦‚ FDV= 1B,æµåŠ¨æ€§= 10k),åˆ™ææ˜“è¢«ç ¸ç›˜(æ»‘ç‚¹å·¨å¤§)\n",
    "            å½’ä¸€åŒ–:\n",
    "                ratio * 4.0:å‡è®¾ç†æƒ³ ratio â‰ˆ 0.25(å³æµåŠ¨æ€§å  FDV çš„ 25%),æ­¤æ—¶è¾“å‡ºä¸º 1.0\n",
    "                clamp(0, 1):é™åˆ¶åœ¨ [0,1] åŒºé—´,0=æå·®,1=æå¥½\n",
    "                âœ… è¾“å‡ºå¯ä½œä¸ºâ€œæŠ—ç ¸ç›˜èƒ½åŠ›â€è¯„åˆ†\n",
    "            \"\"\"\n",
    "        ratio = liquidity / (fdv + 1e-6)\n",
    "        return torch.clamp(ratio * 4.0, 0.0, 1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def buy_sell_imbalance(close, open_, high, low):\n",
    "        \"\"\"\n",
    "            è¡¡é‡å•æ ¹Kçº¿çš„ä¹°å–åŠ›é‡ä¸å¹³è¡¡ç¨‹åº¦(ç±»ä¼¼\"å®ä½“å æ¯”\")ã€‚\n",
    "            è®¡ç®—:\n",
    "                body = close - open_:é˜³çº¿ä¸ºæ­£,é˜´çº¿ä¸ºè´Ÿ\n",
    "                range_hl = high - low:æ•´æ ¹Kçº¿æ³¢åŠ¨èŒƒå›´\n",
    "                strength = body / range_hl:å®ä½“å æ€»æŒ¯å¹…çš„æ¯”ä¾‹(âˆˆ [-1, 1])\n",
    "            éçº¿æ€§å‹ç¼©:\n",
    "                tanh(strength * 3):æ”¾å¤§ä¸­ç­‰å¼ºåº¦ä¿¡å·,æŠ‘åˆ¶æç«¯å€¼(å¹³æ»‘)\n",
    "            âœ… æ­£å€¼è¡¨ç¤ºä¹°æ–¹å¼ºåŠ¿,è´Ÿå€¼è¡¨ç¤ºå–æ–¹å¼ºåŠ¿\n",
    "        \"\"\"\n",
    "        range_hl = high - low + 1e-9\n",
    "        body = close - open_\n",
    "        strength = body / range_hl\n",
    "        return torch.tanh(strength * 3.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def fomo_acceleration(volume, window=5):\n",
    "        \"\"\"\n",
    "            æ£€æµ‹FOMO(é”™å¤±ææƒ§)æƒ…ç»ªçš„åŠ é€Ÿåº¦ â€”â€” æˆäº¤é‡å¢é•¿æ˜¯å¦åœ¨åŠ é€Ÿã€‚\n",
    "            æ­¥éª¤:\n",
    "                1, vol_chg:æˆäº¤é‡ç¯æ¯”å˜åŒ–ç‡(æ˜¨æ—¥ä¸ºåˆ†æ¯,+1 é˜²é™¤é›¶)\n",
    "                2, acc = Î”(vol_chg):æˆäº¤é‡å˜åŒ–ç‡çš„äºŒé˜¶å¯¼æ•°(åŠ é€Ÿåº¦)\n",
    "            æ„ä¹‰:\n",
    "                1, acc > 0:FOMO åœ¨åŠ å‰§(è¶Šæ¥è¶Šå¤šäººå†²å…¥)\n",
    "                2, acc < 0:FOMO å‡é€€(çƒ­åº¦ä¸‹é™)\n",
    "            è£å‰ª:clamp(-5, 5) é˜²æ­¢å¼‚å¸¸å€¼å¹²æ‰°\n",
    "            âš ï¸ æ³¨æ„:torch.roll ä¼šå¯¼è‡´é¦–åˆ—ä½¿ç”¨æœ€åä¸€åˆ—æ•°æ®(å¾ªç¯ç§»ä½),å®é™…åº” paddingã€‚ä½†æ­¤å¤„å¯èƒ½æ¥å—è¾¹ç•Œè¯¯å·®ã€‚\n",
    "        \"\"\"\n",
    "        vol_prev = torch.roll(volume, 1, dims=1)\n",
    "        vol_chg = (volume - vol_prev) / (vol_prev + 1.0)\n",
    "        acc = vol_chg - torch.roll(vol_chg, 1, dims=1)\n",
    "        return torch.clamp(acc, -5.0, 5.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def pump_deviation(close, window=20):\n",
    "        \"\"\"\n",
    "            è¡¡é‡ä»·æ ¼ç›¸å¯¹äºç§»åŠ¨å¹³å‡çº¿çš„åç¦»ç¨‹åº¦(åˆ¤æ–­æ˜¯å¦â€œæš´æ¶¨â€æˆ–â€œè¶…ä¹°â€)ã€‚\n",
    "            å®ç°ç»†èŠ‚:\n",
    "                æ‰‹åŠ¨ padding(å‰è¡¥ 0)â†’ è®¡ç®—ä»ç¬¬ window å¤©å¼€å§‹çš„ MA\n",
    "                unfold(1, window, 1):æ»‘åŠ¨çª—å£æå–å­åºåˆ—\n",
    "                dev = (price - MA) / MA:ç›¸å¯¹åç¦»ç‡(ç±»ä¼¼å¸ƒæ—å¸¦ z-score)\n",
    "            ç”¨é€”:\n",
    "                dev >> 0:ä»·æ ¼è¿œé«˜äºå‡çº¿ â†’ å¯èƒ½è¿‡çƒ­\n",
    "                dev << 0:ä»·æ ¼è¿œä½äºå‡çº¿ â†’ å¯èƒ½è¶…å–\n",
    "            âœ… å…¸å‹â€œè¿½æ¶¨æ€è·Œâ€ä¿¡å·æº\n",
    "        \"\"\"\n",
    "        pad = torch.zeros((close.shape[0], window-1), device=close.device)\n",
    "        c_pad = torch.cat([pad, close], dim=1)\n",
    "        ma = c_pad.unfold(1, window, 1).mean(dim=-1)\n",
    "        dev = (close - ma) / (ma + 1e-9)\n",
    "        return dev\n",
    "\n",
    "    @staticmethod\n",
    "    def volatility_clustering(close, window=10):\n",
    "        \"\"\"Detect volatility clustering patterns\n",
    "            æ£€æµ‹æ³¢åŠ¨ç‡èšé›†æ•ˆåº”(é‡‘èç»å…¸ç°è±¡:é«˜æ³¢åŠ¨åå¾€å¾€æ¥é«˜æ³¢åŠ¨)ã€‚\n",
    "            æ­¥éª¤:\n",
    "                1, è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡ ret = log(P_t / P_{t-1})\n",
    "                2, å¹³æ–¹æ”¶ç›Šç‡ ret_sq â†’ ä»£è¡¨ç¬æ—¶æ³¢åŠ¨ç‡\n",
    "                3, å¯¹ ret_sq åšç§»åŠ¨å¹³å‡ â†’ å¾—åˆ°å±€éƒ¨æ³¢åŠ¨ç‡ä¼°è®¡\n",
    "                4, å¼€æ–¹ â†’ è¿‘ä¼¼æ ‡å‡†å·®(æ³¢åŠ¨ç‡)\n",
    "            è¾“å‡º:æ»šåŠ¨çª—å£å†…çš„å†å²æ³¢åŠ¨ç‡\n",
    "            âœ… ç”¨äºè¯†åˆ«â€œé«˜æ³¢åŠ¨æœŸâ€,å¯èƒ½ä¼´éš meme å¸å‰§çƒˆç‚’ä½œ\n",
    "        \"\"\"\n",
    "        ret = torch.log(close / (torch.roll(close, 1, dims=1) + 1e-9))\n",
    "        ret_sq = ret ** 2\n",
    "        \n",
    "        pad = torch.zeros((ret_sq.shape[0], window-1), device=close.device)\n",
    "        ret_sq_pad = torch.cat([pad, ret_sq], dim=1)\n",
    "        vol_ma = ret_sq_pad.unfold(1, window, 1).mean(dim=-1)\n",
    "        \n",
    "        return torch.sqrt(vol_ma + 1e-9)\n",
    "\n",
    "    @staticmethod\n",
    "    def momentum_reversal(close, window=5):\n",
    "        \"\"\"Capture momentum reversal signals\n",
    "            æ£€æµ‹åŠ¨é‡åè½¬ä¿¡å· â€”â€” ä¸Šæ¶¨/ä¸‹è·Œè¶‹åŠ¿æ˜¯å¦çªç„¶è½¬å‘ã€‚\n",
    "            é€»è¾‘:\n",
    "                1, mom:è¿‡å» window å¤©çš„ç´¯è®¡æ”¶ç›Šç‡(åŠ¨é‡æ–¹å‘)\n",
    "                2, mom_prev:å‰ä¸€å¤©çš„åŠ¨é‡\n",
    "                3, mom * mom_prev < 0:ç¬¦å·ç›¸å â†’ å‘ç”ŸåŠ¨é‡åè½¬\n",
    "            è¾“å‡º:äºŒå€¼ä¿¡å·(1=åè½¬,0=æ— åè½¬)\n",
    "            âœ… ç”¨äºæ•æ‰â€œè§é¡¶å›è½â€æˆ–â€œæ­¢è·Œåå¼¹â€çš„è½¬æŠ˜ç‚¹\n",
    "        \"\"\"\n",
    "        ret = torch.log(close / (torch.roll(close, 1, dims=1) + 1e-9))\n",
    "        \n",
    "        pad = torch.zeros((ret.shape[0], window-1), device=close.device)\n",
    "        ret_pad = torch.cat([pad, ret], dim=1)\n",
    "        mom = ret_pad.unfold(1, window, 1).sum(dim=-1)\n",
    "        \n",
    "        # Detect reversals\n",
    "        mom_prev = torch.roll(mom, 1, dims=1)\n",
    "        reversal = (mom * mom_prev < 0).float()\n",
    "        \n",
    "        return reversal\n",
    "\n",
    "    @staticmethod\n",
    "    def relative_strength(close, high, low, window=14):\n",
    "        \"\"\"RSI-like indicator for strength detection\n",
    "            å®ç° RSI(ç›¸å¯¹å¼ºå¼±æŒ‡æ•°) çš„å˜ä½“,ç”¨äºåˆ¤æ–­è¶…ä¹°/è¶…å–ã€‚\n",
    "            æ ‡å‡† RSI æ­¥éª¤:\n",
    "                1, è®¡ç®—æ¯æ—¥æ¶¨è·Œ(gains, losses)\n",
    "                2, è®¡ç®— window æ—¥å¹³å‡æ¶¨å¹…/è·Œå¹…\n",
    "                3, RS = avg_gain / avg_loss\n",
    "                4, RSI = 100 - 100/(1+RS)\n",
    "            å½’ä¸€åŒ–:\n",
    "                (RSI - 50) / 50 â†’ å°† [0,100] æ˜ å°„åˆ° [-1, 1]\n",
    "                    -1:æç«¯è¶…å–\n",
    "                    0:ä¸­æ€§\n",
    "                    +1:æç«¯è¶…ä¹°\n",
    "            âœ… ç»å…¸éœ‡è¡æŒ‡æ ‡,é€‚ç”¨äº meme å¸çš„é«˜æ³¢åŠ¨ç¯å¢ƒ        \n",
    "        \"\"\"\n",
    "        ret = close - torch.roll(close, 1, dims=1)\n",
    "        \n",
    "        gains = torch.relu(ret)\n",
    "        losses = torch.relu(-ret)\n",
    "        \n",
    "        pad = torch.zeros((gains.shape[0], window-1), device=close.device)\n",
    "        gains_pad = torch.cat([pad, gains], dim=1)\n",
    "        losses_pad = torch.cat([pad, losses], dim=1)\n",
    "        \n",
    "        avg_gain = gains_pad.unfold(1, window, 1).mean(dim=-1)\n",
    "        avg_loss = losses_pad.unfold(1, window, 1).mean(dim=-1)\n",
    "        \n",
    "        rs = (avg_gain + 1e-9) / (avg_loss + 1e-9)\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        return (rsi - 50) / 50  # Normalize\n",
    "\n",
    "\n",
    "class AdvancedFactorEngineer:\n",
    "    \"\"\"Advanced feature engineering with multiple factor types\n",
    "        ä»åŸå§‹å¸‚åœºæ•°æ®ä¸­æ„å»ºä¸€ä¸ª12ç»´çš„é«˜çº§ç‰¹å¾ç©ºé—´(feature space),ç‰¹åˆ«é’ˆå¯¹é«˜æ³¢åŠ¨æ€§èµ„äº§(å¦‚ meme å¸)è®¾è®¡ã€‚\n",
    "        å®ƒç»“åˆäº†åŸºç¡€æŠ€æœ¯æŒ‡æ ‡ã€è¡Œä¸ºé‡‘èä¿¡å·å’Œç¨³å¥å½’ä¸€åŒ–æ–¹æ³•,\n",
    "        è¾“å‡ºå¯ç›´æ¥ç”¨äºæœºå™¨å­¦ä¹ æ¨¡å‹(å¦‚é¢„æµ‹ã€åˆ†ç±»æˆ–å¼ºåŒ–å­¦ä¹ )çš„æ ‡å‡†åŒ–ç‰¹å¾å¼ é‡.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.rms_norm = RMSNormFactor(1)\n",
    "    \n",
    "    def robust_norm(self, t):\n",
    "        \"\"\"Robust normalization using median absolute deviation\n",
    "            ç¨³å¥å½’ä¸€åŒ–(åŸºäºä¸­ä½æ•°å’Œ MAD)\n",
    "            å¯¹è¾“å…¥å¼ é‡ t(shape: [num_tokens, time_steps])è¿›è¡Œç¨³å¥æ ‡å‡†åŒ–,é¿å…å—æç«¯å€¼(outliers)å½±å“ã€‚\n",
    "            æ­¥éª¤:\n",
    "                1,è®¡ç®—æ¯è¡Œ(æ¯ä¸ªä»£å¸)çš„ä¸­ä½æ•° â†’ æ¯”å‡å€¼æ›´æŠ—å¼‚å¸¸å€¼ã€‚\n",
    "                2, è®¡ç®— MAD(Median Absolute Deviation):\n",
    "                    MAD = median(|Xi - median(X)|)\n",
    "                3,æ ‡å‡†åŒ–:(x - median) / MAD\n",
    "            è£å‰ª:é™åˆ¶åœ¨ [-5, 5],é˜²æ­¢æ®‹ä½™å¼‚å¸¸å€¼å¹²æ‰°æ¨¡å‹ã€‚\n",
    "            âœ… ä¸ºä»€ä¹ˆç”¨ MAD?\n",
    "            Meme å¸ä»·æ ¼å¸¸å‡ºç°æš´æ¶¨æš´è·Œ(å¦‚ 100x),å‡å€¼å’Œæ ‡å‡†å·®ä¼šè¢«ä¸¥é‡æ‰­æ›²ã€‚MAD å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ,æ›´é€‚åˆæ­¤ç±»æ•°æ®ã€‚            \n",
    "        \"\"\"\n",
    "        median = torch.nanmedian(t, dim=1, keepdim=True)[0]\n",
    "        mad = torch.nanmedian(torch.abs(t - median), dim=1, keepdim=True)[0] + 1e-6\n",
    "        norm = (t - median) / mad\n",
    "        return torch.clamp(norm, -5.0, 5.0)\n",
    "    \n",
    "    def compute_advanced_features(self, raw_dict):\n",
    "        \"\"\"Compute 12-dimensional feature space with advanced factors æ„å»º12ç»´ç‰¹å¾\n",
    "        \"\"\"\n",
    "        c = raw_dict['close']\n",
    "        o = raw_dict['open']\n",
    "        h = raw_dict['high']\n",
    "        l = raw_dict['low']\n",
    "        v = raw_dict['volume']\n",
    "        liq = raw_dict['liquidity']\n",
    "        fdv = raw_dict['fdv']\n",
    "        \n",
    "        # Basic factors è®¡ç®—åŸºç¡€å› å­(Basic Factors)\n",
    "        ret = torch.log(c / (torch.roll(c, 1, dims=1) + 1e-9)) # å¯¹æ•°æ”¶ç›Šç‡\n",
    "        liq_score = MemeIndicators.liquidity_health(liq, fdv)   # æµåŠ¨æ€§å¥åº·åº¦\n",
    "        pressure = MemeIndicators.buy_sell_imbalance(c, o, h, l) # ä¹°å–å‹åŠ›\n",
    "        fomo = MemeIndicators.fomo_acceleration(v) # fomo åŠ é€Ÿåº¦\n",
    "        dev = MemeIndicators.pump_deviation(c)  # ä»·æ ¼åç¦»å‡çº¿ç¨‹åº¦\n",
    "        log_vol = torch.log1p(v) # torch.log1p(v) = log(1 + v),é¿å… v=0 æ—¶ log(0) é—®é¢˜,åŒæ—¶å‹ç¼©å¤§æˆäº¤é‡ã€‚\n",
    "        \n",
    "        # Advanced factors\n",
    "        vol_cluster = MemeIndicators.volatility_clustering(c) # æ³¢åŠ¨ç‡èšé›†(å±€éƒ¨æ³¢åŠ¨ç‡)\n",
    "        momentum_rev = MemeIndicators.momentum_reversal(c) # åŠ¨é‡åè½¬(0 æˆ– 1)\n",
    "        rel_strength = MemeIndicators.relative_strength(c, h, l) # ç›¸å¯¹å¼ºå¼±(RSI å˜ä½“,-1ï½1)\n",
    "        \n",
    "        # High-low range æ—¥å†…æŒ¯å¹…å æ”¶ç›˜ä»·æ¯”ä¾‹\n",
    "        hl_range = (h - l) / (c + 1e-9)\n",
    "        \n",
    "        # Close position in range æ”¶ç›˜ä»·åœ¨æ—¥å†…åŒºé—´çš„ä½ç½®(0ï½1)\n",
    "        close_pos = (c - l) / (h - l + 1e-9)\n",
    "            # close_pos æ˜¯ç»å…¸â€œKçº¿ä½ç½®â€æŒ‡æ ‡ï¼šæ¥è¿‘ 1 è¡¨ç¤ºå¼ºåŠ¿æ”¶æ¶¨,æ¥è¿‘ 0 è¡¨ç¤ºå¼±åŠ¿æ”¶è·Œã€‚\n",
    "        \n",
    "        # Volume trend æˆäº¤é‡å˜åŒ–ç‡\n",
    "        vol_prev = torch.roll(v, 1, dims=1)\n",
    "        vol_trend = (v - vol_prev) / (vol_prev + 1.0)\n",
    "        \n",
    "        # å †å æ‰€æœ‰ç‰¹å¾ \n",
    "        features = torch.stack([\n",
    "            self.robust_norm(ret),\n",
    "            liq_score,\n",
    "            pressure,\n",
    "            self.robust_norm(fomo),\n",
    "            self.robust_norm(dev),\n",
    "            self.robust_norm(log_vol),\n",
    "            self.robust_norm(vol_cluster),\n",
    "            momentum_rev,\n",
    "            self.robust_norm(rel_strength),\n",
    "            self.robust_norm(hl_range),\n",
    "            close_pos,\n",
    "            self.robust_norm(vol_trend)\n",
    "        ], dim=1)\n",
    "        # torch.stack(..., dim=1) å°† 12 ä¸ª [B, T] å¼ é‡å †å æˆ [B, 12, T],\n",
    "        # è¿™æ˜¯æ—¶é—´åºåˆ—æ¨¡å‹(å¦‚ Transformerã€CNN)çš„æ ‡å‡†è¾“å…¥æ ¼å¼ã€‚\n",
    "        \n",
    "        return features\n",
    "\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "        ä»åŸå§‹å¸‚åœºæ•°æ®(å¦‚ K çº¿ã€æµåŠ¨æ€§ç­‰)ä¸­é«˜æ•ˆæå–ä¸€ç»„æ ‡å‡†åŒ–çš„ 6 ç»´æŠ€æœ¯ç‰¹å¾(factors),\n",
    "        ä¸“ä¸ºé«˜æ³¢åŠ¨æ€§åŠ å¯†èµ„äº§(å¦‚ meme å¸)è®¾è®¡ã€‚\n",
    "        å…¶æ ¸å¿ƒç›®æ ‡æ˜¯å°†åŸå§‹è¡Œæƒ…æ•°æ®è½¬æ¢ä¸ºæ•°å€¼ç¨³å®šã€é‡çº²ç»Ÿä¸€ã€æŠ—å¼‚å¸¸å€¼çš„ç‰¹å¾å¼ é‡,å¯ç›´æ¥è¾“å…¥æœºå™¨å­¦ä¹ æ¨¡å‹(å¦‚é¢„æµ‹ç½‘ç»œã€å¼ºåŒ–å­¦ä¹ ç­–ç•¥ç­‰)ã€‚\n",
    "    \"\"\"\n",
    "    INPUT_DIM = 6\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_features(raw_dict):\n",
    "        c = raw_dict['close']\n",
    "        o = raw_dict['open']\n",
    "        h = raw_dict['high']\n",
    "        l = raw_dict['low']\n",
    "        v = raw_dict['volume']\n",
    "        liq = raw_dict['liquidity']\n",
    "        fdv = raw_dict['fdv']\n",
    "        \n",
    "        ret = torch.log(c / (torch.roll(c, 1, dims=1) + 1e-9)) #  å¯¹æ•°æ”¶ç›Šç‡(Log Return)\n",
    "        liq_score = MemeIndicators.liquidity_health(liq, fdv) # æµåŠ¨æ€§å¥åº·åº¦\n",
    "        pressure = MemeIndicators.buy_sell_imbalance(c, o, h, l) # ä¹°å–å‹åŠ›(Kçº¿å®ä½“å¼ºåº¦)\n",
    "        fomo = MemeIndicators.fomo_acceleration(v) # FOMO æƒ…ç»ªåŠ é€Ÿåº¦\n",
    "        dev = MemeIndicators.pump_deviation(c) # ä»·æ ¼åç¦»åº¦(æ˜¯å¦æš´æ¶¨ï¼Ÿ)\n",
    "        log_vol = torch.log1p(v) # å¯¹æ•°æˆäº¤é‡\n",
    "        \n",
    "        def robust_norm(t):\n",
    "            median = torch.nanmedian(t, dim=1, keepdim=True)[0]\n",
    "            mad = torch.nanmedian(torch.abs(t - median), dim=1, keepdim=True)[0] + 1e-6\n",
    "            norm = (t - median) / mad\n",
    "            return torch.clamp(norm, -5.0, 5.0)\n",
    "\n",
    "        features = torch.stack([\n",
    "            robust_norm(ret),\n",
    "            liq_score,\n",
    "            pressure,\n",
    "            robust_norm(fomo),\n",
    "            robust_norm(dev),\n",
    "            robust_norm(log_vol)\n",
    "        ], dim=1)\n",
    "        \n",
    "        return features\n",
    "print(\"success factors.py\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25cf9819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success data_loader.py\n"
     ]
    }
   ],
   "source": [
    "# data_loader.py\n",
    "class CryptoDataLoader:\n",
    "    def __init__(self):\n",
    "        self.engine = sqlalchemy.create_engine(ModelConfig.DB_URL)\n",
    "        self.feat_tensor = None\n",
    "        self.raw_data_cache = None\n",
    "        self.target_ret = None\n",
    "        \n",
    "    def load_data(self, limit_tokens=500):\n",
    "        print(\"Loading data from SQL...\")\n",
    "        top_query = f\"\"\"\n",
    "        SELECT address FROM tokens \n",
    "        LIMIT {limit_tokens} \n",
    "        \"\"\"\n",
    "        addrs = pd.read_sql(top_query, self.engine)['address'].tolist()\n",
    "        if not addrs: raise ValueError(\"No tokens found.\")\n",
    "        addr_str = \"'\" + \"','\".join(addrs) + \"'\"\n",
    "        data_query = f\"\"\"\n",
    "        SELECT time, address, open, high, low, close, volume, liquidity, fdv\n",
    "        FROM ohlcv\n",
    "        WHERE address IN ({addr_str})\n",
    "        ORDER BY time ASC\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(data_query, self.engine)\n",
    "        def to_tensor(col):\n",
    "            pivot = df.pivot(index='time', columns='address', values=col)\n",
    "            pivot = pivot.fillna(0.0)\n",
    "            return torch.tensor(pivot.values.T, dtype=torch.float32, device=ModelConfig.DEVICE)\n",
    "        self.raw_data_cache = {\n",
    "            'open': to_tensor('open'),\n",
    "            'high': to_tensor('high'),\n",
    "            'low': to_tensor('low'),\n",
    "            'close': to_tensor('close'),\n",
    "            'volume': to_tensor('volume'),\n",
    "            'liquidity': to_tensor('liquidity'),\n",
    "            'fdv': to_tensor('fdv')\n",
    "        }\n",
    "        self.feat_tensor = FeatureEngineer.compute_features(self.raw_data_cache)\n",
    "        op = self.raw_data_cache['open']\n",
    "        t1 = torch.roll(op, -1, dims=1)\n",
    "        t2 = torch.roll(op, -2, dims=1)\n",
    "        self.target_ret = torch.log(t2 / (t1 + 1e-9))\n",
    "        self.target_ret[:, -2:] = 0.0\n",
    "        print(f\"Data Ready. Shape: {self.feat_tensor.shape}\")\n",
    "\n",
    "print(\"success data_loader.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08069adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success alphagpt.py\n"
     ]
    }
   ],
   "source": [
    "# alphagpt.py\n",
    "class NewtonSchulzLowRankDecay:\n",
    "    \"\"\"\n",
    "    Low-Rank Decay (LoRD) using Newton-Schulz iteration.\n",
    "    \n",
    "    A more efficient regularization method that targets low-rank structure\n",
    "    in attention and key parameters. Uses Newton-Schulz iteration to compute\n",
    "    the minimum singular vectors without explicit SVD.\n",
    "    \n",
    "    Args:\n",
    "        named_parameters: Model's named parameters\n",
    "        decay_rate: Strength of low-rank decay\n",
    "        num_iterations: Number of Newton-Schulz iterations (default: 5)\n",
    "        target_keywords: If specified, only decay parameters matching these keywords\n",
    "\n",
    "    åä¸º AlphaGPT çš„ç¥ç»ç½‘ç»œæ¨¡å‹,ä¸“ä¸ºè‡ªåŠ¨ç”Ÿæˆäº¤æ˜“ç­–ç•¥å…¬å¼(å¦‚é‡åŒ–å› å­) è€Œè®¾è®¡ã€‚\n",
    "        å®ƒç»“åˆäº†å¤šç§å…ˆè¿›æ¶æ„æŠ€æœ¯(å¦‚ Looped Transformerã€QK-Normã€SwiGLUã€MTP Head ç­‰),\n",
    "        å¹¶å¼•å…¥äº†ä½ç§©æ­£åˆ™åŒ–(LoRD) å’Œç¨³å®šç§©ç›‘æ§æœºåˆ¶,\n",
    "        ä»¥æå‡æ¨¡å‹åœ¨å°æ ·æœ¬ã€é«˜å™ªå£°é‡‘èæ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›\n",
    "    \"\"\"\n",
    "    def __init__(self, named_parameters, decay_rate=1e-3, num_iterations=5, target_keywords=None):\n",
    "        self.decay_rate = decay_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.target_keywords = target_keywords or [\"qk_norm\", \"attention\"]\n",
    "        self.params_to_decay = []\n",
    "        \n",
    "        for name, param in named_parameters:\n",
    "            if not param.requires_grad or param.ndim != 2:\n",
    "                continue\n",
    "            if not any(k in name for k in self.target_keywords):\n",
    "                continue\n",
    "            self.params_to_decay.append((name, param))\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Apply Newton-Schulz low-rank decay to attention parameters.\"\"\"\n",
    "        for name, W in self.params_to_decay:\n",
    "            orig_dtype = W.dtype\n",
    "            X = W.float()\n",
    "            r, c = X.shape\n",
    "            \n",
    "            # Transpose if needed for efficiency\n",
    "            transposed = False\n",
    "            if r > c:\n",
    "                X = X.T\n",
    "                transposed = True\n",
    "            \n",
    "            # Normalize by spectral norm\n",
    "            norm = X.norm() + 1e-8\n",
    "            X = X / norm\n",
    "            \n",
    "            # Initialize Y for Newton-Schulz iteration\n",
    "            Y = X\n",
    "            I = torch.eye(X.shape[-1], device=X.device, dtype=X.dtype)\n",
    "            \n",
    "            # Newton-Schulz iteration: Y_{k+1} = 0.5 * Y_k * (3*I - Y_k^T * Y_k)\n",
    "            # This converges to the orthogonal matrix with same singular vectors\n",
    "            for _ in range(self.num_iterations):\n",
    "                A = Y.T @ Y\n",
    "                Y = 0.5 * Y @ (3.0 * I - A)\n",
    "            \n",
    "            if transposed:\n",
    "                Y = Y.T\n",
    "            \n",
    "            # Apply low-rank decay\n",
    "            W.sub_(self.decay_rate * Y.to(orig_dtype))\n",
    "\n",
    "\n",
    "class StableRankMonitor:\n",
    "    \"\"\"Monitor the effective rank (stable rank) of model parameters.\"\"\"\n",
    "    def __init__(self, model, target_keywords=None):\n",
    "        self.model = model\n",
    "        self.target_keywords = target_keywords or [\"q_proj\", \"k_proj\", \"attention\"]\n",
    "        self.history = []\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def compute(self):\n",
    "        \"\"\"Compute average stable rank of target parameters.\"\"\"\n",
    "        ranks = []\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.ndim != 2:\n",
    "                continue\n",
    "            if not any(k in name for k in self.target_keywords):\n",
    "                continue\n",
    "            \n",
    "            W = param.detach().float()\n",
    "            S = torch.linalg.svdvals(W)\n",
    "            # Stable Rank = ||W||_F^2 / ||W||_2^2\n",
    "            stable_rank = (S.norm() ** 2) / (S[0] ** 2 + 1e-9)\n",
    "            ranks.append(stable_rank.item())\n",
    "        \n",
    "        avg_rank = sum(ranks) / len(ranks) if ranks else 0.0\n",
    "        self.history.append(avg_rank)\n",
    "        return avg_rank\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization\"\"\"\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        return (x / rms) * self.weight\n",
    "\n",
    "\n",
    "class QKNorm(nn.Module):\n",
    "    \"\"\"Query-Key Normalization for Attention\"\"\"\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(1, 1, 1, d_model) * (d_model ** -0.5))\n",
    "    \n",
    "    def forward(self, q, k):\n",
    "        # Normalize Q and K independently\n",
    "        q_norm = F.normalize(q, p=2, dim=-1)\n",
    "        k_norm = F.normalize(k, p=2, dim=-1)\n",
    "        return q_norm * self.scale, k_norm * self.scale\n",
    "\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"Swish GLU activation function\"\"\"\n",
    "    def __init__(self, d_in, d_ff):\n",
    "        super().__init__()\n",
    "        self.w = nn.Linear(d_in, d_ff * 2)\n",
    "        self.fc = nn.Linear(d_ff, d_in)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_glu = self.w(x)\n",
    "        x, gate = x_glu.chunk(2, dim=-1)\n",
    "        x = x * F.silu(gate)  # Swish activation\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class MTPHead(nn.Module):\n",
    "    \"\"\"Multi-Task Pooling Head for multi-objective learning\"\"\"\n",
    "    def __init__(self, d_model, vocab_size, num_tasks=3):\n",
    "        super().__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "        self.task_heads = nn.ModuleList([\n",
    "            nn.Linear(d_model, vocab_size) for _ in range(num_tasks)\n",
    "        ])\n",
    "        self.task_weights = nn.Parameter(torch.ones(num_tasks) / num_tasks)\n",
    "        self.task_router = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model // 2, num_tasks)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Route to appropriate task heads\n",
    "        task_logits = self.task_router(x)\n",
    "        task_probs = F.softmax(task_logits, dim=-1)\n",
    "        \n",
    "        # Compute all task outputs\n",
    "        task_outputs = [head(x) for head in self.task_heads]\n",
    "        task_outputs = torch.stack(task_outputs, dim=1)  # [B, num_tasks, vocab_size]\n",
    "        \n",
    "        # Weighted combination\n",
    "        weighted = (task_probs.unsqueeze(-1) * task_outputs).sum(dim=1)\n",
    "        return weighted, task_probs\n",
    "\n",
    "\n",
    "class LoopedTransformerLayer(nn.Module):\n",
    "    \"\"\"Looped Transformer Layer - recurrent processing within a layer\"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, num_loops=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_loops = num_loops\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        \n",
    "        # QK-Norm attention\n",
    "        self.qk_norm = QKNorm(d_model // nhead)\n",
    "        \n",
    "        # Standard attention components\n",
    "        self.attention = nn.MultiheadAttention(d_model, nhead, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # RMSNorm instead of LayerNorm\n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "        \n",
    "        # SwiGLU FFN instead of standard FFN\n",
    "        self.ffn = SwiGLU(d_model, dim_feedforward)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None, is_causal=False):\n",
    "        # Looped processing - recurrent refinement\n",
    "        for _ in range(self.num_loops):\n",
    "            # Self-attention with residual\n",
    "            x_norm = self.norm1(x)\n",
    "            attn_out, _ = self.attention(x_norm, x_norm, x_norm, attn_mask=mask, is_causal=is_causal)\n",
    "            x = x + self.dropout(attn_out)\n",
    "            \n",
    "            # FFN with residual\n",
    "            x_norm = self.norm2(x)\n",
    "            ffn_out = self.ffn(x_norm)\n",
    "            x = x + self.dropout(ffn_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class LoopedTransformer(nn.Module):\n",
    "    \"\"\"Looped Transformer Encoder with multiple loop iterations\"\"\"\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, num_loops=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            LoopedTransformerLayer(d_model, nhead, dim_feedforward, num_loops, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, mask=None, is_causal=False):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask, is_causal=is_causal)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AlphaGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d_model = 64\n",
    "        self.features_list = ['RET', 'VOL', 'V_CHG', 'PV', 'TREND']\n",
    "        self.ops_list = [cfg[0] for cfg in OPS_CONFIG]\n",
    "        \n",
    "        self.vocab = self.features_list + self.ops_list\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        # Embedding\n",
    "        self.token_emb = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, ModelConfig.MAX_FORMULA_LEN + 1, self.d_model))\n",
    "        \n",
    "        # Enhanced Transformer with Looped Transformer\n",
    "        self.blocks = LoopedTransformer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=4,\n",
    "            num_layers=2,\n",
    "            dim_feedforward=128,\n",
    "            num_loops=3,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # RMSNorm instead of LayerNorm\n",
    "        self.ln_f = RMSNorm(self.d_model)\n",
    "        \n",
    "        # MTPHead for multi-task output\n",
    "        self.mtp_head = MTPHead(self.d_model, self.vocab_size, num_tasks=3)\n",
    "        self.head_critic = nn.Linear(self.d_model, 1)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # idx: [Batch, SeqLen]\n",
    "        B, T = idx.size()\n",
    "        \n",
    "        x = self.token_emb(idx) + self.pos_emb[:, :T, :]\n",
    "        \n",
    "        # Causal Mask\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(T).to(idx.device)\n",
    "        \n",
    "        # Process through looped transformer\n",
    "        x = self.blocks(x, mask=mask, is_causal=True)\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        last_emb = x[:, -1, :]\n",
    "        \n",
    "        # Multi-task pooling head for logits\n",
    "        logits, task_probs = self.mtp_head(last_emb)\n",
    "        value = self.head_critic(last_emb)\n",
    "        \n",
    "        return logits, value, task_probs\n",
    "print(\"success alphagpt.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52b11c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success vm.py\n"
     ]
    }
   ],
   "source": [
    "# vm.py\n",
    "\n",
    "class StackVM:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        feat_offset = 6\n",
    "            åˆå§‹åŒ–æ˜ å°„è¡¨\n",
    "            token 0~5 : è¾“å…¥ç‰¹å¾\n",
    "            token 6+ æ“ä½œç¬¦\n",
    "        \"\"\"\n",
    "        self.feat_offset = FeatureEngineer.INPUT_DIM\n",
    "        self.op_map = {i + self.feat_offset: cfg[1] for i, cfg in enumerate(OPS_CONFIG)}\n",
    "        # å°†æ“ä½œæ•°tokenæ˜ å°„åˆ°å‡½æ•°\n",
    "        self.arity_map = {i + self.feat_offset: cfg[2] for i, cfg in enumerate(OPS_CONFIG)}\n",
    "        # è®°å½•æ¯ä¸ªæ“ä½œç¬¦éœ€è¦å‡ ä¸ªå‚æ•°\n",
    "\n",
    "    def execute(self, formula_tokens, feat_tensor): # æ‰§è¡Œå¼•æ“\n",
    "        \"\"\"            \n",
    "            è¾“å…¥:\n",
    "                formula_tokens: List[int] , è¡¨ç¤ºå…¬å¼çš„token åºåˆ—(åç¼€è¡¨è¾¾å¼/é€†æ³¢å…°è¡¨è¾¾å¼)\n",
    "                feat_tensor:  Tensor, shape[B,6,T], ç”±Feature Engineer.compute_feature() ç”Ÿæˆ.\n",
    "            è¾“å‡º:\n",
    "                æˆåŠŸ: Tensor , shape[B,T] (æ¯ä¸€ä¸ªä»£å¸çš„æ—¶é—´åºåˆ—ä¿¡å·)\n",
    "                å¤±è´¥: None(è¯­æ³•é”™è¯¯,æ ˆä¸åŒ¹é…,NaNç­‰)\n",
    "            å…³é”®ç‚¹ï¼šä½¿ç”¨åç¼€è¡¨è¾¾å¼(RPN),å¤©ç„¶é€‚åˆæ ˆæœºæ‰§è¡Œ,æ— éœ€æ‹¬å·ã€‚\n",
    "        \"\"\"\n",
    "        stack = []\n",
    "        try:\n",
    "            for token in formula_tokens:\n",
    "                token = int(token)\n",
    "                if token < self.feat_offset:\n",
    "                    stack.append(feat_tensor[:, token, :])\n",
    "                elif token in self.op_map:\n",
    "                    arity = self.arity_map[token]\n",
    "                    if len(stack) < arity: return None\n",
    "                    args = []\n",
    "                    for _ in range(arity):\n",
    "                        args.append(stack.pop())\n",
    "                    args.reverse()\n",
    "                    func = self.op_map[token]\n",
    "                    res = func(*args)\n",
    "                    if torch.isnan(res).any() or torch.isinf(res).any():\n",
    "                        res = torch.nan_to_num(res, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "                    stack.append(res)\n",
    "                else:\n",
    "                    return None\n",
    "            if len(stack) == 1:\n",
    "                return stack[0]\n",
    "            else:\n",
    "                return None\n",
    "        except Exception:\n",
    "            return None\n",
    "print(\"success vm.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fb945e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success backtest.py\n"
     ]
    }
   ],
   "source": [
    "# backtest.py\n",
    "class MemeBacktest:\n",
    "    \"\"\"\n",
    "        ç”¨äºå¯¹ meme å¸(é«˜æ³¢åŠ¨æ€§åŠ å¯†è´§å¸)äº¤æ˜“ç­–ç•¥è¿›è¡Œå›æµ‹è¯„ä¼°ã€‚\n",
    "        å®ƒæ¥æ”¶æ¨¡å‹ç”Ÿæˆçš„ä¿¡å·(factors)ã€åŸå§‹å¸‚åœºæ•°æ®(raw_data)å’Œç›®æ ‡æ”¶ç›Šç‡(target_ret),\n",
    "        æ¨¡æ‹ŸçœŸå®äº¤æ˜“ç¯å¢ƒ(åŒ…æ‹¬æµåŠ¨æ€§é™åˆ¶ã€æ»‘ç‚¹ã€æ‰‹ç»­è´¹ç­‰),\n",
    "        å¹¶è¾“å‡ºä¸€ä¸ªç»¼åˆé€‚åº”åº¦åˆ†æ•°(fitness score),å¸¸ç”¨äºè¿›åŒ–ç®—æ³•æˆ–å¼ºåŒ–å­¦ä¹ ä¸­çš„ç­–ç•¥è¯„ä¼°.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.trade_size = 1000.0\n",
    "        self.min_liq = 500000.0\n",
    "        self.base_fee = 0.0060\n",
    "\n",
    "    def evaluate(self, factors, raw_data, target_ret):\n",
    "        \"\"\"\n",
    "            å‚æ•°        ç±»å‹            shape           è§£é‡Š\n",
    "            factors:    torch.Tensor    [B,T]           æ¨¡å‹è¾“å‡ºçš„åŸå§‹ä¿¡å·\n",
    "            raw_data:   dict            --              åŒ…å«'liquidity'ç­‰åŸå§‹æ•°æ®\n",
    "            target_ret: torch.Tensor    [B,T]           ç›®æ ‡æŒæœ‰æ”¶ç›Šç‡\n",
    "        \"\"\"\n",
    "        # ç”Ÿæˆäº¤æ˜“ä¿¡æ¯\n",
    "        liquidity = raw_data['liquidity']   # æµåŠ¨æ€§æŒ‡æ ‡\n",
    "        signal = torch.sigmoid(factors)     # å°†ä¿¡å·å‹ç¼©åˆ°(0,1)\n",
    "        is_safe = (liquidity > self.min_liq).float()    # æµåŠ¨æ€§è¾¾æ ‡? 1= å®‰å…¨,0=å±é™©\n",
    "        position = (signal > 0.85).float() * is_safe    #ä»…å½“ä¿¡å·å¼ºçš„ä¼é¹…å®‰å…¨çš„æ—¶å€™æŒä»“\n",
    "        # è®¡ç®—äº¤æ˜“æˆæœ¬(æ»‘ç‚¹+æ‰‹ç»­è´¹)\n",
    "        impact_slippage = self.trade_size / (liquidity + 1e-9)\n",
    "        impact_slippage = torch.clamp(impact_slippage, 0.0, 0.05) #æœ€å¤§æ»‘ç‚¹ 5%\n",
    "        total_slippage_one_way = self.base_fee + impact_slippage\n",
    "        # è®¡ç®—æ¢æ‰‹ç‡å’Œäº¤æ˜“æˆæœ¬\n",
    "        prev_pos = torch.roll(position, 1, dims=1)\n",
    "        prev_pos[:, 0] = 0 # ç¬¬ä¸€æ—¶é—´æ­¥æ— å‰åºæŒä»“\n",
    "        turnover = torch.abs(position - prev_pos)   # 0->1 æˆ– 1->0 è¡¨ç¤ºäº¤æ˜“\n",
    "        tx_cost = turnover * total_slippage_one_way\n",
    "        # è®¡ç®—ç›ˆäº\n",
    "        gross_pnl = position * target_ret # æŒä»“æœŸé—´æ¯›æ”¶ç›Š\n",
    "        net_pnl = gross_pnl - tx_cost       # æ‰£é™¤äº¤æ˜“æˆæœ¬åçš„å‡€æ”¶ç›Š\n",
    "        # æ„å»ºç»¼åˆè¯„åˆ†(fitness score)\n",
    "        cum_ret = net_pnl.sum(dim=1)    # æ¯ä¸ªä»£å¸çš„ç´¯è®¡å‡€æ”¶ç›Š\n",
    "        big_drawdowns = (net_pnl < -0.05).float().sum(dim=1)    # å•æœŸäºæŸ>5% çš„æ¬¡æ•°\n",
    "        score = cum_ret - (big_drawdowns * 2.0) #æƒ©ç½šå¤§å›æ’¤\n",
    "        # è¿‡æ»¤ä½æ´»è·ƒåº¦ç­–ç•¥\n",
    "        activity = position.sum(dim=1) # æ€»äº¤æ˜“æ¬¡æ•°\n",
    "        score = torch.where(activity < 5, torch.tensor(-10.0, device=score.device), score)\n",
    "        # æœ€ç»ˆé€‚åº”åº¦\n",
    "        final_fitness = torch.median(score)\n",
    "        return final_fitness, cum_ret.mean().item()        \n",
    "print(\"success backtest.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3978cf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success class AlphaEngine\n"
     ]
    }
   ],
   "source": [
    "# engine.py\n",
    "class AlphaEngine:\n",
    "    def __init__(self, use_lord_regularization=True, lord_decay_rate=1e-3, lord_num_iterations=5):\n",
    "        \"\"\"\n",
    "        Initialize AlphaGPT training engine.\n",
    "        \n",
    "        Args:\n",
    "            use_lord_regularization: Enable Low-Rank Decay (LoRD) regularization\n",
    "            lord_decay_rate: Strength of LoRD regularization\n",
    "            lord_num_iterations: Number of Newton-Schulz iterations per step\n",
    "        \"\"\"\n",
    "        self.loader = CryptoDataLoader()\n",
    "        self.loader.load_data()\n",
    "        \n",
    "        self.model = AlphaGPT().to(ModelConfig.DEVICE)\n",
    "        \n",
    "        # Standard optimizer\n",
    "        self.opt = torch.optim.AdamW(self.model.parameters(), lr=1e-3)\n",
    "        # ipdb.set_trace()\n",
    "        \n",
    "        # Low-Rank Decay regularizer\n",
    "        self.use_lord = use_lord_regularization\n",
    "        if self.use_lord:\n",
    "            self.lord_opt = NewtonSchulzLowRankDecay(\n",
    "                self.model.named_parameters(),\n",
    "                decay_rate=lord_decay_rate,\n",
    "                num_iterations=lord_num_iterations,\n",
    "                target_keywords=[\"q_proj\", \"k_proj\", \"attention\", \"qk_norm\"]\n",
    "            )\n",
    "            self.rank_monitor = StableRankMonitor(\n",
    "                self.model,\n",
    "                target_keywords=[\"q_proj\", \"k_proj\"]\n",
    "            )\n",
    "        else:\n",
    "            self.lord_opt = None\n",
    "            self.rank_monitor = None\n",
    "        \n",
    "        self.vm = StackVM()\n",
    "        self.bt = MemeBacktest()\n",
    "        \n",
    "        self.best_score = -float('inf')\n",
    "        self.best_formula = None\n",
    "        self.training_history = {\n",
    "            'step': [],\n",
    "            'avg_reward': [],\n",
    "            'best_score': [],\n",
    "            'stable_rank': []\n",
    "        }\n",
    "print(\"success class AlphaEngine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5403eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from SQL...\n",
      "Data Ready. Shape: torch.Size([8, 6, 1002])\n",
      "ğŸš€ Starting Meme Alpha Mining with LoRD Regularization...\n",
      "   LoRD Regularization enabled\n",
      "   Target keywords: ['q_proj', 'k_proj', 'attention', 'qk_norm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:11<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] New King: Score -10.00 | Ret nan% | Formula [5, 1, 15, 0, 2, 2, 13, 6, 12, 11, 8, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:43<12:03:59, 43.48s/it, AvgRew=-5.027, BestScore=-10.000, Rank=0.00]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 101\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    100\u001b[39m     eng = AlphaEngine(use_lord_regularization=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[43meng\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msuccess engin.py\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Gradient step\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m.opt.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28mself\u001b[39m.opt.step()\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Apply Low-Rank Decay regularization\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\workenv\\code\\AlphaGPT\\venv\\Lib\\site-packages\\torch\\_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\workenv\\code\\AlphaGPT\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\workenv\\code\\AlphaGPT\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def train(self):\n",
    "    print(\"ğŸš€ Starting Meme Alpha Mining with LoRD Regularization...\" if self.use_lord else \"ğŸš€ Starting Meme Alpha Mining...\")\n",
    "    if self.use_lord:\n",
    "        print(f\"   LoRD Regularization enabled\")\n",
    "        print(f\"   Target keywords: ['q_proj', 'k_proj', 'attention', 'qk_norm']\")\n",
    "    \n",
    "    pbar = tqdm(range(ModelConfig.TRAIN_STEPS))\n",
    "    # ipdb.set_trace()\n",
    "    \n",
    "    for step in pbar:\n",
    "        # ipdb.set_trace()\n",
    "        bs = ModelConfig.BATCH_SIZE\n",
    "        inp = torch.zeros((bs, 1), dtype=torch.long, device=ModelConfig.DEVICE)\n",
    "        \n",
    "        log_probs = []\n",
    "        tokens_list = []\n",
    "        \n",
    "        for _ in range(ModelConfig.MAX_FORMULA_LEN):\n",
    "            logits, _, _ = self.model(inp)\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            \n",
    "            log_probs.append(dist.log_prob(action))\n",
    "            tokens_list.append(action)\n",
    "            inp = torch.cat([inp, action.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        seqs = torch.stack(tokens_list, dim=1)\n",
    "        \n",
    "        rewards = torch.zeros(bs, device=ModelConfig.DEVICE)\n",
    "        \n",
    "        for i in range(bs):\n",
    "            formula = seqs[i].tolist()\n",
    "            \n",
    "            res = self.vm.execute(formula, self.loader.feat_tensor)\n",
    "            \n",
    "            if res is None:\n",
    "                rewards[i] = -5.0\n",
    "                continue\n",
    "            \n",
    "            if res.std() < 1e-4:\n",
    "                rewards[i] = -2.0\n",
    "                continue\n",
    "            \n",
    "            score, ret_val = self.bt.evaluate(res, self.loader.raw_data_cache, self.loader.target_ret)\n",
    "            rewards[i] = score\n",
    "            \n",
    "            if score.item() > self.best_score:\n",
    "                self.best_score = score.item()\n",
    "                self.best_formula = formula\n",
    "                tqdm.write(f\"[!] New King: Score {score:.2f} | Ret {ret_val:.2%} | Formula {formula}\")\n",
    "        \n",
    "        # Normalize rewards\n",
    "        adv = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        \n",
    "        loss = 0\n",
    "        for t in range(len(log_probs)):\n",
    "            loss += -log_probs[t] * adv\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        # Gradient step\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        \n",
    "        # Apply Low-Rank Decay regularization\n",
    "        if self.use_lord:\n",
    "            self.lord_opt.step()\n",
    "        \n",
    "        # Logging\n",
    "        avg_reward = rewards.mean().item()\n",
    "        postfix_dict = {'AvgRew': f\"{avg_reward:.3f}\", 'BestScore': f\"{self.best_score:.3f}\"}\n",
    "        \n",
    "        if self.use_lord and step % 100 == 0:\n",
    "            stable_rank = self.rank_monitor.compute()\n",
    "            postfix_dict['Rank'] = f\"{stable_rank:.2f}\"\n",
    "            self.training_history['stable_rank'].append(stable_rank)\n",
    "        \n",
    "        self.training_history['step'].append(step)\n",
    "        self.training_history['avg_reward'].append(avg_reward)\n",
    "        self.training_history['best_score'].append(self.best_score)\n",
    "        \n",
    "        pbar.set_postfix(postfix_dict)\n",
    "\n",
    "    # Save best formula\n",
    "    with open(\"best_meme_strategy.json\", \"w\") as f:\n",
    "        json.dump(self.best_formula, f)\n",
    "    \n",
    "    # Save training history\n",
    "    import json as js\n",
    "    with open(\"training_history.json\", \"w\") as f:\n",
    "        js.dump(self.training_history, f)\n",
    "    \n",
    "    print(f\"\\nâœ“ Training completed!\")\n",
    "    print(f\"  Best score: {self.best_score:.4f}\")\n",
    "    print(f\"  Best formula: {self.best_formula}\")\n",
    "\n",
    "AlphaEngine.train=train\n",
    "if __name__ == \"__main__\":\n",
    "    eng = AlphaEngine(use_lord_regularization=True)\n",
    "    eng.train()\n",
    "print(\"success engin.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52d613e",
   "metadata": {},
   "source": [
    "å½“ç„¶å¯ä»¥ï¼ä¸‹é¢æ˜¯å¯¹ `train(self)` å‡½æ•°çš„**é€æ®µè¯¦ç»†è§£æ**ï¼ŒåŒ…æ‹¬æ¯éƒ¨åˆ†çš„ç›®çš„ã€å…³é”®é€»è¾‘ã€æ½œåœ¨ä¼˜åŒ–ç‚¹å’Œè°ƒè¯•å»ºè®®ã€‚è¿™ä¸ªå‡½æ•°å®ç°äº†ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆPolicy Gradientï¼‰çš„â€œMeme Alpha Miningâ€è®­ç»ƒæµç¨‹ï¼Œå¹¶å¯é€‰åœ°é›†æˆ **LoRDï¼ˆLow-Rank Decayï¼‰æ­£åˆ™åŒ–**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  æ•´ä½“ç›®æ ‡\n",
    "\n",
    "è¯¥å‡½æ•°çš„ç›®æ ‡æ˜¯ï¼š\n",
    "> **é€šè¿‡ç­–ç•¥æ¢¯åº¦ï¼ˆREINFORCEï¼‰æ–¹æ³•ï¼Œè®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼ˆself.modelï¼‰ç”Ÿæˆæœ‰æ•ˆçš„é‡‘èå› å­å…¬å¼ï¼ˆformulaï¼‰ï¼Œä»¥æœ€å¤§åŒ–æŸç§ alpha è¯„åˆ†ï¼ˆç”± self.bt.evaluate æä¾›ï¼‰**ã€‚\n",
    "\n",
    "ç”Ÿæˆçš„å…¬å¼æ˜¯ä¸€ä¸² token åºåˆ—ï¼Œç”±è™šæ‹Ÿæœºï¼ˆ`self.vm`ï¼‰è§£é‡Šæ‰§è¡Œï¼Œè¾“å‡ºä¸€ä¸ªå› å­å€¼åºåˆ—ï¼Œå†ç”±è¯„ä¼°å™¨æ‰“åˆ†ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ ç¬¬ 1 æ®µï¼šåˆå§‹åŒ–ä¸æ—¥å¿—æ‰“å°\n",
    "\n",
    "```python\n",
    "def train(self):\n",
    "    print(\"ğŸš€ Starting Meme Alpha Mining with LoRD Regularization...\" if self.use_lord else \"ğŸš€ Starting Meme Alpha Mining...\")\n",
    "    if self.use_lord:\n",
    "        print(f\"   LoRD Regularization enabled\")\n",
    "        print(f\"   Target keywords: ['q_proj', 'k_proj', 'attention', 'qk_norm']\")\n",
    "```\n",
    "\n",
    "### âœ… åŠŸèƒ½è¯´æ˜ï¼š\n",
    "- æ‰“å°è®­ç»ƒå¼€å§‹ä¿¡æ¯ã€‚\n",
    "- å¦‚æœå¯ç”¨äº† `LoRD`ï¼ˆä½ç§©è¡°å‡æ­£åˆ™åŒ–ï¼‰ï¼Œé¢å¤–æ‰“å°å…¶çŠ¶æ€å’Œç›®æ ‡æ¨¡å—å…³é”®è¯ï¼ˆç”¨äºåç»­å¯¹ç‰¹å®šå±‚æ–½åŠ æ­£åˆ™ï¼‰ã€‚\n",
    "\n",
    "### ğŸ’¡ èƒŒæ™¯çŸ¥è¯†ï¼š\n",
    "- **LoRD** æ˜¯ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œé€šè¿‡é¼“åŠ±æƒé‡çŸ©é˜µä¿æŒä½ç§©ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¸¸ç”¨äºå¤§æ¨¡å‹å¾®è°ƒã€‚\n",
    "- è¿™é‡Œåªå¯¹åŒ…å« `'q_proj'`, `'k_proj'` ç­‰å…³é”®è¯çš„æ¨¡å—åº”ç”¨ LoRDã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ ç¬¬ 2 æ®µï¼šè¿›åº¦æ¡åˆå§‹åŒ–\n",
    "\n",
    "```python\n",
    "pbar = tqdm(range(ModelConfig.TRAIN_STEPS))\n",
    "```\n",
    "\n",
    "### âœ… åŠŸèƒ½è¯´æ˜ï¼š\n",
    "- ä½¿ç”¨ `tqdm` åˆ›å»ºä¸€ä¸ªè¿›åº¦æ¡ï¼Œæ€»æ­¥æ•°ä¸º `ModelConfig.TRAIN_STEPS`ï¼ˆä¾‹å¦‚ 10,000 æ­¥ï¼‰ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ ç¬¬ 3 æ®µï¼šä¸»è®­ç»ƒå¾ªç¯ï¼ˆæ¯ä¸€æ­¥ï¼‰\n",
    "\n",
    "```python\n",
    "for step in pbar:\n",
    "```\n",
    "\n",
    "è¿›å…¥ä¸»è®­ç»ƒå¾ªç¯ï¼Œæ¯ä¸€æ­¥ï¼ˆstepï¼‰æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„ **rollout + reward + loss + update** æµç¨‹ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ ç¬¬ 4 æ®µï¼šå‡†å¤‡è¾“å…¥ & é‡‡æ ·åŠ¨ä½œåºåˆ—ï¼ˆRolloutï¼‰\n",
    "\n",
    "```python\n",
    "bs = ModelConfig.BATCH_SIZE\n",
    "inp = torch.zeros((bs, 1), dtype=torch.long, device=ModelConfig.DEVICE)\n",
    "\n",
    "log_probs = []\n",
    "tokens_list = []\n",
    "\n",
    "for _ in range(ModelConfig.MAX_FORMULA_LEN):\n",
    "    logits, _, _ = self.model(inp)\n",
    "    dist = Categorical(logits=logits)\n",
    "    action = dist.sample()\n",
    "    \n",
    "    log_probs.append(dist.log_prob(action))\n",
    "    tokens_list.append(action)\n",
    "    inp = torch.cat([inp, action.unsqueeze(1)], dim=1)\n",
    "\n",
    "seqs = torch.stack(tokens_list, dim=1)\n",
    "```\n",
    "\n",
    "### âœ… åŠŸèƒ½è¯´æ˜ï¼š\n",
    "- **æ‰¹é‡ rollout**ï¼šåŒæ—¶ç”Ÿæˆ `BATCH_SIZE` ä¸ªå…¬å¼ï¼ˆå¹¶è¡Œé‡‡æ ·ï¼‰ã€‚\n",
    "- åˆå§‹è¾“å…¥ `inp` æ˜¯å…¨é›¶ tokenï¼ˆå¯èƒ½ä»£è¡¨ `<bos>` æˆ–èµ·å§‹ç¬¦ï¼‰ã€‚\n",
    "- å¾ªç¯ `MAX_FORMULA_LEN` æ¬¡ï¼Œæ¯æ¬¡ï¼š\n",
    "  - æ¨¡å‹è¾“å‡º logitsï¼ˆä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡åˆ†å¸ƒï¼‰\n",
    "  - ç”¨ `Categorical` é‡‡æ ·ä¸€ä¸ª actionï¼ˆtokenï¼‰\n",
    "  - è®°å½•è¯¥ action çš„ **log probability**ï¼ˆç”¨äºç­–ç•¥æ¢¯åº¦ï¼‰\n",
    "  - å°† action æ‹¼æ¥åˆ°è¾“å…¥åºåˆ—æœ«å°¾ï¼ˆè‡ªå›å½’ç”Ÿæˆï¼‰\n",
    "- æœ€ç»ˆ `seqs` å½¢çŠ¶ä¸º `(bs, MAX_FORMULA_LEN)`ï¼Œå³ä¸€æ‰¹ç”Ÿæˆçš„å…¬å¼ã€‚\n",
    "\n",
    "### âš ï¸ æ³¨æ„ï¼š\n",
    "- è¿™æ˜¯ **æ—  teacher forcing çš„çº¯é‡‡æ ·ç”Ÿæˆ**ï¼Œç¬¦åˆå¼ºåŒ–å­¦ä¹ è®¾å®šã€‚\n",
    "- æ²¡æœ‰ä½¿ç”¨ beam search æˆ– top-kï¼Œå®Œå…¨æ˜¯éšæœºé‡‡æ ·ï¼ˆæ¢ç´¢æ€§å¼ºï¼Œä½†æ–¹å·®å¤§ï¼‰ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ ç¬¬ 5 æ®µï¼šè®¡ç®—å¥–åŠ±ï¼ˆRewardï¼‰\n",
    "\n",
    "```python\n",
    "rewards = torch.zeros(bs, device=ModelConfig.DEVICE)\n",
    "\n",
    "for i in range(bs):\n",
    "    formula = seqs[i].tolist()\n",
    "    \n",
    "    res = self.vm.execute(formula, self.loader.feat_tensor)\n",
    "    \n",
    "    if res is None:\n",
    "        rewards[i] = -5.0\n",
    "        continue\n",
    "    \n",
    "    if res.std() < 1e-4:\n",
    "        rewards[i] = -2.0\n",
    "        continue\n",
    "    \n",
    "    score, ret_val = self.bt.evaluate(res, self.loader.raw_data_cache, self.loader.target_ret)\n",
    "    rewards[i] = score\n",
    "    \n",
    "    if score.item() > self.best_score:\n",
    "        self.best_score = score.item()\n",
    "        self.best_formula = formula\n",
    "        tqdm.write(f\"[!] New King: Score {score:.2f} | Ret {ret_val:.2%} | Formula {formula}\")\n",
    "```\n",
    "\n",
    "### âœ… åŠŸèƒ½è¯´æ˜ï¼š\n",
    "- å¯¹æ¯ä¸ªç”Ÿæˆçš„å…¬å¼ `formula`ï¼š\n",
    "  1. **æ‰§è¡Œ**ï¼šé€šè¿‡ `self.vm.execute()` åœ¨ç‰¹å¾å¼ é‡ä¸Šè¿è¡Œå…¬å¼ï¼Œå¾—åˆ°å› å­å€¼ `res`ï¼ˆå½¢çŠ¶å¦‚ `[num_stocks * num_days]`ï¼‰\n",
    "  2. **å¼‚å¸¸å¤„ç†**ï¼š\n",
    "     - è‹¥ `res is None`ï¼ˆæ‰§è¡Œå¤±è´¥ï¼Œå¦‚é™¤é›¶ã€éæ³•æ“ä½œï¼‰â†’ å¥–åŠ± `-5.0`\n",
    "     - è‹¥ `res.std() < 1e-4`ï¼ˆå› å­æ— å˜åŒ–ï¼Œå¸¸æ•°ï¼‰â†’ å¥–åŠ± `-2.0`\n",
    "  3. **è¯„ä¼°**ï¼šè°ƒç”¨ `self.bt.evaluate()` è®¡ç®— alpha åˆ†æ•°ï¼ˆå¦‚ ICã€IRã€å¹´åŒ–æ”¶ç›Šç­‰ï¼‰\n",
    "  4. **è®°å½•æœ€ä¼˜**ï¼šå¦‚æœå½“å‰åˆ†æ•°è¶…è¿‡å†å²æœ€ä½³ï¼Œæ›´æ–° `best_score` å’Œ `best_formula`ï¼Œå¹¶æ‰“å°æ—¥å¿—\n",
    "\n",
    "### ğŸ’¡ å…³é”®ç»„ä»¶ï¼š\n",
    "- `self.vm`ï¼šè™šæ‹Ÿæœºï¼Œè§£é‡Šæ‰§è¡Œ token åºåˆ—ä¸ºæ•°å­¦è¡¨è¾¾å¼ï¼ˆç±»ä¼¼ DSL è§£é‡Šå™¨ï¼‰\n",
    "- `self.bt`ï¼šBacktest Engineï¼Œè´Ÿè´£è®¡ç®—å› å­æœ‰æ•ˆæ€§æŒ‡æ ‡\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ ç¬¬ 6 æ®µï¼šä¼˜åŠ¿å‡½æ•°è®¡ç®—ï¼ˆAdvantage Normalizationï¼‰\n",
    "\n",
    "```python\n",
    "adv = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "```\n",
    "\n",
    "### âœ… åŠŸèƒ½è¯´æ˜ï¼š\n",
    "- ä½¿ç”¨ **reward baseline**ï¼ˆå‡å€¼ï¼‰æ¥é™ä½ç­–ç•¥æ¢¯åº¦çš„æ–¹å·®ã€‚\n",
    "- æ ‡å‡†åŒ–ä¼˜åŠ¿ï¼ˆadvantageï¼‰ï¼š`(R - mean) / std`\n",
    "- åŠ  `1e-5` é˜²æ­¢é™¤é›¶ï¼ˆå½“æ‰€æœ‰ reward ç›¸åŒæ—¶ï¼‰\n",
    "\n",
    "> è¿™æ˜¯ REINFORCE with Baseline çš„æ ‡å‡†åšæ³•ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ ç¬¬ 7 æ®µï¼šç­–ç•¥æ¢¯åº¦æŸå¤±è®¡ç®—\n",
    "\n",
    "```python\n",
    "loss = 0\n",
    "for t in range(len(log_probs)):\n",
    "    loss += -log_probs[t] * adv\n",
    "\n",
    "loss = loss.mean()\n",
    "```\n",
    "\n",
    "### âœ… åŠŸèƒ½è¯´æ˜ï¼š\n",
    "- ç´¯ç§¯æ—¶é—´æ­¥ä¸Šçš„ç­–ç•¥æ¢¯åº¦æŸå¤±ï¼š\n",
    "  $$\n",
    "  \\mathcal{L} = -\\frac{1}{B} \\sum_{i=1}^B \\sum_{t=1}^T \\log \\pi(a_t | s_t) \\cdot A_i\n",
    "  $$\n",
    "  å…¶ä¸­ $A_i$ æ˜¯ç¬¬ i ä¸ªæ ·æœ¬çš„ä¼˜åŠ¿ï¼ˆè¿™é‡Œå¯¹ batch ç»´åº¦åšäº† meanï¼‰\n",
    "- æ³¨æ„ï¼š**adv æ˜¯æŒ‰æ ·æœ¬ç»´åº¦å¹¿æ’­çš„**ï¼Œæ‰€ä»¥ `log_probs[t]`ï¼ˆå½¢çŠ¶ `[bs]`ï¼‰ä¸ `adv`ï¼ˆ`[bs]`ï¼‰é€å…ƒç´ ç›¸ä¹˜\n",
    "\n",
    "### âš ï¸ æ½œåœ¨é—®é¢˜ï¼š\n",
    "- æ²¡æœ‰è€ƒè™‘ **æ—¶é—´æŠ˜æ‰£ï¼ˆdiscount factor Î³ï¼‰**ï¼Œå› ä¸ºè¿™æ˜¯â€œä¸€æ¬¡æ€§â€ä»»åŠ¡ï¼ˆå…¬å¼ç”Ÿæˆåç«‹å³å¾— rewardï¼‰\n",
    "- æŸå¤±æœªåŠ  entropy regularizationï¼ˆå¯èƒ½å¯¼è‡´æ—©ç†Ÿæ”¶æ•›ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ ç¬¬ 8 æ®µï¼šåå‘ä¼ æ’­ä¸ä¼˜åŒ–å™¨æ›´æ–°\n",
    "\n",
    "```python\n",
    "self.opt.zero_grad()\n",
    "loss.backward()\n",
    "self.opt.step()\n",
    "```\n",
    "\n",
    "- æ ‡å‡† PyTorch ä¼˜åŒ–æ­¥éª¤ï¼šæ¸…é›¶æ¢¯åº¦ â†’ åå‘ä¼ æ’­ â†’ æ›´æ–°æ¨¡å‹å‚æ•°\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ ç¬¬ 9 æ®µï¼šLoRD æ­£åˆ™åŒ–æ›´æ–°ï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "```python\n",
    "if self.use_lord:\n",
    "    self.lord_opt.step()\n",
    "```\n",
    "\n",
    "### âœ… åŠŸèƒ½è¯´æ˜ï¼š\n",
    "- å¦‚æœå¯ç”¨ LoRDï¼Œé¢å¤–è°ƒç”¨ `lord_opt.step()`ã€‚\n",
    "- é€šå¸¸ `lord_opt` æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºæ›´æ–° LoRD å‚æ•°ï¼ˆå¦‚ä½ç§©çŸ©é˜µï¼‰çš„ä¼˜åŒ–å™¨ã€‚\n",
    "- è¿™è¡¨æ˜ LoRD å¯èƒ½æ˜¯ä»¥ **å¯å­¦ä¹ ä½ç§©æ‰°åŠ¨** çš„å½¢å¼å®ç°çš„ï¼ˆç±»ä¼¼ LoRAï¼Œä½†ç”¨äºæ­£åˆ™è€Œéé€‚é…ï¼‰ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ ç¬¬ 10 æ®µï¼šæ—¥å¿—è®°å½•ä¸è¿›åº¦æ¡æ›´æ–°\n",
    "\n",
    "```python\n",
    "avg_reward = rewards.mean().item()\n",
    "postfix_dict = {'AvgRew': f\"{avg_reward:.3f}\", 'BestScore': f\"{self.best_score:.3f}\"}\n",
    "\n",
    "if self.use_lord and step % 100 == 0:\n",
    "    stable_rank = self.rank_monitor.compute()\n",
    "    postfix_dict['Rank'] = f\"{stable_rank:.2f}\"\n",
    "    self.training_history['stable_rank'].append(stable_rank)\n",
    "\n",
    "self.training_history['step'].append(step)\n",
    "self.training_history['avg_reward'].append(avg_reward)\n",
    "self.training_history['best_score'].append(self.best_score)\n",
    "\n",
    "pbar.set_postfix(postfix_dict)\n",
    "```\n",
    "\n",
    "### âœ… åŠŸèƒ½è¯´æ˜ï¼š\n",
    "- è®°å½•å¹³å‡å¥–åŠ±ã€æœ€ä½³åˆ†æ•°\n",
    "- æ¯ 100 æ­¥ï¼ˆè‹¥å¯ç”¨ LoRDï¼‰è®¡ç®—å¹¶è®°å½• **ç¨³å®šç§©ï¼ˆstable rankï¼‰**ï¼Œç”¨äºç›‘æ§æ¨¡å‹å¤æ‚åº¦\n",
    "- æ›´æ–° `training_history` å­—å…¸ï¼ˆç”¨äºåç»­ç»˜å›¾æˆ–åˆ†æï¼‰\n",
    "- åœ¨ tqdm è¿›åº¦æ¡æœ«å°¾æ˜¾ç¤ºå®æ—¶æŒ‡æ ‡\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ ç¬¬ 11 æ®µï¼šè®­ç»ƒç»“æŸï¼Œä¿å­˜ç»“æœ\n",
    "\n",
    "```python\n",
    "with open(\"best_meme_strategy.json\", \"w\") as f:\n",
    "    json.dump(self.best_formula, f)\n",
    "\n",
    "import json as js\n",
    "with open(\"training_history.json\", \"w\") as f:\n",
    "    js.dump(self.training_history, f)\n",
    "\n",
    "print(f\"\\nâœ“ Training completed!\")\n",
    "print(f\"  Best score: {self.best_score:.4f}\")\n",
    "print(f\"  Best formula: {self.best_formula}\")\n",
    "```\n",
    "\n",
    "### âœ… åŠŸèƒ½è¯´æ˜ï¼š\n",
    "- ä¿å­˜æœ€ä¼˜å…¬å¼ï¼ˆtoken listï¼‰åˆ° `best_meme_strategy.json`\n",
    "- ä¿å­˜å®Œæ•´è®­ç»ƒå†å²ï¼ˆç”¨äºå¤ç°æˆ–åˆ†æï¼‰\n",
    "- æ‰“å°æœ€ç»ˆç»“æœ\n",
    "\n",
    "> ğŸ’¡ æ³¨ï¼š`import json as js` æ˜¯ä¸ºäº†é¿å…ä¸å‰é¢å¯èƒ½å­˜åœ¨çš„ `json` å†²çªï¼ˆè™½ç„¶æ­¤å¤„æ²¡å¿…è¦ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© æ€»ç»“ï¼šè¯¥è®­ç»ƒæµç¨‹çš„ç‰¹ç‚¹\n",
    "\n",
    "| ç‰¹æ€§ | è¯´æ˜ |\n",
    "|------|------|\n",
    "| **ç®—æ³•** | REINFORCEï¼ˆè’™ç‰¹å¡æ´›ç­–ç•¥æ¢¯åº¦ï¼‰ |\n",
    "| **ä»»åŠ¡ç±»å‹** | åºåˆ—ç”Ÿæˆï¼ˆå…¬å¼åˆæˆï¼‰+ å¼ºåŒ–å­¦ä¹  |\n",
    "| **å¥–åŠ±è®¾è®¡** | åŸºäºå› å­å›æµ‹è¡¨ç°ï¼ˆIC/IR/æ”¶ç›Šç­‰ï¼‰ |\n",
    "| **æ­£åˆ™åŒ–** | å¯é€‰ LoRDï¼ˆä½ç§©çº¦æŸï¼‰ |\n",
    "| **æ¢ç´¢æ–¹å¼** | çº¯éšæœºé‡‡æ ·ï¼ˆæ—  Îµ-greedyï¼‰ |\n",
    "| **æ‰¹å¤„ç†** | æ”¯æŒ batch rolloutï¼Œæå‡æ•ˆç‡ |\n",
    "| **è°ƒè¯•å‹å¥½** | æœ‰è¯¦ç»†æ—¥å¿—ã€è¿›åº¦æ¡ã€æœ€ä¼˜å…¬å¼å®æ—¶æ‰“å° |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ è°ƒè¯•å»ºè®®ï¼ˆé’ˆå¯¹ VS Codeï¼‰\n",
    "\n",
    "å¦‚æœä½ æƒ³åœ¨ VS Code ä¸­è°ƒè¯•æ­¤å‡½æ•°ï¼š\n",
    "\n",
    "1. åœ¨ `res = self.vm.execute(...)` æˆ– `score, ret_val = ...` è¡Œè®¾ç½®æ–­ç‚¹\n",
    "2. ä½¿ç”¨å‰é¢æä¾›çš„ `launch.json` é…ç½®\n",
    "3. æŒ‰ `F5` å¯åŠ¨è°ƒè¯•\n",
    "4. å¯æŸ¥çœ‹ `formula`, `res`, `rewards`, `log_probs` ç­‰å˜é‡\n",
    "\n",
    "> âš ï¸ æ³¨æ„ï¼šç”±äºä½¿ç”¨äº† `tqdm`ï¼Œè°ƒè¯•æ—¶å¯èƒ½çœ‹ä¸åˆ° print è¾“å‡ºï¼Œå»ºè®®ä¸´æ—¶æ³¨é‡Šæ‰ `tqdm` æˆ–ä½¿ç”¨ `print` æ›¿ä»£ `tqdm.write`\n",
    "\n",
    "---\n",
    "\n",
    "å¦‚æœä½ éœ€è¦æˆ‘è¿›ä¸€æ­¥è§£é‡Š `vm.execute`ã€`bt.evaluate` æˆ– LoRD çš„å®ç°ç»†èŠ‚ï¼Œä¹Ÿå¯ä»¥ç»§ç»­æé—®ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d7f658a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# å®šä¹‰ä¸€ä¸ªåµŒå…¥å±‚ï¼šè¯æ±‡è¡¨å¤§å°10ï¼Œå‘é‡ç»´åº¦4\n",
    "embedding_layer = nn.Embedding(10, 4)\n",
    "\n",
    "# è¾“å…¥ï¼šå‡è®¾è¿™æ˜¯ä¸¤ä¸ªå•è¯çš„ID\n",
    "input_ids = torch.tensor([2, 5])\n",
    "\n",
    "# å‰å‘ä¼ æ’­ï¼šè‡ªåŠ¨æ ¹æ®IDæŸ¥æ‰¾å¯¹åº”çš„å‘é‡\n",
    "vectors = embedding_layer(input_ids)\n",
    "print(vectors.shape)  # torch.Size([2, 4])ï¼ŒæŸ¥æ‰¾åˆ°äº†2ä¸ª4ç»´å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa000c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# æ‰‹åŠ¨åˆ›å»ºä¸€ä¸ªæƒé‡çŸ©é˜µ (10ä¸ªè¯ï¼Œæ¯ä¸ª4ç»´)ï¼Œå¹¶æ ‡è®°ä¸ºå¯å­¦ä¹ å‚æ•°\n",
    "weight_matrix = nn.Parameter(torch.randn(10, 4))\n",
    "\n",
    "# è¾“å…¥ID\n",
    "input_ids = torch.tensor([2, 5])\n",
    "\n",
    "# æ‰‹åŠ¨â€œæŸ¥è¡¨â€ï¼šé€šè¿‡ç´¢å¼•è·å–å¯¹åº”çš„è¡Œ\n",
    "vectors = weight_matrix[input_ids]  # è¿™é‡Œçš„ weight_matrix å°±æ˜¯ Parameter\n",
    "print(vectors.shape)  # torch.Size([2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85765c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
